{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"5d8b76c9c1333ff2d0470d566227fd7ba1c835a6de7ed7d43a8afd95ebafadc4","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n</center>\n\n# Stacking For Classification with Python\n\nEstimated time needed: **45** minutes\n\n## Objectives\n\nAfter completing this lab you will be able to:\n\n*   **Understand** what Stacking is and how it works\n*   **Understand**  that Random Forests have less Correlation between predictors in their ensemble, improving accuracy\n*   **Apply** Stacking\n*   **Understand** Hyperparameters selection in  Stacking\n","metadata":{}},{"cell_type":"markdown","source":"Stacking takes several classification models called base learners and uses their output as the input for the meta-classifier. Consider the figure below the base learners $h_{1}(x)$, $h_{2}(x)$, $h_{3}(x)$, and $h_{4}(x)$ has the output $\\hat{y}_{1}$, $\\hat{y}_{2}$, $\\hat{y}_{3}$, $\\hat{y}_{4}$. These are used as an input to the meta classifier $H( \\hat{y}_{1}, \\hat{y}_{2}, \\hat{y}_{3}, \\hat{y}_{4})$, makes the final prediction $\\hat{y}=H( \\hat{y}_{1}, \\hat{y}_{2}, \\hat{y}_{3}, \\hat{y}_{4})$.\n","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/module5_Ensemble_Based_Methods/Screen_Shot_2022-01-12_at_9.40.33_PM.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\">\n</center>\n<center>Fig. 1 Stacking takes several classification models called base learners and uses their output as the input for the meta-classifier. </center>\n","metadata":{}},{"cell_type":"markdown","source":"We can train all the models using all the data but this causes over-fitting. To get a better idea of how the algorithm works we use K-fold Cross-validation. We use K-1 folds to train the base classifiers and the last fold to train the meta classifier. We repeat the process using different combinations of each fold. This is shown in Fig 2 where the color-coded square represents the different runs and folds. Each row represents a different run of K fold cross-validation, each column is one of K folds where K=3. For each column, we use the same color columns to train the classifiers and the different color is used to train the meta classifier.\n","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/module5_Ensemble_Based_Methods/stacking-kfold.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\">\n</center>\n<center>Fig. 2  K-fold Cross-validation to train Stacking classifier. </center>\n","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of contents</h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a>Apply Staking Using Wine Data </a></li>\n        <li><a href=\"https://practice/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">Practice</a></li>\n        <li><a>Cancer Data Example</a></li>\n    </ol>\n</div>\n<br>\n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"First, let's install and import the required libraries:\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries, pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n\n# You will need scikit-learn>=0.22.0 as StackingClassifier does not exist in version <0.22.0\n!mamba install -c conda-forge -qy scikit-learn=0.22.1\n!pip install pandas\n!pip install numpy\n!pip install matplotlib   \n!pip install seaborn\n\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# import pylab as plt\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ignore error warnings\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function will calculate the accuracy of the training and testing data given a model.\n","metadata":{}},{"cell_type":"code","source":"def get_accuracy(X_train, X_test, y_train, y_test, model):\n    return  {\"test Accuracy\":metrics.accuracy_score(y_test, model.predict(X_test)),\"trian Accuracy\": metrics.accuracy_score(y_train, model.predict(X_train))}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Apply   Staking Using   Wine Data  </h1>\n\nThe class is an essential factor in determining the quality of the wine; this dataset uses chemical analysis of wines grown in the same region in Italy from three different cultivars. Your task is to determine the class  of the wine using the features from the chemical analysis.\nThe features or attributes include\n\n<pre>\n1) Alcohol\n2) Malic acid\n3) Ash\n4) Alcalinity of ash\n5) Magnesium\n6) Total phenols\n7) Flavanoids\n8) Nonflavanoid phenols\n9) Proanthocyanins\n10)Color intensity\n11)Hue\n12)OD280/OD315 of diluted wines\n13)Proline\n</pre>\n\nFor more info <a href=\"https://archive.ics.uci.edu/ml/datasets/wine?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">here </a>,let's load the dataset:\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/wine.data\",names= ['Class', 'Alcohol', 'Malic acid', 'Ash',\n         'Alcalinity of ash' ,'Magnesium', 'Total phenols',\n         'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n         'Proline'])\n\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see all the dataset is comprised of numerical values using the method <code>dtypes</code>\n","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"the column <code>class</code> has the class of the wine, we can use the method <code>unique()</code> to obtain the classes:\n","metadata":{}},{"cell_type":"code","source":"df['Class'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can examine the correlation between each feature and the class variable. By examining the first row or column we see the features are correlated with the class variable.\n","metadata":{}},{"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also examine the <code>Pairplot</code> between pairs of features and the histogram; color-coded to each class. We see the separation between different classes:\n","metadata":{}},{"cell_type":"code","source":"# May need to specify bandwidth (bw) in order to plot, else can delete the `, diag_kws={'bw': 0.2})` code.\nsns.pairplot(df, hue=\"Class\", diag_kws={'bw': 0.2})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Pre-Processing and Selection\n","metadata":{}},{"cell_type":"markdown","source":"Letâ€™s examine the feature list\n","metadata":{}},{"cell_type":"code","source":"features=list(df)\nfeatures[1:]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We assign the class variables to <code>y</code> and feature variables to <code>X</code>\n","metadata":{}},{"cell_type":"code","source":"y,X=df[features[0]] ,df[features[1:]]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can standardize the data\n","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(X)\nX= scaler.transform(X)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can check if the data is standardized by checking the mean and standard deviation, which are approximately zero:\n","metadata":{}},{"cell_type":"code","source":"X.mean(axis=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.std(axis=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In Random Forest, we would use these data subsets to train each node of a tree.\n","metadata":{}},{"cell_type":"markdown","source":"### Train/Test dataset\n","metadata":{}},{"cell_type":"markdown","source":"We split our dataset into train and test set:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1)\nprint ('Train set', X_train.shape,  y_train.shape)\nprint ('Test set', X_test.shape,  y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Stacking consists of creating a Stacking Classifier object, but first, you require a dictionary of <code>estimators</code> (individual model objects or base learners). The key of the dictionary is a name that is up to you, we use the usual acronym for the model. The value is the model object in this case SVC for Support Vector Classifier, dt for Decision Tree Classifier and KNN for K Neighbors Classifier.\n","metadata":{}},{"cell_type":"code","source":"estimators = [('SVM',SVC(random_state=42)),('KNN',KNeighborsClassifier()),('dt',DecisionTreeClassifier())]\nestimators","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To train the final model we create a Stacking Classifier, this combines the base estimators using the meta estimator. The meta-classifier is determined by the parameter <code>final_estimator</code> in this case we use Logistic Regression, we also input the base classifiers using the <code>estimators</code> parameter and fit the model.\n","metadata":{}},{"cell_type":"code","source":"clf = StackingClassifier( estimators=estimators, final_estimator= LogisticRegression())\nclf.fit(X_train, y_train)\nclf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can make a prediction\n","metadata":{}},{"cell_type":"code","source":"yhat=clf.predict(X_test)\nyhat","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can obtain the training and testing accuracy, we see the model performs well.\n","metadata":{}},{"cell_type":"code","source":"get_accuracy(X_train, X_test, y_train, y_test, clf)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:red\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:red\">\n   <b> Note: Like most complex models Stacking is prone to overfitting</b>\n","metadata":{}},{"cell_type":"markdown","source":"<h1>Practice</h1>\n","metadata":{}},{"cell_type":"markdown","source":"Create a Stacking Classifier object as before but exchange the Decision Tree Classifier with the SVM classifier. Calculate the accuracy on the training and testing data.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details><summary>Click here for the solution</summary>\n\n```python\nestimators = [('SVM',SVC(random_state=42)),('KNN',KNeighborsClassifier()),('lr',LogisticRegression())]\nclf = StackingClassifier( estimators=estimators, final_estimator= DecisionTreeClassifier())\nclf.fit(X_train, y_train)\n\nget_accuracy(X_train, X_test, y_train, y_test, clf)\n\n```\n\n</details>\n","metadata":{}},{"cell_type":"markdown","source":"<h1> GridSearchCV and Stacking Classifiers  </h1>\n","metadata":{}},{"cell_type":"markdown","source":"Imagine that you are a medical researcher compiling data for a study. You have collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.\n\nPart of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. The features of this dataset are Age, Sex, Blood Pressure, and the Cholesterol of the patients, and the target is the drug that each patient responded to.\n\nIt is a sample of multiclass classifier, and you can use the training part of the dataset to build a decision tree, and then use it to predict the class of a unknown patient, or to prescribe a drug to a new patient. You will use GridSearchCV and Stacking Classifiers  to find the best results.\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv\", delimiter=\",\")\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's create the X and y for our dataset:\n","metadata":{}},{"cell_type":"code","source":"X = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\nX[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df[\"Drug\"]\ny[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now lets use a <code>LabelEncoder</code> to turn categorical features into numerical:\n","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1]) \n\n\nle_BP = preprocessing.LabelEncoder()\nle_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\nX[:,2] = le_BP.transform(X[:,2])\n\n\nle_Chol = preprocessing.LabelEncoder()\nle_Chol.fit([ 'NORMAL', 'HIGH'])\nX[:,3] = le_Chol.transform(X[:,3]) \n\nX[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(X)\nX= scaler.transform(X)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split the data into training and testing data with a 80/20 split\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have our dictionary of estimators, the individual model objects or base learners.\n","metadata":{}},{"cell_type":"code","source":"estimators = [('SVM',SVC(random_state=42)),('knn',KNeighborsClassifier()),('dt',DecisionTreeClassifier())]\nestimators","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create a Stacking Classifier:\n","metadata":{}},{"cell_type":"code","source":"clf = StackingClassifier( estimators=estimators, final_estimator= LogisticRegression())\nclf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In order to alter the base models in the dictionary of hyperparameter values, we add the key value of each model followed by the parameter of the model we would like to vary.\n","metadata":{}},{"cell_type":"code","source":"param_grid = {'dt__max_depth': [n for n in range(10)],'dt__random_state':[0],'SVM__C':[0.01,0.1,1],'SVM__kernel':['linear', 'poly', 'rbf'],'knn__n_neighbors':[1,4,8,9] }\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use <code>GridSearchCV</code> to search over specified parameter values of the model.\n","metadata":{}},{"cell_type":"code","source":"search = GridSearchCV(estimator=clf, param_grid=param_grid,scoring='accuracy')\nsearch.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can find the accuracy of the best model.\n","metadata":{}},{"cell_type":"code","source":"search.best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can find the best parameter values:\n","metadata":{}},{"cell_type":"code","source":"search.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can find the accuracy test data:\n","metadata":{}},{"cell_type":"code","source":"get_accuracy(X_train, X_test, y_train, y_test, search)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We use sklearn version 0.20.1 for all other labs, please run this command after finishing the lab\n\n!mamba install -c conda-forge -qy scikit-learn=0.20.1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n\n### Other Contributors\n\n<a href=\"https://www.linkedin.com/in/richard-ye/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Richard Ye</a>\n\n## <h3 align=\"center\"> Â© IBM Corporation 2021. All rights reserved. <h3/>\n\n<!--## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description          |\n| ----------------- | ------- | ---------- | --------------------------- |\n| 2021-01-01        | 1.0     | Joseph S   | Created the initial version |\n| 2022-02-09        | 1.1     | Steve Hord | QA pass                     |\n| 2022-05-03        | 1.2     | Richard Ye | Updated sklearn package to `0.22.1` to remove errors |--!>\n\n\n","metadata":{}}]}