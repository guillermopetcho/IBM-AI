{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Optimizers in Gradient Descent**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **45** minutes\n","metadata":{}},{"cell_type":"markdown","source":"Suppose you are given a classification task of determining if a food item is healthy or not. This task would bring a lot of values to people who need diet management such as diabetic patients.\n","metadata":{}},{"cell_type":"markdown","source":"Each food item has nutrition information per serving such as its carbohydrates, fat, fibers, vitamins, etc. We know that for unhealthy food items such as fried chicken, we should choose less frequently in our daily diet and for healthy food items such as salad without sauce, we should chooose them more often in our diet. Now given the complexity of the problem, you plan to use Neural Networks to classify those food items.\n","metadata":{}},{"cell_type":"markdown","source":"However, the first neural network you trained had pretty bad performance and essentially underfits the dataset. As a data scientist, you know the performance may be caused by many factors such as bad data quality, architecture of your neural network, non-optimized hyperparameter tuning, etc. So you tried them all and the model result still looks pretty bad. \n","metadata":{}},{"cell_type":"markdown","source":"Then how to improve its performance, in this lab, you will see different optimizers could also play a big role in neural network training process.\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n","metadata":{}},{"cell_type":"markdown","source":"After completing this lab you will be able to:\n","metadata":{}},{"cell_type":"markdown","source":"* Explain several popular optimizers\n* Evaluate their performance in a real-world classification scenario\n* (Optional)Implement adam optimizer from scrach\n","metadata":{}},{"cell_type":"markdown","source":"----\n","metadata":{}},{"cell_type":"markdown","source":"## Prepare and setup the lab environment\n","metadata":{}},{"cell_type":"markdown","source":"At the beginning, we need to install one required Python packages.\n","metadata":{}},{"cell_type":"code","source":"%%capture \n!pip install --upgrade tensorflow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"and import necessary class/methods in the packages\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport sys\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn import metrics\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load and explore the dataset\n","metadata":{}},{"cell_type":"code","source":"food_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/food_items.csv\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's quickly take a look at the dataset:\n","metadata":{}},{"cell_type":"code","source":"food_df.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, it has 13260 data instances and 18 columns (features + label):\n","metadata":{}},{"cell_type":"code","source":"food_df.dtypes","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"More features are about the nutritions of a specific food items. Then let's check the target variable, i.e., the `class` column:\n","metadata":{}},{"cell_type":"code","source":"# # Get the row entries with the last col 'class'\nfood_df.iloc[:, -1:].value_counts(normalize=True)\nfood_df.iloc[:, -1:].value_counts().plot.bar(color=['#e67e22', '#27ae60', '#2980b9'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The target variable indicates how often we should include this food item in our diet, and it has three classes `In Moderation`, `Less Often`, `More Often`. It indicates that this is a multi-class classification task and the class distribution is a little bit imbalanced.\n","metadata":{}},{"cell_type":"markdown","source":"## Data processing\n","metadata":{}},{"cell_type":"markdown","source":"Then, let's quickly process the dataset:\n","metadata":{}},{"cell_type":"markdown","source":"First split the features `X` (the input) and labels `y` (the output)\n","metadata":{}},{"cell_type":"code","source":"X_raw = food_df.iloc[:, :-1]\ny_raw = food_df.iloc[:, -1:]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Then normalize the `X`:\n","metadata":{}},{"cell_type":"code","source":"# Create a MinMaxScaler object\nscaler = MinMaxScaler()\n# Scaling the raw input features\nX = scaler.fit_transform(X_raw)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"and encode the target variable:\n","metadata":{}},{"cell_type":"code","source":"# Create a LabelEncoder object\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y_raw.values.ravel())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally we can split the dataset into training and testing datasets, and we're ready to train a neural network.\n","metadata":{}},{"cell_type":"code","source":"rs = 123 # set random state","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, let's split the training and testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)\nprint(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train a Neural Network with SGD (Stochastic Gradient Descent)\n","metadata":{}},{"cell_type":"markdown","source":"As the initial model, we plan to train a basic multilayer perceptron neural network with the following settings:\n- Two hidden layers with shapes `(17, 32)` and `(32, 8)`. Note the number `17` represents the dimension of the feature vector X. You may try different hidden layers setting as well.\n- Set the solver/optimizer to be SGD\n- Disable momentum\n- Enable early stopping\n","metadata":{}},{"cell_type":"code","source":"base_ann = MLPClassifier(random_state=rs,  hidden_layer_sizes=(32, 8), \n                    solver='sgd', momentum=0, \n                    early_stopping=True,\n                    max_iter=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you may recall, the formula for SGD is using the gradient calculated from instance `i` only:\n","metadata":{}},{"cell_type":"markdown","source":"$$W_t := W_{t-1} - \\alpha \\nabla_iJ$$\n","metadata":{}},{"cell_type":"markdown","source":"Then we may use the following helper function to quickly train the neural network and report its training and test score.\n","metadata":{}},{"cell_type":"code","source":"def fit_and_score(model, X_train, X_test, y_train, y_test):\n    start = time.time()\n    model.fit(X_train, y_train)\n    fit_time = time.time() - start\n    n_iter = model.n_iter_\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    loss_curve = model.loss_curve_\n    return round(fit_time, 2), n_iter, train_score, test_score","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fit_time, n_iter, train_score, test_score = fit_and_score(base_ann, X_train, X_test, y_train, y_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Training converged after {n_iter} iterations with train score (accuracy) {round(train_score, 2)} \\\nand test score (accuracy) {round(test_score, 2)}.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see from above cell output, a classification task with accuracy around `40%` is definitly unacceptable. But how is it even possible as we are using a Neural Network to do the task!\n","metadata":{}},{"cell_type":"markdown","source":"Such bad train and test score is normally considered as underfitting which actually happends very often in training neural networks which is normally more complex than shallow models. The reasons could be too simple network architecture, insufficient training data or iterations, trapped in local minima, etc.\n","metadata":{}},{"cell_type":"markdown","source":"For our case, it could be the bad quality or low predictability of our dataset. To verify that, let's try to build a simple logistic regression to see if it can perform the task well.\n","metadata":{}},{"cell_type":"code","source":"# Define a logistic regression model with above arguments\nlr_model = LogisticRegression(random_state=rs, max_iter = 200)\nlr_model.fit(X_train, y_train)\nlr_score = lr_model.score(X_test, y_test)\nprint(f\"The test score for the logistic regression is {round(lr_score, 2)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can see a logistic linear regression can get around 0.75 accuracy which is much better than the neural network. We can see our dataset is pretty predictable. We must have done something improperly in the neural network training.\n","metadata":{}},{"cell_type":"markdown","source":"One common reason is that the SGD was trapped in one of the local minima. Let's illusrate this idea in the following simple example:\n","metadata":{}},{"cell_type":"markdown","source":"First we define a helper function to draw some cost functions.\n","metadata":{}},{"cell_type":"code","source":"def draw_cost(X, Y, Z, title):\n    fig = plt.figure()\n    fig.set_size_inches(8, 6, forward=True)\n    fig.set_dpi(100)\n    ax = plt.axes(projection='3d')\n    ax.view_init(30, 35)\n    ax.contour3D(X, Y, Z, 100, cmap=plt.cm.coolwarm)\n    ax.set_title(title)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ideally, a cost function only has one global minima and SGD can find it.\n","metadata":{}},{"cell_type":"code","source":"def one_mini_function():\n    w1 = np.linspace(-10, 10, 50)\n    w2 = np.linspace(-10, 10, 50)\n    X, Y = np.meshgrid(w1, w2)\n    Z = np.log(np.sqrt(X ** 2 + Y ** 2))\n    return X, Y, Z\n\nX, Y, Z = one_mini_function()\ndraw_cost(X, Y, Z, \"Cost function with the global minima\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def multi_mini_function():\n    w1 = np.linspace(-10, 11, 50)\n    w2 = np.linspace(-10, 11, 50)\n    X, Y = np.meshgrid(w1, w2)\n    Z = X ** 2  - Y ** 2 + 4*X\n    return X, Y, Z\n\nX1, Y1, Z1 = multi_mini_function()\ndraw_cost(X1, Y1, Z1, \"Cost function with a saddle point in the middle\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"However, if the cost function has multiple minima or even saddle points (where gradients are very small or zeros so the weights won't be updated), the SGD may be stuck at the local minima. For example, the following cost function has a saddle point at `(0, 0)` in the middle which could stop the gradients from further going down, as shown in the following picture:\n","metadata":{}},{"cell_type":"markdown","source":"![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/saddle.png)\n","metadata":{}},{"cell_type":"markdown","source":"As a result, the total loss won't be reduced and the model would perform badly in the classification task. To tackle this local minima challenge, we could either try to add some noises in SGD or randomly switch the starting point. Furthermore, we may also utilize momentum to make gradient jump out of local minima or saddle points.\n","metadata":{}},{"cell_type":"markdown","source":"## Retrain the Neural Network with momentum\n","metadata":{}},{"cell_type":"markdown","source":"With the regular SGD, the weights are generally moving slowly towards to a optimum (can be a local one like the previous example). Now, with momentum, you're going to smooth out this process. You can do this by taking somewhat of a running average of each of the steps and then smoothing out that variation of each of the individual steps for regular gradient descent.\n","metadata":{}},{"cell_type":"markdown","source":"$$v_t := \\eta v_{t-1} + \\alpha \\nabla J$$\n","metadata":{}},{"cell_type":"markdown","source":"$$W_t := W_{t-1}  - v_t$$\n","metadata":{}},{"cell_type":"markdown","source":"We can see that rather than just simply updating our weights with that gradient, we also look back to prior values $v_{t-1}$ to smooth out these steps. So this is at the step $t$ that we have, which will incorporate some amount of $v_{t-1}$ at step $t-1$. \n","metadata":{}},{"cell_type":"markdown","source":"![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/momentum.png)\n","metadata":{}},{"cell_type":"markdown","source":"As a summary, adding momentum in weights update may bring the following benefits:\n1. Smooth the weight updating process by incorporating previous direction\n2. Tend to generate larger and consistent steps which may tackle the saddle points problem\n","metadata":{}},{"cell_type":"markdown","source":"![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/jumpout.png)\n","metadata":{}},{"cell_type":"markdown","source":"Let's retrain our neural network with momentum. With `MLPClassifier` class, you just need to give a positive momentum value such as 0.9.\n","metadata":{}},{"cell_type":"code","source":"momentum_ann = MLPClassifier(random_state=123,  hidden_layer_sizes=(32, 8), \n                    solver='sgd', momentum=0.9, \n                    early_stopping=True,\n                    max_iter=100)\nfit_time, n_iter, train_score, test_score = fit_and_score(momentum_ann, X_train, X_test, y_train, y_test)\nprint(f\"Training converged after {n_iter} iterations with test score (accuracy) {round(test_score, 2)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, as you can see with momentum added in SGD process, our performance gets much better from 0.42 to 0.73. \n","metadata":{}},{"cell_type":"markdown","source":"### Exercise: Retrain the Neural Network with Nesterov’s momentum\n","metadata":{}},{"cell_type":"markdown","source":"However, sometimes adding momentum may overshoot the minima and make the training process longer. So we may add Nesterov’s momentum to further improve training process.\n","metadata":{}},{"cell_type":"markdown","source":"To use Nesterov’s momentum, you just need to to set argument `nesterovs_momentum=True` in `MLPClassifier` class.`\n","metadata":{}},{"cell_type":"code","source":"# Write your code here\nnesterovs_ann = MLPClassifier(random_state=123,  hidden_layer_sizes=(32, 8), \n                    solver='sgd', momentum=0.95, \n                    nesterovs_momentum=True,\n                    early_stopping=True,\n                    max_iter=100)\nfit_time, n_iter, train_score, test_score = fit_and_score(nesterovs_ann, X_train, X_test, y_train, y_test)\nprint(f\"Training converged after {n_iter} iterations with score (accuracy) {round(test_score, 2)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adam Optimizer\n","metadata":{}},{"cell_type":"markdown","source":"For efficient training, sometimes we may want the learning rates to be adaptive, i.e., large learning rate at the beginning and gets decaying over time. Adam combines the concepts of momentum we have seen previously and gradient decaying over time. As such, it could take very smooth steps and avoid overshooting the optima.\n","metadata":{}},{"cell_type":"markdown","source":"Adam is designed to improve the training process, it may either:\n- Decrease number of training iterations \n- Improve the model performance by finding a better or global optima\n","metadata":{}},{"cell_type":"markdown","source":"### Exercise: Retrain the Neural Network with Adam optimizer\n","metadata":{}},{"cell_type":"markdown","source":"To use Adam in `MLPClassifier` is also very simple, you just need to change the solver to be 'adam' (which is actually the default setting).\n","metadata":{}},{"cell_type":"code","source":"# Write your code here\nadam_ann = MLPClassifier(random_state=123,  hidden_layer_sizes=(32, 8), \n                    solver='adam',\n                    early_stopping=True,\n                    max_iter=100)\nfit_time, n_iter, train_score, test_score = fit_and_score(adam_ann, X_train, X_test, y_train, y_test)\nprint(f\"Training converged after {n_iter} iterations with score (accuracy) {round(test_score, 2)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You should see that Adam has the smallest iterations but best performance. \n","metadata":{}},{"cell_type":"markdown","source":"If you are interested in how Adam is implemented, you may refer to the following optional section.\n","metadata":{}},{"cell_type":"markdown","source":"## Optional: Implement Adam optimizer from scrach\n","metadata":{}},{"cell_type":"markdown","source":"The momentum or first-moment component:\n","metadata":{}},{"cell_type":"markdown","source":"$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla J$$\n","metadata":{}},{"cell_type":"markdown","source":"$m_t$ also need to be bias-corrected from being a very small number (as it is initialized with zero)\n","metadata":{}},{"cell_type":"markdown","source":"$$\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$$\n","metadata":{}},{"cell_type":"markdown","source":"The shrink or second-moment component:\n","metadata":{}},{"cell_type":"markdown","source":"$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) \\nabla^2 J$$\n","metadata":{}},{"cell_type":"markdown","source":"$v_t$ also need to be bias-corrected\n","metadata":{}},{"cell_type":"markdown","source":"$$\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}$$\n","metadata":{}},{"cell_type":"markdown","source":"$\\alpha$ is the initial learning rate or step size.\n","metadata":{}},{"cell_type":"markdown","source":"$\\beta_1$ and $\\beta_2$ are hyper-parameters normally with default initial values $\\beta_1=0.9$, $\\beta_2=0.9999$\n","metadata":{}},{"cell_type":"markdown","source":"$\\beta_1$ and $\\beta_2$ are also decaying with time: \n","metadata":{}},{"cell_type":"markdown","source":"$$\\beta_1 = \\beta_1^t$$\n","metadata":{}},{"cell_type":"markdown","source":"$$\\beta_2 = \\beta_2^t$$\n","metadata":{}},{"cell_type":"markdown","source":"Now, we can put them together into the weight update:\n","metadata":{}},{"cell_type":"markdown","source":"$$w_t = w_{t-1} - \\alpha \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + e}$$\n","metadata":{}},{"cell_type":"markdown","source":"Let's try to implement the above Adam formulas with a very simple cost function: \n","metadata":{}},{"cell_type":"markdown","source":"$J = (w-4)^2 + 2w$ \n","metadata":{}},{"cell_type":"markdown","source":"Its gradient with respect to `w` is:\n","metadata":{}},{"cell_type":"markdown","source":"$\\nabla J = 2(w-4) + 2$ \n","metadata":{}},{"cell_type":"code","source":"# cost function\ndef cost_function(w):\n    return (w - 4) ** 2 + 2*w\n\n## take derivative\ndef grad_function(w):\n    return 2*(w-4) + 2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's plot this cost function with the following helper function.\n","metadata":{}},{"cell_type":"code","source":"def plot_cost():\n    fig, axis = plt.subplots()\n    fig.set_size_inches(8, 6, forward=True)\n    fig.set_dpi(100)\n\n    x = np.linspace(0,6,100)\n    y = cost_function(x)\n    axis.plot(x, y, 'b')\n    axis.set_xlabel(\"Weight\")\n    axis.set_ylabel(\"Cost\")\n\nplot_cost()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_converged(w0, w1):\n    return abs(w0 - w1) <= 1e-6\n\n# Implement Adam\ndef adam(t, w, dw, m, v, alpha = 0.1, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n    # para\n    # First-moment\n    m = beta1*m + (1-beta1)*dw\n    # Second-moment\n    v = beta2*v + (1-beta2)*(dw**2)\n    # Bias correction\n    m_unbiased = m/(1-beta1**t)\n    v_unbiased = v/(1-beta2**t)\n    # Update weights\n    w = w - alpha*(m_unbiased/(np.sqrt(v_unbiased) + epsilon))\n    return w, m, v","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we could use the gradients and Adam to optimize the cost function:\n","metadata":{}},{"cell_type":"code","source":"w,m,v,t = 0,0,0,1\nconverged = False\nw_his = []\nloss_his = []\nwhile not converged:\n    dw = grad_function(w)\n    w_prev = w\n    w, m, v = adam(t, w, dw, m, v)\n    loss_his.append(cost_function(w))\n    w_his.append(w)\n    if is_converged(w, w_prev):\n        break\n    else:\n        t+=1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_cost_w_grad(w_his, loss_his):\n    fig, axis = plt.subplots()\n    fig.set_size_inches(8, 6, forward=True)\n    fig.set_dpi(100)\n    x = np.linspace(-2,6,100)\n    y = cost_function(x)\n    axis.scatter(w_his, loss_his, marker='<', color='r', s=18)\n    axis.plot(x, y, 'grey')\n    axis.text(0, 16, 'Iteration 0', fontsize=8)\n    axis.text(3, 8, f'Iteration {len(loss_his)}', fontsize=8)\n    axis.set_xlabel(\"Weight\")\n    axis.set_ylabel(\"Cost\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_cost_w_grad(w_his, loss_his)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see from above plot, the weights updated by Adam are gradually reaches to the global minima.\n","metadata":{}},{"cell_type":"markdown","source":"## Exercise: Comparing Optimization Methods on Fashion MNIST\n","metadata":{}},{"cell_type":"markdown","source":"In this exercise, we will build a simple neural network model to classify Fashion MNIST images.\n\nWe will use the 2 optimizers we discussed above, with 3 different settings. These include: SGD with disabled momentum, SGD with momentum and Adam. In all three settings we will use the same base model architecture to allow for direct comparison.\n","metadata":{}},{"cell_type":"code","source":"NAMES = ['SGD', 'SGD_momentum', 'Adam']","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us start by loading and splitting our data into testing and training sets.\n","metadata":{}},{"cell_type":"code","source":"# Write your code here\n\n# TODO: load and split data\n(xtrain, ytrain), (xtest, ytest) = fashion_mnist.load_data()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we will reshape our `xtrain` and `xtest` such that they're in the shape `(len(xtrain), -1)` and `(len(xtest), -1)`, respectively. We will also convert our target labels to categorical values using `to_categorical()`\n","metadata":{}},{"cell_type":"code","source":"# Write your code here\n\n# TODO: reshape xtrain and xtest\nxtrain = np.reshape(xtrain, (len(xtrain), -1))\nxtest = np.reshape(xtest, (len(xtest), -1))\n\nytrain = to_categorical(ytrain)\nytest = to_categorical(ytest)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the next step, we will perform basic normalization. We will also carve out a small validation set out of our original training dataset.  \n","metadata":{}},{"cell_type":"code","source":"# normalization\nxtrain = np.apply_along_axis(lambda x: x/255, 1, xtrain)\nxtest = np.apply_along_axis(lambda x: x/255, 1, xtest)\n\n# validation set\nindex = 50000\nxval, yval = xtrain[index:], ytrain[index:]\nxtrain, ytrain = xtrain[:index], ytrain[:index]\nxtrain.shape, xval.shape, xtest.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us define a simple sequential neural network model. In the cell below set the input shape of the first Dense layer to `xtrain.shape[1:]`. Finally, build the model with the same input shape.\n","metadata":{}},{"cell_type":"code","source":"# Write your code here\nmodel = Sequential(\n[\n    Dense(128, \n          # TODO: input_shape=,\n          activation='relu', \n          name='dense_1'),\n    Dense(64, \n          activation='relu', \n          name='dense_2'),\n    Dropout(0.2),\n    Dense(10, \n          activation='softmax', \n          name='dense_3')\n], name='Sequential')\n\n# TODO: model.build(input_shape=)\nmodel.summary()","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will be using a popular Python library called `tqdm` to keep track of the training of our 3 models. Each model will be trained for 10 epochs, and for each iteration we will save the results in separate dictionary objects.\n","metadata":{}},{"cell_type":"code","source":"# params\nepochs = 10\nbatch_size = 64\nshuffle = True\n\n# dicts for storing results\nloss      = {opt:[] for opt in NAMES}\nval_loss  = {opt:[] for opt in NAMES}\nacc       = {opt:[] for opt in NAMES}\nval_acc   = {opt:[] for opt in NAMES}\ntest_acc  = {}\ntest_loss = {}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the cell below, define three different types of optimizers with the following settings: **(Note that this may take a while to run (10+ minutes))**\n    \n* SGD: lr = 0.001\n* SGD: lr = 0.001, momentum = 0.9\n* Adam: lr = 0.001, beta_1 = 0.9, beta_2 = 0.999 \n","metadata":{}},{"cell_type":"code","source":"\nweights = model.get_weights().copy()\n\nwith tqdm(desc='Training', total=len(NAMES*epochs)) as pbar:\n    for name in NAMES:\n        optimizer=''\n        \n        # prepare model\n        model.set_weights(weights)\n        if name == 'SGD':\n            optimizer= SGD(lr=0.001)\n        elif name=='SGD_momentum':\n            optimizer=SGD(lr=0.001, momentum=0.9)\n        elif name=='Adam':\n            optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n        print('Optimizer: ', name)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n        \n        # train model\n        for epoch in range(epochs):\n            his = model.fit(xtrain, ytrain,\n                            epochs=1,\n                            batch_size=batch_size,\n                            validation_data=(xval, yval),\n                            shuffle=shuffle,\n                            verbose=0)\n            \n            # update dictionaries\n            loss[name].append(his.history['loss'][0])\n            val_loss[name].append(his.history['val_loss'][0])\n            acc[name].append(his.history['acc'][0])\n            val_acc[name].append(his.history['val_acc'][0])   \n            pbar.update(1)\n            \n        # inference\n        t_loss, t_acc = model.evaluate(xtest, ytest, verbose=0)\n        test_loss[name] = t_loss\n        test_acc[name] = t_acc\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, let us visualize the results we saved during the training procedure.\n","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(13, 7))\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.2)\n\n\nfor index, result, title in zip([[0, 0], [0, 1], [1, 0], [1, 1]], \n                                [loss, val_loss, acc, val_acc], \n                                ['loss', 'val_loss', 'acc', 'val_acc']):\n    i, j = index\n    for name, values in result.items():\n        axs[i, j].plot(values, label=name)\n        axs[i, j].set_title(title, size=15)\n        axs[i, j].set_xticks([e for e in range(epochs)])\n        axs[i, j].legend(loc=\"best\", prop={'size': 10})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As seen above, SGD with momentum outperforms SGD without momentum. In this case, Adam performs the best. In fact, it works well in most cases so it often set to be the default optimizer for many neural network models.\n","metadata":{}},{"cell_type":"markdown","source":"Let us look at a few images from our test dataset and see how our model classifies them.\n","metadata":{}},{"cell_type":"code","source":"w, h = 28, 28\nfashion_mnist_labels = [\"T-shirt/top\",  # index 0\n                        \"Trouser\",      # index 1\n                        \"Pullover\",     # index 2 \n                        \"Dress\",        # index 3 \n                        \"Coat\",         # index 4\n                        \"Sandal\",       # index 5\n                        \"Shirt\",        # index 6 \n                        \"Sneaker\",      # index 7 \n                        \"Bag\",          # index 8 \n                        \"Ankle boot\"]   # index 9\n\ny_hat = model.predict(xtest)\n\nfigure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(range(16)):\n    \n    ax = figure.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(xtrain.reshape(xtrain.shape[0], w, h, 1)[index]))\n    predict_index = np.argmax(y_hat[index])\n    true_index = np.argmax(ytest[index])\n    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index],\n                                  fashion_mnist_labels[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))\n    ","metadata":{},"outputs":[],"execution_count":null}]}