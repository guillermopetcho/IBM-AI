{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":""},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# __Autoencoder with Python__\n\nEstimated time needed: **60** minutes\n","metadata":{"id":"Phy2dbedLTl4"}},{"cell_type":"markdown","source":"In this lab, we will first review how to build a shallow autoencoder using the Keras functional API and the Model subclassing. Then, we will review some applications of autoencoders such as image denoising and image compression. Lastly, we will use what we have learned in the later sections to build deeper autoencoders.\n\nAs the figure shows below, when used on images, autoencoders can not only return the reconstructed image but also return a compressed version of it. The compressed data is very useful as it achieves the goal of dimensionality reduction while retaining meaningful information from the image. We will soon dive deep into how different types of autoencoders help us realize all these goals!\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/intro_pic.png\" width=\"65%\" style=\"vertical-align:middle;margin:30px 0px\"></center>\n\nImage credits to [Yifei Zhang](https://www.semanticscholar.org/paper/A-Better-Autoencoder-for-Image%3A-Convolutional-Zhang/b1786e74e233ac21f503f59d03f6af19a3699024?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01)\n","metadata":{"id":"g6BXnBV1LTl6"}},{"cell_type":"markdown","source":"## Table of Contents\n\n<ol>\n    <li><a href=\"#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n        </ol>\n    </li>\n    <li><a href=\"#Introduction\">Introduction</a></li>\n    <li>\n        <a href=\"#Shallow Autoencoders\">Shallow Autoencoders</a>\n        <ol>\n            <li><a href=\"#Autoencoders using the Functional API \">Autoencoders using the Functional API </a></li>\n            <li><a href=\"#Autoencoders using Model Subclassing\">Autoencoders using Model Subclassing </a></li>\n        </ol>\n    </li>\n    <li><a href=\"Applications of Autoencoders\">Applications of Autoencoders</a>\n        <ol>\n            <li><a href=\"#Anomaly Detection with Auto-Encoders\">Anomaly Detection with Auto-Encoders</a></li>\n            <li><a href=\"#Denoising Autoencoders\">Denoising Autoencoders</a></li>\n             <li><a href=\"#Exercise 1\">Exercise 1</a></li>\n             <li><a href=\"#Exercise 2\">Exercise 2</a></li>\n        </ol>           \n    </li>\n    <li><a href=\"Deep Autoencoders\">Deep Autoencoders</a>\n        <ol>\n            <li><a href=\"#Loading Images From a Directory for Autoencoders\">Loading Images From a Directory for Autoencoders</a></li>\n        </ol>     \n    </li>      \n    <li><a href=\"#Convolutional Autoencoders\">Convolutional Autoencoders</a></li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab, you will be able to:\n\n - Apply Autoencoders to real world problems \n - Implement different autoencoder architecture\n - Train different autoencoders \n","metadata":{"id":"VhtVTt7VLTl7"}},{"cell_type":"markdown","source":"----\n","metadata":{"id":"1vEj5eQkLTl8"}},{"cell_type":"markdown","source":"## Setup\n\nFor this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","metadata":{"id":"BhWvaYBlLTl8"}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Anaconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n","metadata":{"id":"r_puaFgwLTl8","tags":[]}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Upgrade the tensorflow and skillsnetwork library:\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install --upgrade tensorflow\n!pip install --upgrade skillsnetwork","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n*We recommend you import all required libraries in one place (here):*\n","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\nimport skillsnetwork\n\nimport numpy as np\nfrom numpy.core.fromnumeric import reshape\n\nimport tensorflow as tf\n    \nimport keras\nfrom keras import layers,Input,Sequential \nfrom keras.layers import Dense,Flatten,Reshape,Conv2DTranspose,Conv2D\nfrom keras.models import Model\n\n\nfrom keras.layers import Conv2D\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt","metadata":{"id":"3Jc2ncJxLTl8","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining Helper Functions\n","metadata":{}},{"cell_type":"code","source":"def plot_images(top,bottom,start=0,stop=5,reshape_x=(28,28),reshape_xhat=(28,28)):\n    \n    '''\n    this function plots images from the start index to the stop index from two datasets\n    \n    '''\n\n    n_samples=stop-start\n\n    for i,img_index in enumerate(range(start,stop)):\n        \n        # Display original\n        ax = plt.subplot(2, n_samples, i + 1)\n        plt.imshow(top[img_index].reshape(reshape_x[0], reshape_x[1]), cmap=\"gray\")\n\n        if i==n_samples//2:\n            plt.title(\"original images\")\n\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n        # Display reconstruction\n        ax = plt.subplot(2, n_samples, i + 1 + n_samples)\n        plt.imshow(bottom[img_index].reshape(reshape_xhat[0], reshape_xhat[1]), cmap=\"gray\")\n\n\n        if i==n_samples//2:\n            plt.title(\"encoded images\")\n\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)","metadata":{"id":"Uh8HKeYYkCyi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def graph_history(history, title='Log Loss and Accuracy over iterations'):\n\n    fig = plt.figure(figsize=(16, 8))\n    fig.suptitle(title)\n    N_plots=len(history.history.keys())\n    color_list=['b', 'r', 'g', 'c', 'm', 'y', 'k', 'w','bx','rx']\n    for i,(key, items) in enumerate(history.history.items()):\n        ax = fig.add_subplot(1, N_plots, i+1)\n        ax.plot(items,c=color_list[i])\n        ax.grid(True)\n        ax.set(xlabel='iterations', title=key)","metadata":{"id":"aJ4XeGxxLXHw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_noise(x_train, x_test, noise_factor = 0.3):\n    '''\n    this function adds random values from a normal distribution as noises to the data \n    \n    returns the noisy datasets \n    \n    '''\n    noise_factor = 0.3\n    x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape) \n    x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape) \n\n    x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.).numpy()\n    x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.).numpy()\n    \n    return x_train_noisy,x_test_noisy","metadata":{"id":"G79t0tXRRsPU"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_code(h, y, numbers=[0,1,2,3,4,5,6,7,8,9], scale=[1,1,1]):\n    \"\"\"\n    number: list of classes to be plotted \n    scale: scale activations to plot better \n    \n    \"\"\"\n    h=h.numpy()\n    color_list=['b', 'g', 'r', 'c', 'm', 'y', 'k', 'pink','darkorange','lime']\n    logic_array =np.zeros(len(y), dtype=bool)\n    \n    fig=plt.figure(figsize=(6,6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    for num, color in zip(numbers, color_list):\n        logic_array = (y==num)\n        plt.scatter(scale[0]*h[logic_array,0],scale[1]*h[logic_array,1],scale[2]*h[logic_array,2],c=color, label=num)\n \n    plt.title(\"3D output of encoder, colored by digits\")\n    plt.legend(loc=[1.1,0.3])\n    plt.show()","metadata":{"id":"6-vWPAz-CBMg"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def avg(shape, dtype=None):\n    grad = np.array([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]\n        ]).reshape((3, 3, 1, 1))/9\n    \n    assert grad.shape == shape\n    return keras.backend.variable(grad, dtype='float32')\n\na_conv = layers.Conv2D(filters=1,\n                       kernel_size=3,\n                       kernel_initializer=avg,\n                       strides=1,\n                       padding='same')\n\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_auto(Xiter,n=1,B=1):\n\n    \"\"\"\n    This function displays the ouput of an autoencder \n    Returns:  void images \n    Args: \n    Xiter:image_dataset_from_directory object converted to iterator \n    n:sample within batch \n    B: batch number \n    \"\"\"\n\n    for b in range(B):    \n        x = next(Xiter)\n    \n        plt.imshow(x[1].numpy()[n,:,:,0],cmap=\"gray\")\n        plt.title(\"input\")\n        plt.show()\n        plt.imshow(x[0].numpy()[n,:,:,0],cmap=\"gray\")\n        plt.title(\"output\")\n        plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Introduction\n","metadata":{"id":"MiJVJzxKLTl-"}},{"cell_type":"markdown","source":"An autoencoder is a neural network that minimizes the difference between the input and the output. The network has two parts: the **encoder** part which outputs the code $\\textbf{Z}$ using the message $\\textbf{X}$ such that $\\textbf{Z} = \\textbf{E(X)}$. The code $\\textbf{Z}$ is then passed along to reconstruct the message $\\textbf{X}$, and the **decoder** is the second part does the reconstruction, which produces $\\hat{\\textbf{X}}= \\textbf{D(Z)}$.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/flow.png\" width=\"60%\" style=\"vertical-align:middle;margin:50px 0px\"></center>\n\nIn general $\\textbf{D(E(X))} = \\hat{\\textbf{X}}$ such that $\\hat{\\textbf{X}}$ is an approximation of $\\textbf{X}$. This forces the model to learn useful properties of the data, which would be useful in applications such as image enhancement and outlier detection. \n","metadata":{"id":"q6-MU_LELTl-"}},{"cell_type":"markdown","source":"## Shallow Autoencoders \n\nThey are the simplest autoencoders, which typically consist of a one-layer encoder followed by a one-layer decoder. The encoder can have an activation function or not. Similarly, the decoder can have a real output or a bounded output, for example, between 0 and 1.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/shallow_model.png\" width=\"45%\" style=\"vertical-align:middle;margin:30px 0px\"></center>\n\n\nLet's use the MNIST data from `keras.datasets` as a beginning example:\n","metadata":{"id":"6Na94QY_t4UP"}},{"cell_type":"code","source":"from keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) =keras.datasets.mnist.load_data()\n\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5WKNtqJ_zmS","outputId":"224753cd-cfef-43d2-bb1b-02ba4ab0c7c8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we introduce two ways to build shallow autoencoders in Keras, one is using the **Keras functional API**, and the other one is **subclassing the Keras Model class**. \n","metadata":{}},{"cell_type":"markdown","source":"### Autoencoders using the Functional API \n\nThe functional interface **uses the layers as functions**, taking a tensor as input and outputting a tensor instead of stacking layers on top of one another (like the Sequential class in Keras). By doing so, your way of building up the model layers is **more flexible** in the sense that your **model can take inputs from multiple paths and then add the tensors together at some point**.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/functional_API.png\" width=\"30%\" style=\"vertical-align:middle;margin:0px 0px\"></center>\n","metadata":{"id":"_dEPHrwEuJY2"}},{"cell_type":"markdown","source":"We create an input object, ```input_img```, which allows us to input tensors into our model object; as each image is 784 dimensional, the input shape is (784, ).\n","metadata":{"id":"k2jJJs1YEWzt"}},{"cell_type":"code","source":"input_img=Input(shape=(784,))","metadata":{"id":"0DtBUvc8EZ_V"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create an encoder layer using a Dense layer and the Relu activation. The encoding dimension will be the output dimension of the Dense layer. We also call the object ``input_img`` to specify the input of the encoding layer.\n","metadata":{"id":"Wv4rJXDmFHHQ"}},{"cell_type":"code","source":"encoding_dim=36\nencoder = Dense(encoding_dim, activation='relu')(input_img)","metadata":{"id":"CrNb78xi_zmT"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output dimension of the image. We create a decoding layer using a Dense layer; as the image values are between 0 to 1, we use the sigmoid activation for the output. We also call the object ```encoder``` to specify the input of the decoding layer, and this layer can also be linear.\n","metadata":{"id":"cGZRnCeALOxr"}},{"cell_type":"code","source":"decoder=Dense(784,activation='sigmoid')(encoder)","metadata":{"id":"ktAQILkP_zmT"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We combine the encoder and decoder using the model class for training and inference, specifying the input ```input_img``` and output  ```decoder```\n","metadata":{"id":"05D2ZiMrLZEH"}},{"cell_type":"code","source":"autoencoder =Model(input_img, decoder)","metadata":{"id":"X1hvc8VfLVZE"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We compile the model using the adam optimizer and the cross entropy loss, and you can also use the mean square error as the loss function.\n","metadata":{"id":"1yIHfAILNjm5"}},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')","metadata":{"id":"zmsc1LOcNCd6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We now fit the model; if you recall, in the supervised case, the first two inputs are (y_true, y_pred), where y_true are the ground truth values, and y_pred is the predicted value. As the autoencoder tries to replicate the input, y_true and y_pred are the input x, similarly for the validation data. The rest of the parameters behave similarly. \n","metadata":{"id":"sv_chNH6OLUN"}},{"cell_type":"code","source":"history=autoencoder.fit(x_train, x_train,\n                epochs=25,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"er6E40nTNgLk","outputId":"870de3a3-cf5b-47e9-de29-855a021da7a7","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot the loss for each iteration:\n","metadata":{"id":"o_QvTirAAGSX"}},{"cell_type":"code","source":"graph_history(history, title='Log Loss and Accuracy over iterations')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"id":"f01R8qJu94PV","outputId":"f06dffa9-1816-4c13-ba3a-e7edb941c506"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the loss is lower; furthermore, the training data is similar to the testing data. This suggests the Autoencoder does well at modeling the data.\n","metadata":{}},{"cell_type":"markdown","source":"We can make a prediction using the method ```predict```\n","metadata":{"id":"XuFNuqYeBjNs"}},{"cell_type":"code","source":"xhat=autoencoder.predict(x_test)","metadata":{"id":"M7am9V8E_zmU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can compare the output of the autoencoder with the original input. We see they are pretty similar:\n","metadata":{"id":"in0bC5pHByHl"}},{"cell_type":"code","source":"plot_images(x_test,xhat,start=0,stop=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"dPwheWBW_zmU","outputId":"23f07261-9d0a-47ab-a6bf-cd49ce1d069e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Autoencoders using Model Subclassing\n\nKeras also provides an object-oriented approach to creating models, where we can subclass the `keras.Model` class to have our own class of Autoencoder models inherit or override the properties and methods of `keras.Model`. This subclassing approach **helps with reusability and allows you to represent the models you want to create as classes**. \n\nIt's easier to use model subclassing to build autoencoders with the following boilerplate. Let's create an ```Autoencoder``` class using the `Model` class from `keras.models` as the parent class.\n","metadata":{"id":"WmxbUO-NDIlR"}},{"cell_type":"code","source":"class Autoencoder(tf.keras.Model):\n    def __init__(self, latent_dim):\n        super(Autoencoder, self).__init__()\n        self.latent_dim = latent_dim   \n        self.encoder = Dense(latent_dim, activation='relu')\n        self.decoder = Dense(784, activation='sigmoid')\n\n    def call(self, x):\n        \n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded","metadata":{"id":"L70NntCilriQ","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```latent_dim``` is the size of the input, and the encoder is  given by \n\n```encoder = tf.keras.Sequential([layers.Dense(latent_dim, activation='relu')])```\n\nThe decoder is given by:\n\n```decoder = tf.keras.Sequential([layers.Dense(784, activation='sigmoid')])```\n\nThe method ```call``` will make a prediction and will be called when the method ```predict``` is called.\n","metadata":{"id":"tJKerKtLNJc0"}},{"cell_type":"markdown","source":"With subclassing, we can now create an ```autoencoder``` instance and the only parameter that needs to be specified is the ```encoding_dim```.\n","metadata":{"id":"pCgNPJwdRq9l"}},{"cell_type":"code","source":"encoding_dim=36\nautoencoder = Autoencoder(encoding_dim)","metadata":{"id":"NM9winFLuihc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `autoencoder` instance created will not only have the attributes and methods defined within the `autoencoder` class but also inherits all the attributes of the `Model` class.\n","metadata":{}},{"cell_type":"markdown","source":"All the steps are the same as above to fit the model and make a prediction:\n","metadata":{"id":"MAbQ2hbcWQYM"}},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nhistory=autoencoder.fit(x_train, x_train,epochs=25,batch_size=256,shuffle=True,validation_data=(x_test, x_test))\ngraph_history(history, title='Log Loss and Accuracy over iterations')","metadata":{"id":"9zSyyAVTMCCm","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's compare the images reconstructed by the trained autoencoder and the original image:\n","metadata":{}},{"cell_type":"code","source":"xhat=autoencoder.predict(x_test)\nplot_images(x_test,xhat,start=100,stop=105)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also generate the output of the encoder by calling the method ```encoder```, which returns the activation of the encoder layer:\n","metadata":{"id":"LFdrK_KmVDO9"}},{"cell_type":"code","source":"h=autoencoder.encoder(x_test)\nh.shape","metadata":{"id":"fltEudE4VA4y"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the output dimension of the `encoder` layer is 36, we will need to reshape it to $6\\times 6$ in order to plot it as images:\n","metadata":{"id":"CUSN-Iz9aVcH"}},{"cell_type":"code","source":"plot_images(x_test, h.numpy(),start=200,stop=205,reshape_x=(28,28),reshape_xhat=(6,6))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"3iAdyA9rOLd4","outputId":"b72c7682-e647-4f1a-f798-84d5b2b32782","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Should you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier, and safer, and has a number of features that subclassed models do not support.\n\nHowever, Model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass the Model directly.\n\nAs a last note, choosing between the functional API or Model subclassing isn't a binary decision that restricts you into one category of models. **All models in the `tf.keras` API can interact with each other, whether they're Sequential models, functional models, or subclassed models that are written from scratch!**\n","metadata":{}},{"cell_type":"markdown","source":"Now you know how an autoencoder is constructed and how it works, let's see some real-world applications of autoencoders!\n","metadata":{"id":"SXRIwwGepGY4"}},{"cell_type":"markdown","source":"## Applications of Autoencoders\n","metadata":{"id":"931ufueW3hEE"}},{"cell_type":"markdown","source":"### Anomaly Detection with Autoencoders\nDue to the autoencoder architecture (encoder + decoder), autoencoders are forced to learn representations of the images. As a result, only data samples with a similar distribution can be reconstructed with fidelity. This means a high reconstruction loss can be interpreted and used as a method to detect anomalous samples within the data.\n\nConsider the last example. We can find anomalies of the data as follows:\n\nFirst, we calculate the loss between the predicted samples and the original data. Samples reconstructed with the highest error values (losses) are the anomalies. \n","metadata":{"id":"FXDvB-WaUtnW"}},{"cell_type":"markdown","source":"We caculate the loss for each of the samples: \n","metadata":{"id":"eMQcemkotF-L"}},{"cell_type":"code","source":"bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"id":"xVcm9b2MqcPK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss=[bce(x, x_s).numpy() for x, x_s in zip (x_test,xhat)]","metadata":{"id":"4VY0tIsSqfnj"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's sort the losses with **np.argsort** and then use **np.flip** to flip the order of the indexes so that the losses are sorted in descending order:\n","metadata":{"id":"btrR8qCOvi0j"}},{"cell_type":"code","source":"indexs=np.flip(np.argsort(loss))","metadata":{"id":"n2X6bZfirMY8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We plot the samples with the top ten losses:\n","metadata":{"id":"LWewLVHkvmO2"}},{"cell_type":"code","source":"plt.figure(figsize=(18,3))\nfor i, index in enumerate(indexs[0:10]):\n\n    plt.subplot(1, 10, i+1)\n    plt.imshow(x_test[index].reshape(28,28))\n    plt.title(f\"No.{index}\")\n    plt.axis(\"off\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cTweF_WkPDyR","outputId":"0e2c8d40-f3ee-4ea7-ff00-97a236baf295"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Denoising Autoencoders\n\nWe can also use Autoencoders for denoising. The following figure illustrates what we would like a denoising autoencoder to achieve:\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/denoising.png\" width=\"40%\" style=\"vertical-align:middle;margin:15px 0px\"></center>\n\n\nLet's first add some artificial noises to our digit dataset using the helper function **add_noise**.\n","metadata":{"id":"TROZjBbE3seX"}},{"cell_type":"code","source":"x_train_noisy,x_test_noisy= add_noise(x_train, x_test,noise_factor = 0.4)","metadata":{"id":"r9mJBcyK33Wm"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can display the training images before and after adding the noises:\n","metadata":{"id":"GuTXy0NSSdNJ"}},{"cell_type":"code","source":"fig1 = plt.figure(figsize=(10,2))\nfig1.suptitle(\"original images\")\nfig2 = plt.figure(figsize=(10,2))\nfig2.suptitle(\"noisy images\")\n\nfor i, img_index in enumerate(range(5)):\n    ax1 = fig1.add_subplot(1, 5, i+1)\n    ax1.imshow(x_train[img_index].reshape((28,28)))\n    ax1.axis(\"off\")\n    ax2 = fig2.add_subplot(1, 5, i+1)\n    ax2.imshow(x_train_noisy[img_index].reshape((28,28)))\n    ax2.axis(\"off\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"k9zlbK6o4oPz","outputId":"2057bb74-bdec-41e0-cdc5-be1cf3aa445f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One way to build a **denoising Autoencoder** is to make the encoding dimension larger than the input dimension; Here, we create an ```autoencoder``` object with an encoding dimension **two times** the input dimension:\n","metadata":{"id":"8TteEHL07sc4"}},{"cell_type":"code","source":"encoding_dim=2*x_test.shape[1]\nautoencoder = Autoencoder(encoding_dim)","metadata":{"id":"7HF-sQa-4oeU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We fit the model, for the fit method (y_true, y_pred), is the noisy data $x_{noisy}$ is the ground truth values, and y_pred is x_train the regular data $x$. The assumption is that where $x_{noisy}$ is a copy of $x$ with some form of noise. Training forces the encoder and decoder to learn the structure of the data implicitly.\n","metadata":{"id":"QgzMW3vl8ofv"}},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nhistory=autoencoder.fit(x_train_noisy , x_train,epochs=25,batch_size=256,shuffle=True,validation_data=(x_test_noisy, x_test))\ngraph_history(history, title='Log Loss and Accuracy over iterations')\nxhat=autoencoder.predict(x_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6JmFN_fC4XSr","outputId":"ebfb1976-7833-472e-9083-1f3ff2401079","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we plot the images, and we can see that most of the noises are gone:\n","metadata":{"id":"v7i2or4VBmnQ"}},{"cell_type":"code","source":"fig1 = plt.figure(figsize=(10,2))\nfig1.suptitle(\"noisy images\")\nfig2 = plt.figure(figsize=(10,2))\nfig2.suptitle(\"de-noised images\")\n\nfor i, img_index in enumerate(range(5)):\n    ax1 = fig1.add_subplot(1, 5, i+1)\n    ax1.imshow(x_test_noisy[img_index].reshape((28,28)))\n    ax1.axis(\"off\")\n    ax2 = fig2.add_subplot(1, 5, i+1)\n    ax2.imshow(xhat[img_index].reshape((28,28)))\n    ax2.axis(\"off\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"IlrYPh3r71Ao","outputId":"5291983b-a8bd-46a3-baa2-64a7cf088ebc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1 \nIn this exercise, you will use the encoder part of an Autoencoder for low-dimensional data visualization. \n\n- Create an autocoder using the class `Autoencoder`, with latent dimension of 3; \n- Fit the autoencoder model using the training set `x_train` and validate using the test set `x_test`; \n- Apply the `.encoder` method of the autoencoder on the test set `x_test`;\n- Display the output using the **plot_code** helper function.\n","metadata":{"id":"QiB9zhPrFpBO"}},{"cell_type":"code","source":"# write your own code here according to the instructions above\n\nh= # output of the encoder","metadata":{"id":"SViBQendAY3F","tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details>\n    <summary>Click here for Solution</summary>\n\n```python\nautoencoder = Autoencoder(3)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nhistory=autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))\nh=autoencoder.encoder(x_test)\n```\n\n</details>\n","metadata":{"id":"ZGT3AP7fGe_N"}},{"cell_type":"code","source":"plot_code(h,y_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 2\n","metadata":{"id":"O26zrfIkG1nJ","tags":[]}},{"cell_type":"markdown","source":"In this exercise, you will work with a different dataset, which is the fashion MNIST dataset downloaded from Keras. You will use an autoencoder to try to reconstruct the images and analyze the results.\n","metadata":{}},{"cell_type":"code","source":"(x_train, y_train), (x_test,y_test) = keras.datasets.fashion_mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\nprint(x_train.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlyt1c5HG11_","outputId":"e8cb2be7-fa29-4017-f9e7-fa2782b8beee"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's flatten the training and validation data and pass it to the Autoencoder:\n","metadata":{"id":"wVa8-c95H29Z"}},{"cell_type":"code","source":"x_temp=layers.Flatten()(x_train)\nx_temp_test=layers.Flatten()(x_test)\nx_temp.shape, x_temp_test.shape","metadata":{"id":"BNXXPxX-It9B"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoding_dim=3\nautoencoder = Autoencoder(encoding_dim)\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nhistory=autoencoder.fit(x_temp,x_temp,epochs=25,batch_size=256,shuffle=True,validation_data=(x_temp_test,x_temp_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2DvObvpH2M3","outputId":"4159d03f-2972-42e9-c6f2-5b459c744738"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looking at the output, we see the autoencoder can't reconstruct the data very well as it's too complex; Therefore, we need a better approach to represent more complex data.\n","metadata":{}},{"cell_type":"code","source":"xhat=autoencoder.predict(x_temp_test)\nplot_images(x_test,xhat,start=0,stop=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"a2SDdK6GS6r8","outputId":"e4ed5cbb-6590-447a-ee22-f863555b27a8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deep Autoencoders \n","metadata":{"id":"6ZQW-K49IoMY"}},{"cell_type":"markdown","source":"Like deep neural networks, adding more layers to the encoder and decoder will grant your autoencoders more representational power, allowing you to solve increasingly complex problems. For example, the reconstructed images of the fashion MNIST dataset plotted above did not retain enough features of the original images, so people will instantly recognize a shoe by looking at the first reconstructed image. Analogous to the fact that deep neural networks can learn and extract more complex features from the data than shallow neural networks, a deep autoencoder will also be able to do a better job in tasks that involve complex data than shallow autoencoders.\n\nLet's create a deep autoencoder class, analogous to creating a deep neural network:\n","metadata":{"id":"Kvi4EjOTLR03"}},{"cell_type":"code","source":"class Deep_Autoencoder (Model):\n    def __init__(self, latent_dim_1, latent_dim_2):\n        super(Deep_Autoencoder, self).__init__()\n        self.latent_dim_1= latent_dim_1  \n        self.latent_dim_1= latent_dim_2 \n        self.encoder = Sequential([layers.Flatten(),Dense(latent_dim_1, activation='relu'),Dense(latent_dim_2, activation='relu')])\n        self.decoder = tf.keras.Sequential([Dense(latent_dim_1, activation='relu'), Dense(784, activation='sigmoid'), Reshape((28, 28))])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"12bQD6W-G184","outputId":"869e764b-2a82-4d85-dff2-4686347aec7c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have the encoder:\n\n```encoder = Sequential([layers.Flatten(),Dense(latent_dim_1, activation='relu'),Dense(latent_dim_2, activation='relu')])```\n\nWe add a flattening layer for convenience as we will input rectangular image tensors. The encoder consists of two linear layers with Relu activations. Each layer, in this case, gets consistently smaller.\n\nWe have the decoder: \n\n```decoder = tf.keras.Sequential([ Dense(latent_dim_1, activation='relu'),Dense(784, activation='sigmoid'),Reshape((28, 28))])```\n\nwhich consists of two linear layers. The first layer has a Relu activation; the second layer has a sigmoid activation. For convenience, we add a layer to reshape the output to a rectangular image.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/deep_ae.png\" width=\"70%\" style=\"vertical-align:middle;margin:15px 0px\"></center>\n\nBy doing so, we create a deep autoencoder object; We set the output shape of each layer smaller and smaller until the latent dimension reaches three. Let's train the model.\n","metadata":{}},{"cell_type":"code","source":"latent_dim_1 =128\nlatent_dim_2=3\ndeep_autoencoder=Deep_Autoencoder(latent_dim_1=latent_dim_1,latent_dim_2=latent_dim_2)\n\n\ndeep_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nhistory=deep_autoencoder.fit(x_train,x_train,epochs=50,batch_size=256,shuffle=True,validation_data=(x_test,x_test))","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We plot the output; we can see the reconstructed images look similar to the original images; this is even after the dimension has been reduced to three in the bottleneck.\n","metadata":{}},{"cell_type":"code","source":"xhat=deep_autoencoder.predict(x_test)\nplot_images(x_test,xhat,start=0,stop=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"beqFXohhGDep","outputId":"3a3f606a-2cb4-4b5a-80ef-ed3856e5dd71"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading Images From a Directory for Autoencoders \n","metadata":{}},{"cell_type":"markdown","source":"Loading images from a directory for Autoencoders is similar to that for neural networks, but there are also several small differences. Like neural networks we create an object using ```image_dataset_from_directory``` that consists of human faces.\n","metadata":{}},{"cell_type":"code","source":"dataset_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/face_data.zip\"\nawait skillsnetwork.prepare(dataset_url, overwrite=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_height=50\nimg_width=50\nbatch_size=100\ndata_dir_face=os.path.join(os.getcwd(), 'face_data')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"VNXOxQQNUCge","outputId":"5db59b9a-4ab6-41b8-ba19-b9db50c9bffe"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xface = tf.keras.utils.image_dataset_from_directory(\n  data_dir_face,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n    color_mode=\"grayscale\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quCiInlQWdNl","outputId":"178f03b5-0bb9-4289-a493-80c5ebcd58ac"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will copy the object as we will use it multiple times \n","metadata":{}},{"cell_type":"code","source":"X_face_copy=copy.copy(Xface)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unlike neural networks, autoencoders have no labels. Instead, the input and output are both a set of features $\\textbf{X}$. Therefore you have to map the output from the directory object from $(\\textbf{x},\\textbf{y})$ to $(\\textbf{x},\\textbf{x}$), using the following function:\n","metadata":{}},{"cell_type":"code","source":"def change_inputs(images, labels):\n  \n    return images, images","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To apply the transform to the image, we use the ```apply``` method to apply the function ```change_inputs``` :\n","metadata":{}},{"cell_type":"code","source":"X_face_1=Xface.map(change_inputs)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will convert the object to an iterator so we can cycle through the samples:\n","metadata":{}},{"cell_type":"code","source":"X_iter=iter(X_face_1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see each output is two tuples, each an array of 100 50x50 images \n","metadata":{}},{"cell_type":"code","source":"images1, images2 = next(X_iter)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"images1 shape {}, and images2 {}\".format(images1.shape, images2.shape))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot some of the images using the function ```display_auto``` where ```n``` is the sample in a batch and ```B``` is the batch number, assuming next has only been called once \n","metadata":{}},{"cell_type":"code","source":"display_auto(X_iter,n=1,B=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can also apply other transformations like normalizing the image \n","metadata":{}},{"cell_type":"code","source":"normalization_layer = tf.keras.layers.Rescaling(1./255)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In many cases we would like a corrupted version of the image $\\textbf{x},\\tilde{\\textbf{x}}$. Let's construct a from-the-directory object that can be used to train an autoencoder to deblur an image. In many applications like noise removal, you add noise to $\\textbf{x}$ and get $\\tilde{\\textbf{x}}$. In this case, the input  $\\tilde{\\textbf{x}}$ will be a blurred image.\n","metadata":{}},{"cell_type":"code","source":"def blur_image(images, labels):\n    \n    x = normalization_layer(images)\n    x_b=a_conv(x)\n    return x_b, x","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We normalize the image and then blur the image using a filter, we apply the function to the dataset object.\n","metadata":{}},{"cell_type":"code","source":"Xface=Xface.map(blur_image)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see the input image is blurred:\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"eq-HnrAl4yQl","outputId":"4f0f2743-54e5-4d02-a06b-de59b760d912"}},{"cell_type":"code","source":"Xiter=iter(Xface)\ndisplay_auto(Xiter,n=1,B=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convolutional Autoencoders\n","metadata":{}},{"cell_type":"markdown","source":"Analogous to Convolutional neural networks, there are Convolutional Autoencoders (CAEs). Compared to the Autoencoders with fully connected layers, Convolutional Autoencoders with Conv2D layers do a better job encapsulating the pixel data's underlying patterns.\n\nThe following is an illustration of the CAE's architecture:\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/CAE.png\" width=\"80%\" style=\"vertical-align:middle;margin:15px 0px\"></center>\n\nImage from [this article](https://ai.plainenglish.io/convolutional-autoencoders-cae-with-tensorflow-97e8d8859cbe?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01). \n\nWe can build a convolutional autoencoder class:\n","metadata":{}},{"cell_type":"code","source":"class CNN_Autoencoder(Model):\n    def __init__(self):\n        super(CNN_Autoencoder, self).__init__()\n        self.encoder = tf.keras.Sequential([\n            layers.Input(shape=(50, 50, 1)),\n            Conv2D(16, (3, 3), activation='relu', padding='same', strides=1),\n            Conv2D(8, (3, 3), activation='relu', padding='same', strides=1)])\n\n        self.decoder = tf.keras.Sequential([\n            Conv2DTranspose(8, kernel_size=3, strides=1, activation='relu', padding='same'),\n            Conv2DTranspose(16, kernel_size=3, strides=1, activation='relu', padding='same'),\n            Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jLDdyrCAYObm","outputId":"90fd6420-0ab5-4117-90b4-879b6f346e4f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The encoder consists of several convolution layers that will produce an activation map:\n\n```encoder = tf.keras.Sequential([layers.Input(shape=(50, 50, 1)),Conv2D(16, (3, 3), activation='relu', padding='same', strides=1),Conv2D(8, (3, 3), activation='relu', padding='same', strides=1)])```\n","metadata":{}},{"cell_type":"markdown","source":"The decoder consists of several transposed convolutional or de-convolutional layers; the final layer is a convolutional layer that combines the activation maps of the previous transposed convolutional layer to form an image:\n\n```decoder = tf.keras.Sequential([Conv2DTranspose(8, kernel_size=3, strides=1, activation='relu', padding='same'),Conv2DTranspose(16, kernel_size=3, strides=1, activation='relu', padding='same'),Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])```\n","metadata":{}},{"cell_type":"markdown","source":"Let's create a ```CNN_Autoencoder``` object\n","metadata":{}},{"cell_type":"code","source":"cnn_autoencoder_face=CNN_Autoencoder()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's train the CNN autoencoder and use it to de-blur an image by minimizing the difference between $\\tilde{\\textbf{x}}$ and $\\textbf{x}$. Note that for training, we pass in `Xface`, which can be seen as an image data generator that will feed our model pairs of blurred images and original image. We expect the model to learn and minimize the difference between the two and eventually be able to recover the original image from a blurred image.\n","metadata":{}},{"cell_type":"code","source":"cnn_autoencoder_face.compile(optimizer='adam',  loss='mse')\nhistory=cnn_autoencoder_face.fit(Xface,epochs=10)\n\ngraph_history(history, title='Log Loss and Accuracy over iterations')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WFes7WypGrzZ","outputId":"e8981d11-8772-4bb7-c0b6-cf3b51dab839"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We take some samples from the set of blurred images and use the trained autoencoder to deblur the images and compare them to the blurred images.\n","metadata":{}},{"cell_type":"code","source":"x = next(Xiter)\nXhat=cnn_autoencoder_face.predict(x[0])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image_b, image_db in zip(x[0].numpy()[0:5,:,:,0],Xhat[0:5,:,:,0]):\n    plt.imshow(image_b, cmap=\"gray\")\n    plt.title(\"blurred image\")\n    plt.show()\n    plt.imshow(image_db, cmap=\"gray\")\n    plt.title(\"de-blurred image\")\n    plt.show()","metadata":{},"outputs":[],"execution_count":null}]}