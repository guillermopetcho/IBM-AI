{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"a0684e88235f7928d65f730559b38003ae5d6fe2e56b1a5a3884713a639fe504","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Padding, Stride, Pooling, Activation**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: 50 minutes\n","metadata":{}},{"cell_type":"markdown","source":"In the image convolution lab, you saw how image convolutions could be used to detect features in an image, such as edges, corners, blobs, etc. However, it could be challenging to develop an intuition of how the shape of the kernel as well as other related configuratiton parameters of a Convolution layer would affect the shape of the output. \n\nIn CNN, having a concrete understanding of the size of the output of each layer is necessary. Hence, in this lab, we will dive into some of the important factors to consider when working with CNNs, namely, Padding, Stride, Pooling, and Activation.\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n<ol>\n    <li><a href=\"#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"#Padding and Stride\">Padding and Stride</a>\n        <ol>\n            <li><a href=\"#Background\">Background</a></li>\n            <li><a href=\"#How-does-Padding-work?\">How does Padding work?</a></li>\n            <li><a href=\"#Using Padding in Tensorflow.keras\">Using Padding in Tensorflow.keras</a></li>\n            <li><a href=\"#How-does-Stride-work?\">How does Stride work?</a>\n            <li><a href=\"#Example 1: Image Processing - Edge Detection\">Example 1: Image Processing - Edge Detection</a></li>\n        </ol>\n    </li>\n    <li><a href=\"Activation\">Activation</a>\n        <ol>\n            <li><a href=\"#Example 2: Feature Detection with Kernel and Activation\">Example 2: Feature Detection with Kernel and Activation</a></li>\n        </ol>           \n    </li>\n    <li><a href=\"Pooling\">Pooling</a>\n        <ol>\n            <li><a href=\"#What is Pooling?\">What is Pooling?</a></li>\n            <li><a href=\"#Example 3: Max or Average?\">Example 3: Max or Average?</a></li>      \n        </ol>     \n    </li>      \n    <li><a href=\"#Exercises\">Exercises</a>\n        <ol>\n            <li><a href=\"#Exercise 1 - Display some images\">Exercise 1 - Display some images</a></li>\n            <li><a href=\"#Exercise 2 - Set up a Convolution layer\">Exercise 2 - Set up a Convolution layer</a></li>\n            <li><a href=\"#Exercise 3 - Set up a Max Pooling layer\">Exercise 3 - Set up a Max Pooling layer</a></li>\n            <li><a href=\"#Exercise 4 - Create a deeper CNN with blocks\">Exercise 4 - Create a deeper CNN with blocks</a></li>\n            <li><a href=\"#Optional section\">Optional section</a></li>\n        </ol>\n    </li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab you will be able to:\n\n - Understand the use of padding and stride in CNN\n - Calculate the size of the output of a layer\n - Understand the necessity of activations\n - Describe the difference between Max pooling and Average Pooling\n","metadata":{}},{"cell_type":"markdown","source":"----\n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n","metadata":{}},{"cell_type":"code","source":"#!mamba install -qy tqdm","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade tensorflow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n_We recommend you import all required libraries in one place (here):_\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport numpy as np\nimport pandas as pd\nfrom itertools import accumulate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_digits, load_wine\n\nimport pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport PIL\nfrom PIL import Image, ImageOps\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, datasets\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import AveragePooling2D\n\nsns.set_context('notebook')\nsns.set_style('white')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining Helper Functions\n","metadata":{}},{"cell_type":"markdown","source":"The Sobel operator for edge detection **v_grad** and **h_grad** will be defined later.\n","metadata":{}},{"cell_type":"code","source":"# This function will allow us to easily plot data taking in x values, y values, and a title\ndef sobel(img, strides, padding, activation=None):\n    \n    input_layer = layers.Input(shape=(img_height, img_width, 1))\n\n    v_conv = layers.Conv2D(filters=1,\n                       kernel_size=3,\n                       kernel_initializer=v_grad,\n                       strides=strides,\n                       padding=padding,\n                       activation=None)\n    h_conv = layers.Conv2D(filters=1, \n                   kernel_size=3,\n                   kernel_initializer=h_grad,\n                   strides=strides,\n                   padding=padding,\n                   activation=None)\n    \n    v_model = keras.Sequential([input_layer, v_conv])\n    h_model = keras.Sequential([input_layer, h_conv])    \n    \n    out_d = h_model.layers[0].output_shape[1:]\n    Gx = h_model.predict(img).reshape(out_d)\n    Gy = v_model.predict(img).reshape(out_d)\n    G = np.sqrt(np.add(np.multiply(Gx, Gx), np.multiply(Gy, Gy)))\n    \n    return G","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Padding and Stride\n","metadata":{}},{"cell_type":"markdown","source":"### Background\n","metadata":{}},{"cell_type":"markdown","source":"In our previous lab where we introduced image convolutions, we mentioned that we tend to lose the pixels on the border of our input image because there are not enough pixels surrounding them to support the kernel matrix multiplications. This could be detrimental to our CNN model performance since we will increasingly lose information as we apply many successive convolution layers.\n\nTo avoid losing important information carried by border pixels and control the size of our convolution output at the same time, one solution would be to use **Padding** and **Stride** when shifting a kernel on the input image. \n","metadata":{}},{"cell_type":"markdown","source":"### How does Padding work?\n\nPadding essentially extends the perimeter of our image by adding extra pixels, commonly **zero-valued**, on the outer frame of the image. This is done so that the border pixels could also be at or near the center of the receptive field when the kernel window slides through, and the information is retained as useful features for the next step.\n\nThe following figure illustrates the case where we apply a $3\\times3$ kernel on a $3\\times3$ image array with 2 rows of padding (1 on top, 1 on bottom) and 2 columns of padding (1 on left, 1 on right):\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/pad1_blue_.jpg\" width=\"50%\"></center>\n\nAs the kernel window slides through the padded image, we will obtain an output image with the same size as the input, which is $3\\times3$. \n","metadata":{}},{"cell_type":"markdown","source":"In general, if we add a total of $p_r$ rows of padding and $p_c$ columns of padding to a $m\\times n$ input, the output shape after applying a $k\\times k$ kernel will be:\n\n$$(m+p_r-k+1)\\times (n+p_c-k+1)$$\n\nThis formula implies that if we want the size of input and output to be the same, we need to make sure that $p_r=k-1=p_c$. As we would pad on the left and right, top and bottom of an image, $p_r$ and $p_c$ are typically even numbers so that all the previous layer pixels would be symmetrical around the outout pixel. This is why we tend to choose odd kernel sizes, such as $3\\times3$, $5\\times5$, etc.\n","metadata":{}},{"cell_type":"markdown","source":"### Use Padding in Tensorflow.Keras\n\nIn Keras, the operation of padding is specified via the `padding` argument in `keras.layers.Conv2D`. The default is `padding = 'valid'`, which means no padding. Setting `padding = 'same'` will calculate the required padding size such that the input and output will have the same shape and also add the padding, given an input and a kernel size.\n\nThe example below adds padding to a convolutional layer:\n","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 1,\n                 kernel_size = (3,3),\n                 padding = 'same',\n                 input_shape = (10, 10, 1)))\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So we built a simple CNN model with one convolution layer, where we specified an input shape of $10\\times10$ with one channel and the kernel applied is $3\\times3$. Via padding, the output shape is also $10\\times10$!\n","metadata":{}},{"cell_type":"markdown","source":"### How does Stride work?\n","metadata":{}},{"cell_type":"markdown","source":"In the previous example, we defaulted to shifting the kernel window one pixel at a time across the image left to right, top to bottom. However, sometimes for computational efficiency or because we simply wish to downsample, we could choose to move the kernel window more than one pixel at a time. \n\nThe number of rows and columns traversed per slide is referred to as the **Stride**. In Keras, we could specify the stride in `keras.layers.Conv2D` via the `strides` argument, as a tuple of 2 integers. For example, `strides = (2,3)` means we shift the kernel 2 pixels right for each horizontal movement and 3 pixels down for each vertical movement. \n\n**Combining the concept of Padding and Stride**, if we add a total of $p_r$ rows of padding and $p_c$ columns of padding to a $m\\times n$ input, the output shape after applying a $k\\times k$ kernel with $(s_r, s_c)$ stride will be:\n\n$$[(m+p_r-k) / s_r+1]\\times [(n+p_c-k) / s_c+1]$$\n\n\nTo illustrate, the full convolution process using one $3\\times3$ kernel on a $3\\times3$ input image with padding in place and `strides = (2,2)` looks like:\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/paddingstride.gif\" width=\"85%\"></center>\n\nWe can code this in Keras:\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters = 1,\n                 kernel_size = (3, 3),\n                 strides = (2, 2),\n                 padding = \"same\",\n                 input_shape = (3, 3, 1)))\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So the output shape calculated by Keras is indeed $2\\times2\\times1$, where the one indicates the output channel (because we only applied one kernel). \n\nWe can code the input image array and the $3\\times3$ kernel, and then use a simple CNN with one **Conv2D** layer to predict the output values.\n\n**NOTE:** Before feeding the input into the CNN, we need to reshape it so that the first **1** represents the batch size and the last **1** represents the number of channels the input contains.\n","metadata":{}},{"cell_type":"code","source":"input_ = np.array([[1, 1, 3],\n              [2, 1, 2],\n              [3, 1, 4]]).reshape(1, 3, 3, 1)\n\nkernel = np.array([[1, 0, -1],\n                   [1, 0, -1],\n                   [1, 0, -1]]).reshape(3, 3, 1, 1) # reshaping the kernel is important\n\nb = np.array([0.0])\n\nmodel.set_weights([kernel, b])\noutput_ = model.predict(input_)\n\nfor r in range(output_.shape[1]):\n    print([output_[0,r,c,0] for c in range(output_.shape[2])])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**With the same input and kernel, the output matrix of our CNN is the same as the one shown in the animated gif.**\n","metadata":{}},{"cell_type":"markdown","source":"### Example 1: Image Processing - Edge Detection\n\nIn the last image convolution lab, you learnt about the Sobel Operator which uses two kernels to convolve with an image to perform edge detection. In this example, we will use the Sobel Operator to detect edges in the image of the [Leaning Tower of Pisa](https://pxhere.com/en/photo/1027167) with padding and stride.\n","metadata":{}},{"cell_type":"markdown","source":"Let's first define the two kernels used for the [Sobel Operator](https://en.wikipedia.org/wiki/Sobel_operatorhttps://en.wikipedia.org/wiki/Sobel_operator):\n","metadata":{}},{"cell_type":"code","source":"# Lets define our custom kernels for the horizontal and vertical gradients\ndef v_grad(shape, dtype=None):\n    # Here we use a single numpy array to define our x gradient kernel\n    grad = np.array([\n        [1, 0, -1],\n        [2, 0, -2],\n        [1, 0, -1]\n    ]).reshape((3, 3, 1, 1))\n    # this line is quite important, we are saying we want one 3x3 kernel each for one channel of pixels (grayscale)\n    \n    # We check to make sure the shape of our kernel is the correct shape\n    # according to the initialization of the Convolutional layer below\n    assert grad.shape == shape\n    return keras.backend.variable(grad, dtype='float32')\n\ndef h_grad(shape, dtype=None):\n    grad = np.array([\n        [1, 2, 1],\n        [0, 0, 0],\n        [-1, -2, -1]\n        ]).reshape((3, 3, 1, 1))\n    \n    assert grad.shape == shape\n    return keras.backend.variable(grad, dtype='float32')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Download the image and display it.\n","metadata":{}},{"cell_type":"code","source":"!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/pisa.jpg\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_width = 350\nimg_height = 500\nimg = PIL.Image.open(\"pisa.jpg\").resize((img_width, img_height))\nimg","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we set `padding = 'same'` and try different values of `strides` for convolving the two kernels of the Sobel operator over the image. We will use the pre-defined function **sobel**.\n","metadata":{}},{"cell_type":"code","source":"input_img = ImageOps.grayscale(img)\ninput_img = np.array(input_img).reshape((1, img_height, img_width, 1))\n\nfig, axs = plt.subplots(1, 3, figsize=(9, 10), constrained_layout=True)\nfig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0.1)\n\nfor i, ax in enumerate(axs.flat):\n    output = sobel(img = input_img, \n                   padding='same',\n                   strides=i+1).astype('int').clip(0,255)\n\n    ax.imshow(output, cmap='gray')\n    ax.set_title(f\"Strides: {i+1}, Shape: {output.shape}\", fontsize=13)\n    ax.axis('off')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we increase the value of `strides`, the output shape becomes proportionally smaller (as shown in the image titles) but edges detected also become blurry. Therefore, whether to use stride and what value to use may be highly dependent on the specific problem at hand.\n","metadata":{}},{"cell_type":"markdown","source":"## Activation\n","metadata":{}},{"cell_type":"markdown","source":"Recall in our previous coded example or in the animated gif, the output matrix contains negative values such as -2. However, since CNN is typically implemented on images which should consist of pixels values ranging from 0 to 255, we need to make our CNN valid by adding what's called an activation after the convolution operation. \n\nIn addition to the need of restricting certain limits for the values in our matrices, activations are also necessary for adding **non-linearity** into our network so that the network can learn complex patterns in the data.\n\nIn `tensorflow.keras.layers.Conv2D`, there is an argument called `activation` where we can specify the type of activation we want for the current convolution layer added. Currently, there are many types of activations existing, such as Sigmoid, Tanh, ReLU, ..., etc. For example, the famous **Sigmoid** function is defined as follows:\n\n$$ f(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x}}  $$ \n\nWe can code the sigmoid function using numpy:\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(X):\n\n    return 1/(1 + np.exp(-X))\n\nX = np.linspace(-10, 10, 100)\nsigmoid_X = sigmoid(X)\nplt.plot(X, sigmoid_X)\nplt.axhline(y=0.5, color='r', linestyle='-')\nplt.xlabel(\"x\")\nplt.ylabel(\"Sigmoid(x)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The sigmoid function is also called a **squashing function**. The input to the sigmoid activation is usually a weighted sum of the outputs of the previous layer. If the input is either a very large negative number or a very large positive number, the output of the activation is always between 0 and 1. \n\n\nOther than the sigmoid activation, you can also read on the different kinds of activations [here](https://en.wikipedia.org/wiki/Activation_function). For this lab we will use the **ReLU** activation.\n\n**ReLU** or **Rectified Linear Unit**, is a widely used activation function, especially with Convolutional Neural networks. It is defined as:\n\n$$f(x) = max(0, x)$$\n\nwhere x is the matrix obtained after convolution. By using ReLU, we can filter out all the negative values as they will become zero's and just keep the positive values.\n\nWe can code the relu activation using numpy:\n","metadata":{}},{"cell_type":"code","source":"def relu(X):\n    return np.maximum(0, X)\n\nX = np.linspace(-10, 10, 100)\nrelu_X = relu(X)\nplt.plot(X, relu_X)\nplt.xlabel(\"x\")\nplt.ylabel(\"Relu(x)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 2: Feature Detection with Kernel and Activation\n\nIn this example, instead of using the Sobel operator for edge detection, we will convolve a simple kernel over all three channels of a RGB image for the same purpose. This means our final result of edge detection will no longer be a boring, grayscale image, but a colorful and exciting one!\n\nSpecifically, we will use the following kernel:\n\n$$\n\t\\begin{bmatrix} \n\t-1 & -1 & -1 \\\\\\\\\\\\\n\t-1 & 8 & -1\\\\\\\\\n\t-1 & -1 & -1\\\\\\\\\n\t\\end{bmatrix}\n\t\\quad\n$$\n\nNote that the kernel we will be using contains negative values, therefore we should specify an activation function (namely **ReLu**) in our Conv2D layer to filter out the negative values.\n\nLet's download the image first!\n","metadata":{}},{"cell_type":"code","source":"!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/lambor.jpeg\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_width = 500\nimg_height = 280\n\nimg = Image.open(\"lambor.jpeg\").resize((img_width, img_height))\nimg","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we will be convolving the kernel over each channel, we need to split the RGB image into three channels or three grayscale images.\n\nLet's display the three channels:\n","metadata":{}},{"cell_type":"code","source":"arr = np.array(img)\nred_c = arr[:,:,0]\ngreen_c = arr[:,:,1]\nblue_c = arr[:,:,2]\n\nchannels = [red_c, green_c, blue_c]\nnames = [\"Red\", \"Green\", \"Blue\"]\n\nplt.figure(figsize=(18, 5))\n\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.imshow(channels[i], cmap='gray')\n    plt.axis(\"off\")\n    plt.title(f\"{names[i]} channel\", fontsize=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can see that the color of the car is mostly coming from the red and green channel, and very little from the blue channel.\n\nNow we build a simple CNN with one Conv2D layer, using the kernel specified above as the weights.\n","metadata":{}},{"cell_type":"code","source":"kernel = np.array([[-1,-1,-1],\n                   [-1,8,-1],\n                   [-1,-1,-1]]).reshape(3,3,1,1)\nb = np.array([0.0])\n\nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(input_shape = (img_height, img_width, 1),\n                 filters=1, \n                 kernel_size=3, \n                 padding='same',\n                 activation='relu'\n                 ))\nmodel.set_weights([kernel, b])\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once we initialize our CNN, we can now use it to predict the edge detection result for each of the R, G, B channel's gray image! \n\nDon't worry if the predicted results look almost all-black to you. Remember we used the ReLu activation to set all the negative pixel values to 0. In each channel, if the pixel value is 0, the pixel will appear black. Only the important information will be kept.\n","metadata":{}},{"cell_type":"code","source":"acts = []\nplt.figure(figsize=(18,5))\n\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    \n    # loop through each channel\n    input_ = channels[i].reshape((1, img_height, img_width, 1)) \n    act = model.predict(input_).squeeze(0).squeeze(2).astype('int').clip(0,255)\n    # store the result in a list called \"acts\"\n    acts.append(act)\n    \n    plt.imshow(act, cmap='gray')\n    plt.axis(\"off\")\n    plt.title(f\"{names[i]} Channel with activation\", fontsize=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's time to stack the three channels back together and see what our edge detection tool did for us!\n","metadata":{}},{"cell_type":"code","source":"arr_hat = np.dstack((acts[0],acts[1],acts[2]))\n\nplt.figure(figsize=(10,5))\nplt.imshow(arr_hat)\nplt.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we applied the edge detector on the three color channels separately, once we merge the results after the convolutions we obtain colorful edges of the car. Each colored edge that you see in the picture above is contributed by some degree of red edges detected in channel R, green edges detected in channel G, and blue edges in channel B.\n","metadata":{}},{"cell_type":"markdown","source":"## Pooling\n","metadata":{}},{"cell_type":"markdown","source":"### What is Pooling?\n\nA CNN model may include local and/or global pooling layers along with traditional convolutional layers. It is a form of non-linear down-sampling to progressively **reduce the spatial size of the representation**, to **reduce the number of parameters**, **memory footprint and amount of computation in the network**, and hence to also **control overfitting**.\n\nThere are two common types of pooling in popular use: **Max Pooling** and **Average Pooling**. Max pooling outputs the maximum value of each sub-region in the feature map, while average pooling outputs the average value.\n\nIt is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture. You can read more about Pooling [here](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer).\n\nAn illustration of **Max Pooling**:\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/maxpooling.gif\" width=\"1100\"></center>\n","metadata":{}},{"cell_type":"markdown","source":"In tensorflow.keras, a **Pooling layer** downsamples the input along its spatial dimensions (height and width) by taking the maximum or average value over an input window (of size defined by `pool_size`) for each channel of the input.\n\nYou can add a Pooling layer to a CNN with **MaxPooling2D** or **AveragePooling2D**. Like a **Conv2D** layer, a Pooling layer also has arguments such as `padding` and `strides`, and they work in a same way as in a Conv2D layer. \n","metadata":{}},{"cell_type":"markdown","source":"### Example 3: Max or Average?\n\n- Max pooling **retains the most prominent features in the feature map**. It is useful when the background of the image is dark and we are interested in the lighter pixels of the image. For example, in the MNIST dataset, the digits are represented in white color and the background is black, so max pooling is more suitable.\n\n- Average pooling **tends to smooth out the image**. Sometimes it cannot extract the important features because it takes everything into account, and gives an average value which may or may not be important. It shows a similar effect irrespective of the background.\n","metadata":{}},{"cell_type":"markdown","source":"Run the next cell to download the MNIST datset, which we will use to illustrate the difference between the output of Max Pooling and Average Pooling.\n","metadata":{}},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\nprint(\"MNIST downloaded!\")\nprint(\"Train set shape:\", X_train.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's extract 5 random images from the train set and display them:\n","metadata":{}},{"cell_type":"code","source":"images = []\nplt.figure(figsize=(15,3))\nfor i in range(5):\n    img = X_train[np.random.randint(0, 60000)].astype('float')\n    images.append(img)\n    \n    plt.subplot(1,5,i+1)\n    plt.imshow(img)\n    plt.axis('off')    ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next we use our **max_pool** and **avg_pool** tool to perform pooling on these five images and see how their results appear differently.\n","metadata":{}},{"cell_type":"code","source":"max_pool = keras.Sequential([MaxPooling2D(pool_size = (2,2))])\navg_pool = keras.Sequential([AveragePooling2D(pool_size = (2,2))])\n\nfig1, axs1 = plt.subplots(1, 5, figsize=(12,3), constrained_layout=True)\nfig2, axs2 = plt.subplots(1, 5, figsize=(12,3), constrained_layout=True)\nfig1.suptitle(\"Max pooling result\", fontsize=20)\nfig2.suptitle(\"Average pooling result\", fontsize=20)\n\nfor img, ax1, ax2 in zip(images, axs1.flat, axs2.flat):\n    input_ = img.reshape(1, 28, 28, 1)\n    ax1.imshow(max_pool.predict(input_).squeeze(0).squeeze(2))\n    ax1.axis('off')\n    ax2.imshow(avg_pool.predict(input_).squeeze(0).squeeze(2))\n    ax2.axis('off')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see, in max pooling the prominent features are highlighted more, whereas in average pooling it gives a smoother image retaining the complete features in the image.\n\nWe cannot say that one particular pooling method is better than the other generally. The choice of pooling operation should be made based on the problem at hand.\n","metadata":{}},{"cell_type":"markdown","source":"## Exercises\n\nIn this exercise, you will support building a CNN to classify images from the famous CIFAR10 dataset.\n\nThe pre-defined **load_cifar10** function will return a preprocessed cifar10 dataset, where:\n\n1. The pixel values in **X_train** and **X_test** are normalized float numbers.\n2. The **y_train** and **y_test** are one-hot encoded into 10-element binary vectors with a 1 for the index of the class value.\n","metadata":{}},{"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\nfrom keras.models import Sequential\n\ndef load_cifar10():\n    (trainX, trainY), (testX, testY) = datasets.cifar10.load_data()\n    \n    trainX = trainX.astype('float32') / 255\n    testX = testX.astype('float32') / 255\n    \n    trainY = to_categorical(trainY)\n    testY = to_categorical(testY)\n    \n    return trainX, trainY, testX, testY","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, y_train, X_test, y_test = load_cifar10()\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1 - Display some images\n\nWrite the code to display the first 25 images from the train set.\n","metadata":{}},{"cell_type":"code","source":"class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\nplt.figure(figsize=(10,10))\n# TO DO\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(X_train[i])\n    plt.title(class_names[np.where(y_train[i]==1)[0][0]])\n    plt.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 2 - Set up a Convolution layer\n\nCreate a Conv2D layer called **Conv** with \n- 32 $3\\times3$ kernels\n- `'he_uniform'` kernel initializer\n- Padding\n- ReLu activation\n","metadata":{}},{"cell_type":"code","source":"# TO DO\nConv = layers.Conv2D(filters=32,\n                     kernel_size=(3,3),\n                     kernel_initializer='he_uniform',\n                     padding='same',\n                     activation='relu')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 3 - Set up a Max pooling layer\n\nCreate a MaxPooling2D layer with pool_size equal 2, name the single layer **Max**.\n","metadata":{}},{"cell_type":"code","source":"# TO DO\nMax = layers.MaxPooling2D(pool_size=(2,2))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 4 - Create a deeper CNN with blocks\n\nNow that you've practiced defining the two types of most commonly used layers in CNN. You can stack two **Conv** layers and one **Max** layer together as a block, and create a deeper CNN with three of those blocks!\n\nYou could choose to double the number of kernels/channels in the **Conv2D** layers as you move from one block to another.\n","metadata":{}},{"cell_type":"code","source":"# TO DO\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n                 input_shape=(32, 32, 3))) \n# Don't forget specifying input_shape in the 1st Conv2D layer of your CNN\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optional section\n","metadata":{}},{"cell_type":"markdown","source":"You've accomplished a lot so far! Now just run the following cells to finish up building your CNN with some Dense layers. Don't worry if you don't know what the code does, as you will learn about Flattening and Dense layers in the next lab.\n","metadata":{}},{"cell_type":"code","source":"model.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()\n# compile model\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{},"outputs":[],"execution_count":null}]}