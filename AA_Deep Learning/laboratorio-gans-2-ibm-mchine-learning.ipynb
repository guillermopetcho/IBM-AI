{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"# Deep Convolutional Generative Adversarial Networks (DCGANs)\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **30** minutes\n\nYou work for an online anime video game company; the company would like to create a unique anime avatar for a game for each player. As there are millions of players, you must use a DCGANs to create each character. \n\n\n<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/face_cartton.png\" width=\"700\" alt=\"Skills Network Logo\">\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n<ol>\n    <li><a href=\"https://#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"https://#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"#Defining Helper Functions\">Defining Helper Functions</a></li>\n        </ol>       \n    </li>\n    <li><a href=\"#Introduction\">Introduction</a></li>\n    <li><a href=\"#Loading the Dataset\">Loading the Dataset</a></li>\n    <li><a href=\"#Creating Data Generator\">Creating Data Generator</a></li>\n    <li><a href=\"#Generator and Discriminator\">Generator and Discriminator</a>\n        <ol>\n            <li><a href=\"#Building the Generator\">Building the Generator</a></li>\n            <li><a href=\"#Building the Discriminator\">Building the Discriminator</a></li>\n        </ol>  \n    </li>   \n    <li><a href=\"#Defining Loss Functions\">Defining Loss Functions</a></li>\n    <li><a href=\"#Defining Optimizers\">Defining Optimizers </a></li>\n    <li><a href=\"#Create Train Step Function\">Create Train Step Function</a></li>\n    <li><a href=\"#Training the Model\">Training the Model</a>\n         <ol>\n            <li><a href=\"#Loading Pre-trained model (150 epochs)\">Loading Pre-trained model (150 epochs)</a></li>\n        </ol>  \n    </li>     \n    <li><a href=\"#Explore Latent Variables\">Explore Latent Variables</a>\n        <ol>\n            <li><a href=\"#Exercise 1\">Exercise 1</a></li>\n            <li><a href=\"#Exercise 2\">Exercise 2</a></li>\n            <li><a href=\"#Exercise 3\">Exercise 3</a></li>\n        </ol>       \n    </li>\n</ol>\n\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Apply DCGANs to a dataset \n*   Understand how to train DCGANs \n*   Generate an image using a DCGAN\n*   Understand how changing the input of the latent space of DCGANs changes the generated image \n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook command in a different Jupyter environment (like Watson Studio or Anaconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n#!mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 skillsnetwork==0.20.6\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the following upgrade and then **RESTART YOUR KERNEL**. Make sure the version of tensorflow imported below is **no less than 2.9.0**.\n","metadata":{}},{"cell_type":"code","source":" %%capture\n!pip3 install --upgrade tensorflow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n*We recommend you import all required libraries in one place (here):*\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport keras\n\nimport numpy as np\nimport tensorflow as tf\nprint(f\"tensorflow version: {tf.__version__}\")\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input,Conv2DTranspose,BatchNormalization,ReLU,Conv2D,LeakyReLU\n\n\nfrom IPython import display\nimport skillsnetwork\nprint(f\"skillsnetwork version: {skillsnetwork.__version__}\")\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\nimport os\nfrom os import listdir\nfrom pathlib import Path\nimport imghdr\n\nimport time\nfrom tqdm.auto import tqdm\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining Helper Functions\n\nThis function will plot five images from an array\n","metadata":{}},{"cell_type":"code","source":"def plot_array(X,title=\"\"):\n    \n    plt.rcParams['figure.figsize'] = (20,20) \n\n    for i,x in enumerate(X[0:5]):\n        x=x.numpy()\n        max_=x.max()\n        min_=x.min()\n        xnew=np.uint(255*(x-min_)/(max_-min_))\n        plt.subplot(1,5,i+1)\n        plt.imshow(xnew)\n        plt.axis(\"off\")\n\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Introduction\n","metadata":{}},{"cell_type":"markdown","source":"In the previous lab, you learned about the working mechanics of Generative Adversarial Networks (GANs) and their various applications, such as Image Generation. However, GANs have also been known to be unstable to train, and often, the generated images suffer from being noisy and incomprehensible.\n\nApplying Convolutional Neural Networks to GANS has led to improved results. They are called Deep Convolutional Generative Adversarial Networks (DCGANs). In this lab, we will build and train DCGANs using several approaches introduced in the original <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">DCGANs paper</a>. \n\n\nThe proposed approaches are summarized here:\n\n\n- Replace any pooling layers with **strided convolutions (discriminator)** and **fractional-strided\nconvolutions (generator)**.\n- Use **batchnorm** in both the generator and the discriminator.\n- **Remove fully connected hidden layers** for deeper architectures.\n- Use **ReLU** activation in generator for all layers except for the output, which uses **Tanh**.\n- Use **LeakyReLU** activation in the discriminator for all layers except for the output, which uses **Sigmoid**.\n- Use **Adam optimizer**.  \n\nThese approaches will result in more stable training of deeper generative models.\n","metadata":{}},{"cell_type":"markdown","source":"## Loading the Dataset\n\nWe will mainly work with the Anime Face dataset from [Kaggle](https://www.kaggle.com/datasets/splcher/animefacedataset?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01). The original dataset has 63,632 \"high-quality\" anime faces, but to make the models train faster in this lab, we randomly sampled 20,000 images and prepared a dataset called `cartoon_20000`. \n\nLet's download the smaller dataset using the Skills Network library's `prepare` function:\n","metadata":{}},{"cell_type":"code","source":"dataset_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/cartoon_20000.zip\"\nawait skillsnetwork.prepare(dataset_url, overwrite=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Anime Face or the Cartoon images are stored in the `cartoon_2000` folder in your current working directory. As a preprocessing step, we have removed any files that are not proper image formats (based on the file extensions) and any duplicate images.\n","metadata":{}},{"cell_type":"markdown","source":"## Creating Data Generator\n","metadata":{}},{"cell_type":"markdown","source":"First, we declare some properties of our images, including image height, image width, and batch size.\n","metadata":{}},{"cell_type":"code","source":"img_height, img_width, batch_size=64,64,128","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we create a Keras <code>image_dataset_from_directory</code> object with a specified image directory and the parameters are defined as above. This process may take some time:\n","metadata":{}},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(directory='cartoon_20000', # change directory to 'cartoon_data' if you use the full dataset\n                                                       image_size=(img_height, img_width),\n                                                       batch_size=batch_size,\n                                                       label_mode=None)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `train_ds` we defined is a `tf.data.Dataset` that yields batches of images with `image_size = (64, 64)` from the directory specified or subdirectories (if any).\n","metadata":{}},{"cell_type":"markdown","source":"**(OPTIONAL)** If you are running this notebook locally and you have multiple cores, then we can use the runtime to tune the value dynamically at runtime as follows:\n","metadata":{}},{"cell_type":"code","source":"#AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n#train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We apply the Lambda function on `train_ds` to normalize the pixel values of all the input images from $[0, 255]$ to $[-1, 1]$:\n","metadata":{}},{"cell_type":"code","source":"normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./127.5, offset=-1)\nnormalized_ds = train_ds.map(lambda x: normalization_layer(x))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's take one batch of images for displaying:\n","metadata":{}},{"cell_type":"code","source":"images=train_ds.take(1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convert the batch dimension to the indexes in a list:\n","metadata":{}},{"cell_type":"code","source":"X=[x for x in images]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can then plot the first five images in the batch using the function   ```plot_array```:\n","metadata":{}},{"cell_type":"code","source":"plot_array(X[0])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Generator and Discriminator\n","metadata":{}},{"cell_type":"markdown","source":"### Building the Generator\n","metadata":{}},{"cell_type":"markdown","source":"The Generator is comprised of several layers of transposed convolution, the opposite of convolution operations.\n\n- Each Conv2DTranspose layer (except the final layer) is followed by a Batch Normalization layer and a **Relu activation**; for more implementation details, check out <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">[1]</a>. \n- The final transpose convolution layer has three output channels since the output needs to be a color image. We use the **Tanh activation** in the final layer. \n\nSee the illustration of the architecture from <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">[1]</a> below.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/generator.png\" alt=\"generator image\" width=\"80%\"></center>\n\nWe build the Generator network by using the parameter values from <a href=\"https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" >[2]<a>.\n","metadata":{}},{"cell_type":"code","source":"def make_generator():\n    \n    model=Sequential()\n    \n    # input is latent vector of 100 dimensions\n    model.add(Input(shape=(1, 1, 100), name='input_layer'))\n    \n    # Block 1 dimensionality of the output space  64 * 8\n    model.add(Conv2DTranspose(64 * 8, kernel_size=4, strides= 4, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_1'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1'))\n    model.add(ReLU(name='relu_1'))\n\n    # Block 2: input is 4 x 4 x (64 * 8)\n    model.add(Conv2DTranspose(64 * 4, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_2'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2'))\n    model.add(ReLU(name='relu_2'))\n\n    # Block 3: input is 8 x 8 x (64 * 4)\n    model.add(Conv2DTranspose(64 * 2, kernel_size=4,strides=  2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_3'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_3'))\n    model.add(ReLU(name='relu_3'))\n\n                       \n    # Block 4: input is 16 x 16 x (64 * 2)\n    model.add(Conv2DTranspose(64 * 1, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_4'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_4'))\n    model.add(ReLU(name='relu_4'))\n\n    model.add(Conv2DTranspose(3, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, \n                              activation='tanh', name='conv_transpose_5'))\n\n    return model","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By printing the summary of the Generator architecture, we can see that the transposed convolutions **upsample** a 100-dim input vector to a high-dimensional image of size 64 x 64 x 3.\n","metadata":{}},{"cell_type":"code","source":"gen = make_generator()\ngen.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Building the Discriminator\n","metadata":{}},{"cell_type":"markdown","source":"The Discriminator has five convolution layers. \n\n- All but the first and final Conv2D layers have Batch Normalization, since directly applying batchnorm to all layers could result in sample oscillation and model instability; \n- The first four Conv2D layers use the **Leaky-Relu activation** with a slope of 0.2. \n- Lastly, instead of a fully connected layer, the  output layer has a convolution layer with a **Sigmoid activation** function.\n","metadata":{}},{"cell_type":"code","source":"def make_discriminator():\n    \n    model=Sequential()\n    \n    # Block 1: input is 64 x 64 x (3)\n    model.add(Input(shape=(64, 64, 3), name='input_layer'))\n    model.add(Conv2D(64, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_1'))\n    model.add(LeakyReLU(0.2, name='leaky_relu_1'))\n\n    # Block 2: input is 32 x 32 x (64)\n    model.add(Conv2D(64 * 2, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_2'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1'))\n    model.add(LeakyReLU(0.2, name='leaky_relu_2'))\n\n    # Block 3\n    model.add(Conv2D(64 * 4, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_3'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2'))\n    model.add(LeakyReLU(0.2, name='leaky_relu_3'))\n\n\n    #Block 4\n    model.add(Conv2D(64 * 8, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_4'))\n    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_3'))\n    model.add(LeakyReLU(0.2, name='leaky_relu_4'))\n\n\n    #Block 5\n    model.add(Conv2D(1, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False,  \n                     activation='sigmoid', name='conv_5'))\n\n    return model ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By printing the summary of the Discriminator architecture, we can see that the strided convolutions **downsample** an input image of size 64 x 64 x 3.\n","metadata":{}},{"cell_type":"code","source":"disc = make_discriminator()\ndisc.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Loss Functions\n\nAs we discussed in the previous lab, the min-max optimization problem can be formulated by minimizing the cross entropy loss for the Generator and Discriminator.  \n\nThe `cross_entropy` object is the Binary Cross Entropy loss that will be used to model the objectives of the two networks.\n","metadata":{}},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generator_loss(Xhat):\n    return cross_entropy(tf.ones_like(Xhat), Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def discriminator_loss(X, Xhat):\n    real_loss = cross_entropy(tf.ones_like(X), X)\n    fake_loss = cross_entropy(tf.zeros_like(Xhat), Xhat)\n    total_loss = 0.5*(real_loss + fake_loss)\n    return total_loss","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Optimizers \n \nWe create two Adam optimizers for the discriminator and the generator, respectively. We pass the following arguments to the optimizers:\n\n- learning rate of 0.0002.\n- beta coefficients $\\beta_1 = 0.5$ and $\\beta_2 = 0.999$, which are responsible for computing the running averages of the gradients during backpropagation.\n","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.0002\n\ngenerator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )\n\ndiscriminator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Train Step Function\n\nAs this lab is more computationally intensive than the last lab, we convert the training step into a function and then use the  @tf.function decorator, which allows the function to be \"compiled\" into a **callable TensorFlow graph**. This will speed up the training; for more information, read <a href=\"https://www.tensorflow.org/guide/function?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">here </a> \n","metadata":{}},{"cell_type":"code","source":"@tf.function\n\ndef train_step(X):\n    \n    #random samples it was found if you increase the  stander deviation, you get better results \n    z= tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])\n      # needed to compute the gradients for a list of variables.\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        #generated sample \n        xhat = generator(z, training=True)\n        #the output of the discriminator for real data \n        real_output = discriminator(X, training=True)\n        #the output of the discriminator for fake data\n        fake_output = discriminator(xhat, training=True)\n        \n        #loss for each \n        gen_loss= generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n      # Compute the gradients for gen_loss and generator\n    \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    # Compute the gradients for gen_loss and discriminator\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    # Ask the optimizer to apply the processed gradients\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Don't be intimidated by the code above, here is a summary of what a train step accomplishes:\n\n- First, we sample `z`, a batch of noise vectors from a normal distribution ($\\mu = 1, \\sigma = 1$) and feed it to the Generator.\n- The Generator produces generated or \"fake\" images `xhat`.\n- We feed real images `X` and fake images `xhat` to the Discriminator and obtain `real_output` and `fake_output` respectively as the scores.\n- We calculate Generator loss `gen_loss` using the `fake_output` from Discriminator since we want the fake images to fool the Discriminator as much as possible.\n- We calculate Discriminator loss `disc_loss` using both the `real_output` and `fake_output` since we want the Discriminator to distinguish the two as much as possible.\n- We calculate `gradients_of_generator` and  `gradients_of_discriminator` based on the losses obtained.\n- Finally, we update the Generator and Discriminator by letting their respective optimizers apply the processed gradients on the trainable model parameters.\n","metadata":{}},{"cell_type":"markdown","source":"We can transform the random noise using the generator. As the generator is not trained yet, the output appears to be noises:\n","metadata":{}},{"cell_type":"code","source":"generator= make_generator()\nBATCH_SIZE=128\n\nlatent_dim=100\nnoise = tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])\nXhat=generator(noise,training=False)\nplot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Model\n","metadata":{}},{"cell_type":"markdown","source":"As this method is computationally intensive, we will train the model for one epoch and then use the generator to produce artificial images.\n","metadata":{}},{"cell_type":"code","source":"epochs=1\n\ndiscriminator=make_discriminator()\n\ngenerator= make_generator()\n\n\nfor epoch in range(epochs):\n    \n    #data for the true distribution of your real data samples training ste\n    start = time.time()\n    i=0\n    for X in tqdm(normalized_ds, desc=f\"epoch {epoch+1}\", total=len(normalized_ds)):\n        \n        i+=1\n        if i%1000:\n            print(\"epoch {}, iteration {}\".format(epoch+1, i))\n            \n        train_step(X)\n    \n\n    noise = tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])\n    Xhat=generator(noise,training=False)\n    X=[x for x in normalized_ds]\n    print(\"orignal images\")\n    plot_array(X[0])\n    print(\"generated images\")\n    plot_array(Xhat)\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see that, with only one epoch of training and a reduced number of training images, our GAN didn't learn much information, and thus, the generator wasn't able to produce images that make sense to human eyes. There are two quick actions you can take to try to improve the results:\n\n1. Re-train the GAN using the full dataset that has 63,632 images. \n    - To do so, simply go back to the **Loading the Dataset** section, replace the url of the dataset with \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/cartoon_data.tgz\", change the `directory` argument in `tf.keras.utils.image_dataset_from_directory` to `'cartoon_data'` and re-run all the cells. \n    - Note that using more training data does allows the model to learn better and perform better, but it will result in longer training time! **With 63K training images and batch size of 128, your model will train for ~497 iterations.**\n\n\n2. Use a pre-trained generator model to generate images.\n    - You don't need to experience the training time at all! \n    - Proceed to the next subsection to load a pre-trained model, and you will see that the generator trained with 150 epochs can produce almost realistic anime faces.\n","metadata":{}},{"cell_type":"markdown","source":"### Loading Pre-trained model (150 epochs)\n","metadata":{}},{"cell_type":"markdown","source":"As you saw, training a GAN with only one epoch takes quite a long time. If we want to evaluate the performance of a fully trained and optimized GAN, we would need to increase the number of epochs. Thus, to help you avoid extremely long training time in this lab, we will just download the pre-trained Generator network parameters and then use Kera `load_model` function to obtain a pre-trained Generator, which we will use to generate images directly.\n","metadata":{}},{"cell_type":"code","source":"generator_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/generator.tar.gz\"\nawait skillsnetwork.prepare(generator_url, overwrite=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the generator:\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n\nfull_generator=load_model(\"generator\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's generate several images using the fully trained Generator and display them:\n","metadata":{}},{"cell_type":"code","source":"latent_dim=100\n\n# input consists of noise vectors\nnoise = tf.random.normal([200, 1, 1, latent_dim])\n\n# feed the noise vectors to the generator\nXhat=full_generator(noise,training=False)\nplot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explore Latent Variables \nValues of $\\mathbf{z}$ that are relatively close together will produce similar images. For example, we can assigns elements of $\\mathbf{z}$ close values such as $[1,0.8,..,0.4]$. \n","metadata":{}},{"cell_type":"code","source":"for c in [1,0.8,0.6,0.4]:\n    Xhat=full_generator(c*tf.ones([1, 1, 1, latent_dim]),training=False) # latent_dim = 100 defined previously\n    plot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1\n\nPlot the generated images by the Generator with elements of $\\mathbf{z}$ equal $[-1,-0.8,-0.6,-0.4]$. \n","metadata":{}},{"cell_type":"code","source":"# Your code here\nfor c in [1,0.8,0.6,0.4]:\n    Xhat=full_generator(-c*tf.ones([1, 1, 1, latent_dim]),training=False)\n    plot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see how changing the latent variable changes the generated image. Here we alter more and more subsequent values of $\\mathbf{z}$ from 1 to -1; we see the images change accordingly; this is evident in the anime character's hair color:\n","metadata":{}},{"cell_type":"code","source":"z=np.ones( (1, 1, 1, latent_dim))\nfor n in range(10):\n\n    z[0, 0, 0, 0:10*n]=-1\n\n    Xhat=full_generator(z,training=False)\n    print(\"elements from 0 to {} is set to -1\".format(10*n))\n    plot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 2\n\nRepeat the above procedure but set the latent variable $z[0, 0, 0, 0:20*n] = -0.5*n$ each time `for n in range(5)`\n","metadata":{}},{"cell_type":"code","source":"# Your code here\nz=np.ones( (1, 1, 1, latent_dim))\nfor n in range(5):\n\n    z[0, 0, 0, 0:20*n]=-0.5*n\n\n    Xhat=full_generator(z,training=False)\n\n    plot_array(Xhat)\n    ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also hold some of the elements of $\\mathbf{z}$ constant and randomly change others. Here, we set the first 20 elements to one and randomly change the rest. We see that all through the images change, the hair color remains light. \n","metadata":{}},{"cell_type":"code","source":"for n in range(10):\n    z=np.random.normal(0, 1, (1, 1, 1, latent_dim))\n\n    z[0,0,0,0:35]=1\n\n    Xhat=full_generator(z,training=False)\n\n    plot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 3\n\nRepeat the procedure above, but set the elements of $\\mathbf{z}$ from index 0 to 35 to -1\n","metadata":{}},{"cell_type":"code","source":"# Your code here\nfor n in range(10):\n    z=np.random.normal(0, 1, (1, 1, 1, latent_dim))\n\n    z[0,0,0,0:35]=-1\n\n    Xhat=full_generator(z,training=False)\n\n    plot_array(Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Authors\n","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n\n[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n","metadata":{}},{"cell_type":"markdown","source":"## Change Log\n","metadata":{}},{"cell_type":"markdown","source":"| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n| ----------------- | ------- | ----------- | ------------------ |\n| 2022-08-30        | 0.1     | Joseph Santarcangelo  | Created Lab       |\n| 2022-09-06        | 0.1     | Roxanne Li  | Reviewed and edited Lab       |\n| 2022-09-23        | 0.1     | Steve Hord  | QA pass edits                 |\n","metadata":{}},{"cell_type":"markdown","source":"Copyright © 2022 IBM Corporation. All rights reserved.\n","metadata":{}}]}