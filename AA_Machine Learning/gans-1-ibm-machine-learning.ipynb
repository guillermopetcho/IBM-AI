{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":""},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Understanding  Generative Adversarial Networks GANs\n\nEstimated time needed: **35** minutes\n\nIn this lab, we will focus on simulated data to better understand Generative Adversarial Networks (GANs).\n","metadata":{"button":false,"id":"qoLaquIpuIdg","new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"## **Table of Contents**\n\n<ol>\n    <li><a href=\"https://#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"https://#Setup\">Setup</a>\n    </li>\n    <li>\n        <a href=\"https://#Background\">Background</a>\n    </li>\n    <li>\n        <a href=\"https://#Toy Data\">Toy Data</a>\n    </li>\n    <li>\n        <a href=\"https://#The Generator\">The Generator</a>\n    </li>\n    <li>\n        <a href=\"https://#The Discriminator\">The Discriminator</a>\n    </li>\n        <li>\n        <a href=\"https://#The Loss Function GANs (Optional)\">The Loss Function GANs (Optional)</a>\n            <ol>\n            <li><a href=\"https://#Discriminator\">Discriminator</a></li>\n            <li><a href=\"https://#Generator\">Generator</a></li>\n        </ol>\n        </li>\n    <li>\n        <a href=\"https://#Training GANs\">Training GANs</a>\n         <ol>\n            <li><a href=\"https://#Training Generator\">Training Generator</a></li>\n             <li><a href=\"https://#Training Discriminator\">Training Discriminator</a></li>\n        </ol>\n    </li>\n","metadata":{"id":"V5pcK4UYuIdk"}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab, you will be able to:\n\n*   **Understand** the original formulation of GANs, and their two separately trained networks: Generator and Discriminator\n*   **Implement** GANs on simulated and real datasets\n","metadata":{"id":"f-Tz-Ds0uIdl"}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n*   [`keras`](https://keras.io/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for loading datasets.\n","metadata":{"id":"RN0Hnn3NOGF6"}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip3 install --upgrade tensorflow` in the cell below.\n","metadata":{"id":"Iu490IDruIdl"}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip3 install --upgrade tensorflow","metadata":{"button":false,"id":"8frwg6E2uIdm","new_sheet":false,"run_control":{"read_only":false},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n","metadata":{"id":"qnwS_f21OVLA"}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras import layers\nimport time\nfrom tensorflow.keras import models\nfrom tqdm import tqdm","metadata":{"button":false,"id":"wNl7o6I-uIdn","new_sheet":false,"run_control":{"read_only":false},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper function\n","metadata":{"id":"fvLxRu4HuIdp"}},{"cell_type":"code","source":"def plot_distribution(real_data,generated_data,discriminator=None,density=True):\n    \n    plt.hist(real_data.numpy(), 100, density=density, facecolor='g', alpha=0.75, label='real data')\n    plt.hist(generated_data.numpy(), 100, density=density, facecolor='r', alpha=0.75,label='generated data q(z) ')\n    \n    if discriminator:\n        max_=np.max([int(real_data.numpy().max()),int(generated_data.numpy().max())])\n        min_=np.min([int(real_data.numpy().min()),int(generated_data.numpy().min())])\n        x=np.linspace(min_, max_, 1000).reshape(-1,1)\n        plt.plot(x,tf.math.sigmoid(discriminator(x,training=False).numpy()),label='discriminator',color='k')\n        plt.plot(x,0.5*np.ones(x.shape),label='0.5',color='b')\n        plt.xlabel('x')\n        \n    plt.legend()\n    plt.show()","metadata":{"id":"i9qHAuQFuIdq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Background\n","metadata":{"id":"LEukgJfROnlH"}},{"cell_type":"markdown","source":"Generative Adversarial Networks (GANs) are generative models that convert random samples of one distribution into another distribution. They have several applications, like the following:\n\n*   Generate Examples for Image Datasets\n*   Generate Photographs of Human Faces\n*   Generate Realistic Photographs\n*   Generate Cartoon Characters\n*   Image-to-Image Translation\n*   Text-to-Image Translation\n*   Face Frontal View Generation\n*   Generate New Human Poses\n*   Face Aging\n*   Photo Blending\n*   Super Resolution\n*   Photo Inpainting\n*   Clothing Translation\n*   Video Prediction\n\nIn this lab, we will use a toy example to help understand the basic theoretical principles behind GANs. The original form of GANs consisted of a discriminator and a generator; let's use the analogy of a currency forger and the police.\n\nThe Generator is the currency forger, and the output is the counterfeit, for example, a 100-dollar bill. The discriminator is analogous to the police taking the counterfeit and trying to determine if it's real by comparing it to a real $100 bill. In real life, if the counterfeit is easy to detect, the forger will adapt; conversely, the police will also improve; GANs emulate this game of cat and mouse.\n\nWhat makes GANs interesting is that the discriminator and generator continuously improve each other by a well-formulated cost function that backpropagates the errors. GANs are a family of algorithms that use *learning by comparison*. In the lab, we will review the original formulation and use a simulated dataset. We will also point you to some more advanced methods and issues you will encounter with the real datasets for the next lab.\n","metadata":{"id":"GeXfAKwuOjzd"}},{"cell_type":"markdown","source":"<h2 id=\"about_dataset\">Toy Data</h2>\n\nConsider the following data, $\\mathbf{x}$, that is normally distributed $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}|10,1) $ with a mean of 10 and a standard deviation of 1. Now we would like to randomly sample data from this distribution.\n","metadata":{"button":false,"id":"28cXlMaruIdq","new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"mean = [10]\ncov = [[1]]\nX = tf.random.normal((5000,1),mean=10,stddev=1.0)\n\nprint(\"mean:\",np.mean(X))\nprint(\"standard deviation:\",np.std(X))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3tT9V2tuIdr","outputId":"cd993b60-a657-4b01-865d-7bde0634e96f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also have the data sample, z, which is also normally distributed $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z}|0,2) $, with mean of 0 and a standard deviation of 2:\n","metadata":{"id":"KYtZQlTYJuwd"}},{"cell_type":"code","source":"Z = tf.random.normal((5000,1),mean=0,stddev=2)","metadata":{"id":"TEQ66EMWVN9X"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"mean:\",np.mean(Z))\nprint(\"standard deviation:\",np.std(Z))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hFImQB8KYSP","outputId":"e2055ef3-b029-4ebc-e526-824f9ea1327d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's compare the two distributions:\n","metadata":{"id":"xPpxWFsLKgV2"}},{"cell_type":"code","source":"plot_distribution(X,Z,discriminator=None,density=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"8WepYpiEVOHx","outputId":"06062a3b-d4c6-460c-f03e-2009df5d1fd5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's create our first generative model by adding 10 to every sample of $z$. We will call the result $\\hat{\\mathbf{x}}$  as it's an approximation of $\\mathbf{x}$. It is not too difficult to show that $\\hat{\\mathbf{x}} \\sim \\mathcal{N}(\\mathbf{x}|10,1)$.\n","metadata":{"id":"WGwZPvMLHvZh"}},{"cell_type":"code","source":"Xhat=Z+10","metadata":{"id":"I87_YAM2v-Iq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the mean and standard deviation are almost identical\n","metadata":{"id":"LfUF1ILhE2iJ"}},{"cell_type":"code","source":"print(\"mean:\",np.mean(Xhat))\nprint(\"standard deviation:\",np.std(Xhat))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kz_D7cdEGzaS","outputId":"00625c8a-3afc-472d-f2aa-fe23d345c701"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Similarly for the histograms\n","metadata":{"id":"4YZQ8LYfHyXG"}},{"cell_type":"code","source":"plot_distribution(X,Xhat,discriminator=None,density=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"-KO1o4J53e4m","outputId":"9741de53-5ae0-4c5f-f008-c78d71b5f8dd"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the case above, since we just add 10 to the latent variable $z$, we transform $z$ using a deterministic function. We can call this an implicit generative model.\n","metadata":{"id":"YZDPNmE2aJpI"}},{"cell_type":"markdown","source":"## The Generator\n","metadata":{"id":"4p9gI1hlV7Cy"}},{"cell_type":"markdown","source":"There are two networks involved in a GAN, the Generator and the Discriminator. Let's understand the Generator network first.\n\nThe Generator is a neural network denoted by $G$; the idea is that a neural network can approximate any function (by the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)), so you should be able to generate data samples from any type of distribution.\n\nOur goal is to convert the samples, $\\mathbf{z}$, to one that approximates $\\hat{\\mathbf{x}}$,  i.e $\\hat{\\mathbf{x}}=G(\\mathbf{z})$. Let's build a simple Generator $G(\\mathbf{z})=\\mathbf{W}^{T}\\mathbf{z}+\\mathbf{b} $ using Keras.\n\nThe following is a function that outputs a generator using Kera's Sequential model object.\n","metadata":{"id":"fNgoYQEWVzsf"}},{"cell_type":"code","source":"def make_generator_model():\n    generator = tf.keras.Sequential()\n    generator.add(layers.Dense(1))\n    return generator","metadata":{"id":"leGqI7_hH3DV"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use the Generator to convert $\\mathbf{z}$ and make a prediction $\\hat{\\mathbf{x}}$, and display the histogram of the distributions of $\\hat{\\mathbf{x}}$ and $\\mathbf{x}$. As the model is not trained, the trained distributions are quite different:\n","metadata":{"id":"DlINgZaYWAa9"}},{"cell_type":"code","source":"generator=make_generator_model()\n\nXhat = generator(Z, training=False)\nplot_distribution(real_data=X,generated_data=Xhat)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"lkFoJv1Y2haS","outputId":"c734c361-9e5a-4708-9bd2-3521acff65f2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will discuss the use of the parameter `training=False` later on.\n","metadata":{"id":"htQ31jtGYUE4"}},{"cell_type":"markdown","source":"## The Discriminator\n","metadata":{"id":"lwqUQ5ghfHh7"}},{"cell_type":"markdown","source":"The discriminator $D(\\mathbf{x})$ is a neural network that learns to distinguish between actual and generated samples. The simplest Discriminator is a simple logistic regression function. Let's create a discriminator in Keras with one Dense layer; we leave the logistic function out as it will be incorporated in the cost function, which is the convention in Keras.\n","metadata":{"id":"xHzpSykOL8E6"}},{"cell_type":"code","source":"def make_discriminator_model():\n    discriminator=tf.keras.Sequential()\n    discriminator.add(layers.Dense(1))\n    return discriminator\n\ndiscriminator=make_discriminator_model()","metadata":{"id":"2cxBhjmOXA3X"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The discriminator and generator are randomly initialized, but we can plot the output of each and compare it to the true data distribution, with the generated data in red and the real data in green, and the logistic function as a function of the x axis. We also include the threshold. If the output of the logistic function is less than 0.5, the sample is classified as generated data; conversely, if the output is greater than 0.5, the sample will be classified as data that came from the real distribution.\n","metadata":{"id":"zpwz7ttQON7M"}},{"cell_type":"code","source":"plot_distribution(real_data=X,generated_data=Xhat,discriminator=discriminator)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"uNxkU2clOLSL","outputId":"72ed198b-62fe-4deb-b557-7a29ed3a46f7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Applying the sigmoid function to the discriminator output, we get the probabilites that the samples belong to the real distribution. We can count the number of true samples that the discriminator correctly classifies.\n\nFor the real data, the discriminator successfully assigns a probability greater than 0.5 for all 5000 samples:\n","metadata":{"id":"nc0kPaFRYK4E"}},{"cell_type":"code","source":"py_x=tf.math.sigmoid(discriminator(X,training=False))\nnp.sum(py_x>0.5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EV1GKoQQWls2","outputId":"9c72f90d-3f77-4655-ebd6-287f30d6e038"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the generated data, only 1425 out of the 5000 samples are classified as having more than 50% chance of coming from the real distribution.\n","metadata":{"id":"AZIThpzTYhhk"}},{"cell_type":"code","source":"py_x=discriminator(Xhat)\nnp.sum(py_x>0.5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bxd1GSSMvMSq","outputId":"23b778e5-5c13-4ca4-b003-629a4a14ec9f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also use the follwoing to find the average value of the sigmoid function for all the samples.\n","metadata":{"id":"j0T7zQVLve2o"}},{"cell_type":"code","source":"def get_accuracy(X,Xhat):\n    total=0\n    py_x=tf.math.sigmoid(discriminator(X,training=False))\n    total=np.mean(py_x)\n    py_x=tf.math.sigmoid(discriminator(Xhat,training=False))\n    total+=np.mean(py_x)\n    return total/2","metadata":{"id":"418ZppfOMFpx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_accuracy(X,Xhat)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In many cases, we can instead study the difference in the distribution; in this case, the discriminator is called a <a href='https://arxiv.org/pdf/2107.06700.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01'>Critic</a>, a real-valued function.\n","metadata":{"id":"xCyrqO1bbOte"}},{"cell_type":"markdown","source":"## The Loss Fuction  GANs (optional)\n\nGANs convert an unsupervised learning problem to a supervised one. Instead of formulating the problem like a two-player minimax game with a value function like in <a href=https://arxiv.org/pdf/1406.2661.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01 >\\[1]</a>, we can treat the problem of maximizing the familiar log-likelihood of the logistic function analogous to minimizing the cross-entropy loss, then incorporate the generator and discriminator.\n\n### Discriminator\n\nIn order to train the GANS, we start off with standard maximization of the likelihood for the discriminator for the standard dataset $\\mathcal{D}={{(x\\_1, y\\_1), ..., (x_N, y_N)}}$:\n\n$$V(D)=\\sum\\_{n=1}^N \\left( y_n \\ln(D(\\mathbf{x}\\_n))+(1-y_n) \\ln(1-D(\\mathbf{x}\\_n))\\right)$$\n\nWhere $y=1$ for samples from the true distribution and $y=0$ for samples from the generator. The goal is to maximize this term with respect to $D$:\n\n$$max\\_{D}(V(D))$$\n","metadata":{"id":"hyUpawnfZpX9"}},{"cell_type":"markdown","source":"To also incorporate the generated samples, we augment the right side of the equation with the generated $k$th sample $\\hat{\\mathbf{x}}\\_k$. As they are not part of the dataset $k \\notin \\mathcal{D} $, we have to include a second summation where $y=0$. Finally, combining the cases of $y=1$ and $y=0$, we get:\n\n$$V(D)=\\sum\\_{ n\t\\in \\mathcal{D}}  \\ln(D(\\mathbf{x}*n))+\\sum*{k \t\\notin \\mathcal{D}} \\ln(1-D(\\hat{\\mathbf{x}}\\_k) ) $$\n","metadata":{"id":"CJm5w5-Fk9Ih"}},{"cell_type":"markdown","source":"### Generator\n\nFor the generator we simply replace $\\hat{\\mathbf{x}}\\_k$ with the $G(\\mathbf{z}\\_k)$ .\n\n$$V(G,D)=\\sum\\_{n\t\\in \\mathcal{D}} \\ln(D(\\mathbf{x}*n))+\\sum*{k \t\\notin \\mathcal{D}} \\ln(1-D(G(\\mathbf{z}\\_k))) $$\n\nAs this is a density estimation problem, it is common to replace the summation with the expected value like in <a href=https://arxiv.org/pdf/1406.2661.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01 >\\[1]</a>. We replace the summations with an expectation where $p(\\mathbf{x})$ is the true distribution and $p(\\mathbf{z})$ is the distribution of $\\mathbf{z}$.\n\n$$V(D,G)=\\mathbb{E}*{x\\sim p(\\mathbf{x})} \\ln(D(\\mathbf{x})) + \\mathbb{E}*{\\mathbf{z} \\sim p(\\mathbf{z})} \\ln(1-D(G(\\mathbf{z}) )) $$\n\nAs we are trying to trick the discriminator, we would like to find a $G$ that minimize the above expression, such as:\n\n$$min\\_{G} max\\_{D} V(D,G)$$\n","metadata":{"id":"mXK_r8G5vqk_"}},{"cell_type":"markdown","source":"## Training GANS\n\n### Training Generator\n\nGANs are quite difficult to train, even for a simple example. Let's start off with training the generator in practice.\n\n$log(1 âˆ’ D(G(\\mathbf{z})))$ is difficult to work with as $D(G(\\mathbf{z}))$ is near one or zero for the first few iterations. This is because the generator is not yet properly trained, and the discriminator can easily distinguish between the generated and actual samples. Therefore we maximize $log(D(G(\\mathbf{z}\\_k)) )$.\n\nAlthough the output of the generator passes through the discriminator, we do not update the generator in the optimization step, hence we set the parameter `training=False` in the actual training steps.\n\nInstead of maximizing the term, we can take the negative and minimize it. The resultant expression can be calculated in Keras using the cross-entropy loss where all the target values are set to one:\n\n$$\\sum\\_{k \t\\notin \\mathcal{D}} log(1 - D(G(\\mathbf{z}\\_k)) )$$\n","metadata":{"id":"12fPqNkBx6zc"}},{"cell_type":"code","source":"# This method returns a helper function to compute crossentropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ndef generator_loss(Xhat):\n    return cross_entropy(tf.ones_like(Xhat), Xhat)","metadata":{"id":"i7KnHVWZYnSv"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Discriminator\n\nWe can also use the cross-entropy to train the discriminator; we simply multiply $V(G,D)$ by a negative number, set $y=0$ for the generated values and $y=1$ for the real values. We do not update the generator parameters.\n\n$$V(G)=\\sum\\_{n\t\\in \\mathcal{D}} (\\ln(D(\\mathbf{x}*n)))+\\sum*{k \t\\notin \\mathcal{D}} \\ln(1-D(G(\\mathbf{z}\\_k) )) $$\n","metadata":{"id":"sdI-JehH3ZeB"}},{"cell_type":"markdown","source":"The first term is the real loss and the second is the fake loss in Keras.\n","metadata":{"id":"uAsn2HCnJQY-"}},{"cell_type":"code","source":"def discriminator_loss(X, Xhat):\n    real_loss = cross_entropy(tf.ones_like(X), X)\n    fake_loss = cross_entropy(tf.zeros_like(Xhat), Xhat)\n    total_loss = 0.5*(real_loss + fake_loss)\n    return total_loss","metadata":{"id":"q-87B4s2mbHi"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create the optimizer for the discriminator and generator:\n","metadata":{"id":"d74AToWZe-Kb"}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(5e-1,beta_1=0.5,beta_2=0.8)\n\ndiscriminator_optimizer = tf.keras.optimizers.Adam(5e-1,beta_1=0.5, beta_2=0.8)\n","metadata":{"id":"xlN_9J2ZnCXl"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We now train the model; as the dataset is small, we will use batch gradient descent.\n\nFor each iteration we will generate $M$ real examples ${\\mathbf{x}*{1}, ...,\\mathbf{x}*{M}}$, these are from the generating distribution $p(\\mathbf{x})$. This would be our actual dataset if we used real data.\n\nWe will then generate a sample batch of $M$ noise samples ${\\mathbf{z}*{1}, ...,\\mathbf{z}*{M}}$ from noise prior $p(\\mathbf{z})$ and convert the result to a generated image using the generator ${\\hat{\\mathbf{x}}*{1}, ...,\\hat{\\mathbf{x}}*{M}}$.\n\nWe determine the output of the discriminator for both the real and generated samples. We calculate the loss and then update the discriminator and generator through their respective stochastic gradients.\n","metadata":{"id":"mImUx6oRkAbG"}},{"cell_type":"markdown","source":"The convergence of GAN training is a subject in itself. But let's explore a method that works for this simple dataset. Intuitively, we know that if our generated data is identical to our actual data, the probability of correctly classifying is random. Therefore if the generated and actual data are of equal proportion, $D(\\mathbf{x}\\_n)=0.5$ and $D(\\hat{\\mathbf{x}}\\_n)=0.5$.\n\nWe only display iterations where the average discriminator output gets closer to 50% for both the generated data and actual data.\n","metadata":{"id":"-iB66QR4KfrT"}},{"cell_type":"code","source":"#paramters for trainng \nepochs=20\nBATCH_SIZE=5000\nnoise_dim=1\nepsilon=100 \n\n\n#discrimator and gernerator \ntf.random.set_seed(0)\ndiscriminator=make_discriminator_model()\ngenerator=make_generator_model()\n\ntf.config.run_functions_eagerly(True)\n\n\n\ngen_loss_epoch=[]\ndisc_loss_epoch=[]\nplot_distribution(real_data=X,generated_data=Xhat,discriminator=discriminator )\nprint(\"epoch\",0)\n\nfor epoch in tqdm(range(epochs)):\n    #data for the true distribution of your real data samples training ste\n    x = tf.random.normal((BATCH_SIZE,1),mean=10,stddev=1.0)\n    #random samples it was found if you increase the  stander deviation, you get better results \n    z= tf.random.normal([BATCH_SIZE, noise_dim],mean=0,stddev=10)\n    # needed to compute the gradients for a list of variables.\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        #generated sample \n        xhat = generator(z, training=True)\n        #the output of the discriminator for real data \n        real_output = discriminator(x, training=True)\n        #the output of the discriminator  data\n        fake_output = discriminator(xhat, training=True)\n        #loss for each \n        gen_loss= generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    # Compute the gradients for gen_loss and generator\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    # Compute the gradients for gen_loss and discriminator\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    # Ask the optimizer to apply the processed gradients\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n  \n  # Save and display the generator and discriminator if the performance increases \n    if abs(0.5-get_accuracy(x,xhat))<epsilon:\n        epsilon=abs(0.5-get_accuracy(x,xhat))\n        generator.save('generator')\n        discriminator.save('discriminator')\n        print(get_accuracy(x,xhat))\n        plot_distribution(real_data=X,generated_data=xhat,discriminator=discriminator )\n        print(\"epoch\",epoch)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gqzCfYWDnkDO","outputId":"3815a9b7-5735-4ad1-89da-ba9b5a773b30"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For more on training GANs check out the following <a href=\"https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01\">blog</a>. We can display the best performing model\n","metadata":{"id":"tEqiwESi02ej"}},{"cell_type":"code","source":"generator=make_generator_model()\ngenerator= models.load_model('generator')\nxhat=generator(z)\ndiscriminator=models.load_model('discriminator')\nplot_distribution(real_data=X,generated_data=xhat,discriminator=discriminator )","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"nufZ7GOTXVhc","outputId":"82f044cc-a533-4290-8642-2ce5ea7fe2b6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We usually use this version of TensorFlow for the rest of the course, so don't forget to downgrade\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install --upgrade tensorflow==1.14.0","metadata":{},"outputs":[],"execution_count":null}]}