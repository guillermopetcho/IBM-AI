{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"fffe2e8a6841414ac70c57a6272bf08575362563ed7123ff0baf7c2db7be259e","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformadores para la generación de Texto plano - IBM PROYECTO DE ENTREGA\n\n## Objetivos:\n\n- Implementar Transformers para tareas de generación de texto\n\n- Construir, entrenar y evaluar modelos de Transformers para la generación de texto con TensorFlow y Keras\n\n- Aplicar la generación de texto en situaciones reales","metadata":{}},{"cell_type":"markdown","source":"----\n","metadata":{}},{"cell_type":"markdown","source":"### Instrucciones paso a paso\n\n#### Paso 1: Configurar el entorno\n\n- Importar las bibliotecas necesarias y cargar el conjunto de datos\n\n- Preprocesar el conjunto de datos para el entrenamiento\n\nEn el siguiente código:\n\n- Importar TensorFlow y otras bibliotecas necesarias.\n\n- Cargar el conjunto de datos de texto de Shakespeare.\n\n- Preprocesar el conjunto de datos utilizando la capa TextVectorization para convertir el texto en secuencias de enteros.\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install tensorflow==2.16.2\n!pip install pandas\n!pip install scikit-learn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf \nimport numpy as np \nfrom tensorflow.keras.layers import TextVectorization \nfrom tensorflow.keras.utils import get_file ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset \npath_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') \ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8') \n\n# Preview the dataset \nprint(text[:1000]) ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess the dataset \nvocab_size = 10000 \nseq_length = 100 \n\n# Adapt TextVectorization to full text \nvectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int') \ntext_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1) \nvectorizer.adapt(text_ds) \n\n# Vectorize the text \nvectorized_text = vectorizer([text])[0] \nprint(\"Vectorized text shape:\", vectorized_text.shape) \nprint(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10]) ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Paso 2: Crear secuencias de entrada y destino\n\nGenerar secuencias de entrada y destino para entrenar el modelo Transformer.\n\nEn el siguiente código:\n\n- Definir una función para generar secuencias de entrada y destino.\n\n- Dividir los datos de texto en secuencias de la longitud especificada.\n\n- Convertir las secuencias en tensores de TensorFlow para el entrenamiento.\n\nSecuencia generativa:\n","metadata":{}},{"cell_type":"code","source":"def create_sequences(text, seq_length): \n    input_seqs = [] \n    target_seqs = [] \n    for i in range(len(text) - seq_length): \n        input_seq = text[i:i + seq_length] \n        target_seq = text[i + 1:i + seq_length + 1] \n        input_seqs.append(input_seq) \n        target_seqs.append(target_seq) \n    return np.array(input_seqs), np.array(target_seqs) \n\n# Generate sequences \nX, Y = create_sequences(vectorized_text.numpy(), seq_length) \n\n# Check if sequences are correctly generated \nprint(\"Number of sequences generated:\", len(X)) \nprint(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\") \n\n# Check if X and Y are not empty \nassert X.size > 0, \"Input data X is empty\" \nassert Y.size > 0, \"Target data Y is empty\" \nX = tf.convert_to_tensor(X) \nY = tf.convert_to_tensor(Y) \nprint(\"Shape of X:\", X.shape) \nprint(\"Shape of Y:\", Y.shape)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Paso 3: Construir el modelo Transformer\n\nDefina la arquitectura del modelo Transformer para la generación de texto.\n\nEn el siguiente código:\n\n- Defina la clase TransformerBlock, que incluye capas de atención multicabezal y de avance con normalización y abandono.\n\n- Defina la clase TransformerModel, que incluye incrustación, codificación posicional y múltiples bloques Transformer.\n\n- Compile el modelo Transformer utilizando el optimizador Adam y la función de pérdida de entropía cruzada categórica dispersa.\n\nModelo Transformer:","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout\nfrom tensorflow.keras.models import Model\n\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim),\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TransformerModel(Model):  # Model is now properly imported\n    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n        super(TransformerModel, self).__init__()\n        self.embedding = Embedding(vocab_size, embed_dim)\n        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n        self.dense = Dense(vocab_size)\n\n    def positional_encoding(self, seq_length, embed_dim):\n        angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n        pos_encoding = angle_rads[np.newaxis, ...]\n        return tf.cast(pos_encoding, dtype=tf.float32)\n\n    def get_angles(self, pos, i, embed_dim):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n        return pos * angle_rates\n\n    def call(self, inputs, training=False):\n        seq_len = tf.shape(inputs)[1]\n        x = self.embedding(inputs)\n        x += self.pos_encoding[:, :seq_len, :]\n        for transformer_block in self.transformer_blocks:\n            x = transformer_block(x, training=training)  # Pass training argument correctly\n        output = self.dense(x)\n        return output","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters \nembed_dim = 256 \nnum_heads = 4 \nff_dim = 512 \nnum_layers = 4 \n\n# Build the Transformer model \nmodel = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n\n# Provide input shape to build the model by passing a dummy input with maxval specified\n_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n\n# Compile the model \nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Summary of the model \nmodel.summary()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Paso 4: Entrenar el modelo Transformer\n\nEntrenar el modelo Transformer con los datos de texto preprocesados.\n\nEn el siguiente código:\n\n- Entrenar el modelo Transformer con las secuencias de entrada y destino.\n\n- Graficar la pérdida de entrenamiento para monitorizar el rendimiento del modelo a lo largo de las épocas.\n\nEntrenamiento del modelo:\n","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> #### Nota: El conjunto de datos original es grande, lo hemos reducido a 10 000 muestras y hemos limitado el entrenamiento a 2 épocas para minimizar el tiempo de ejecución.","metadata":{}},{"cell_type":"code","source":"X = X[:10000]\nY = Y[:10000]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries for training visualization\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Early stopping callback to stop training if the loss doesn't improve\nearly_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n\n# Train the transformer model on the full input and target sequences\nhistory = model.fit(X, Y, epochs=2, batch_size=32, callbacks=[early_stopping])\n\n# Plot training loss to monitor model performance over epochs\nplt.plot(history.history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Paso 5: Generar texto con el modelo entrenado\n\nDefina una función para generar texto usando el modelo Transformer entrenado.\n\nEn el siguiente código:\n\n- Defina la función generate_text para generar texto usando el modelo Transformer entrenado.\n\n- Convierta la cadena inicial a formato numérico.\n\n- Use el modelo para predecir la siguiente palabra y añadirla al texto generado.\n\n- Imprima el texto generado.\n\n#### Generación de texto:","metadata":{}},{"cell_type":"code","source":"def generate_text(model, start_string, num_generate=100, temperature=1.0):\n    # Convert the start string to a vectorized format\n    input_eval = vectorizer([start_string]).numpy()\n    \n    # Ensure the input length is the same as the model's expected input shape\n    if input_eval.shape[1] < seq_length:\n        # Pad the input if it's shorter than the expected sequence length\n        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n        input_eval = np.concatenate((padding, input_eval), axis=1)\n    elif input_eval.shape[1] > seq_length:\n        # Truncate the input if it's longer than the expected sequence length\n        input_eval = input_eval[:, -seq_length:]\n\n    input_eval = tf.convert_to_tensor(input_eval)\n    \n    # Initialize an empty list to store generated text\n    text_generated = []\n\n    # Start generating text\n    for i in range(num_generate):\n        # Make predictions using the model\n        predictions = model(input_eval)\n\n        # Remove only the batch dimension, keep the logits as 2D (batch_size, vocab_size)\n        predictions = predictions[0]  # This should be of shape [vocab_size]\n\n        # Apply temperature to predictions\n        predictions = predictions / temperature\n        \n        # Use a categorical distribution to predict the next word\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n\n        # Update the input tensor to include the predicted word, maintaining the sequence length\n        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)  # Append predicted token\n        input_eval = input_eval[:, -seq_length:]  # Keep only the last `seq_length` tokens\n        input_eval = tf.convert_to_tensor(input_eval)  # Convert back to tensor\n\n        # Append the predicted word to the generated text\n        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n\n    # Return the generated text starting from the initial seed\n    return start_string + ' ' + ' '.join(text_generated)\n\n# Generate text with temperature control\nstart_string = \"To be, or not to be\"\ngenerated_text = generate_text(model, start_string, temperature=0.7)  # Lower temperature for more focused predictions\nprint(generated_text)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ejercicios prácticos\n\n> #### Nota: Los resultados pueden variar debido a que se redujo el tamaño del conjunto de datos y el entrenamiento se limitó a 2 épocas para acortar el tiempo de ejecución. Sin embargo, se recomienda experimentar con diferentes valores de época para un aprendizaje más profundo.\n\n### Ejercicio 1: Experimentar con diferentes longitudes de secuencia\n\n**Objetivo:** Implementar diferentes longitudes de secuencia para comprender su efecto en el rendimiento del modelo Transformer.\n\n**Instrucciones:**\n\n- Cambiar la longitud de la secuencia a 50\n\n- Preprocesar el conjunto de datos con la nueva longitud de secuencia\n\n- Entrenar el modelo y comparar la pérdida de entrenamiento","metadata":{}},{"cell_type":"code","source":"# Preprocess the dataset \nvocab_size = 10000 \nseq_length = 50 \n\n# Adapt TextVectorization to full text \nvectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int') \ntext_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1) \nvectorizer.adapt(text_ds) \n\n# Vectorize the text \nvectorized_text = vectorizer([text])[0] \nprint(\"Vectorized text shape:\", vectorized_text.shape) \nprint(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10]) \n\nX, Y = create_sequences(vectorized_text.numpy(), seq_length) \n\n\n# Check if sequences are correctly generated \nprint(\"Number of sequences generated:\", len(X)) \nprint(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\") \n\n# Check if X and Y are not empty \nassert X.size > 0, \"Input data X is empty\" \nassert Y.size > 0, \"Target data Y is empty\" \nX = tf.convert_to_tensor(X) \nY = tf.convert_to_tensor(Y) \nprint(\"Shape of X:\", X.shape) \nprint(\"Shape of Y:\", Y.shape)\nX = X[:10000]\nY = Y[:10000]\n# Hyperparameters \nembed_dim = 256 \nnum_heads = 4 \nff_dim = 512 \nnum_layers = 4 \n\n# Build the Transformer model \nmodel = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n\n# Provide input shape to build the model by passing a dummy input with maxval specified\n_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n\n# Compile the model \nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Summary of the model \nmodel.summary()\n# Early stopping callback to stop training if the loss doesn't improve\nearly_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n\n# Train the transformer model on the full input and target sequences\nhistory = model.fit(X, Y, epochs=2, batch_size=32, callbacks=[early_stopping])\n\n# Plot training loss to monitor model performance over epochs\nplt.plot(history.history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 2: Add a learning rate scheduler \n\n**Objective:** Implement a learning rate scheduler to adjust the learning rate during training. \n\n**Instructions:**\n\n- Define a learning rate scheduler that reduces the learning rate by half every 10 epochs \n\n- Train the model with the learning rate scheduler and compare the training loss \n","metadata":{}},{"cell_type":"code","source":"# Write your code here\n!pip install pandas\n!pip install scikit-learn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Define a learning rate scheduler  \ndef scheduler(epoch, lr):  \n    if epoch % 10 == 0 and epoch != 0:  \n        lr = lr * 0.5  \n    return lr  \n \n\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)  \n\n\n# Train the model with the learning rate scheduler  \nhistory = model.fit(X, Y, epochs=2, batch_size=64, callbacks=[callback])  \n   \n\n# Plot the training loss  \nplt.plot(history.history['loss'])  \nplt.xlabel('Epoch')  \nplt.ylabel('Loss')  \nplt.title('Training Loss with Learning Rate Scheduler')  \nplt.show() \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ejercicio 3: Generar secuencias de texto más largas\n\n**Objetivo:** Explorar las capacidades de generación de texto del modelo y generar secuencias más largas.\n\n**Instrucciones:**\n\n- Modificar la función `generate_text` para generar 200 tokens en lugar de 100.\n\n- Generar texto utilizando el modelo entrenado y la función modificada.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\ndef generate_text(model, start_string, num_generate=200):\n    # Convert the start string to numbers (vectorize)\n    input_eval = vectorizer([start_string]).numpy()\n\n    # Ensure the input tensor has the correct shape\n    input_eval = tf.convert_to_tensor(input_eval[:, -5:])  # Ensure it has a shape of (1, 5)\n    \n    text_generated = []\n\n    for i in range(num_generate):\n        # Make predictions using the model\n        predictions = model(input_eval)\n\n        # Ensure predictions is a matrix with shape [batch_size, num_classes]\n        predictions = tf.squeeze(predictions, 0)  # Remove the batch dimension\n        predictions = tf.expand_dims(predictions, 0)  # Add back a batch dimension for categorical\n        \n        # Use a categorical distribution to predict the next word\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n\n        # Update the input tensor to include the predicted word, maintaining the sequence length\n        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)  # Append predicted token\n        input_eval = input_eval[:, -5:]  # Keep only the last 5 tokens to match input shape\n        input_eval = tf.convert_to_tensor(input_eval)  # Convert back to tensor\n        \n        # Add the predicted word to the generated text\n        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n\n    return start_string + ' ' + ' '.join(text_generated)\n\n\n# Generate longer text\nstart_string = \"To be, or not to be\"\ngenerated_text = generate_text(model, start_string)\n\nprint(generated_text)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Conclusión\n\nEn el siguiente proyecto se ha creado y entrenado con éxito un modelo Transformer para la generación de texto utilizando TensorFlow y Keras.\nSe preprocesar datos de texto, crearon secuencias de entrada y destino, definieron la arquitectura del modelo Transformer, entrenamos el modelo y generar texto utilizando el modelo entrenado. \nSe demostró la experiencia práctica con Transformers para la generación de texto y exploró las aplicaciones prácticas de esta robusta arquitectura de modelo.\n","metadata":{}}]}