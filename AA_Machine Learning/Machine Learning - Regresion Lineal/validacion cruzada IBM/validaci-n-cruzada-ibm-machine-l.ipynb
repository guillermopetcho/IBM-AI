{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Foundation\n\n## Section 2, Part c: Cross Validation \n","metadata":{}},{"cell_type":"markdown","source":"## Learning objectives\n\nBy the end of this lesson, you will be able to:\n\n* Chain multiple data processing steps together using `Pipeline`\n* Use the `KFolds` object to split data into multiple folds.\n* Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n","metadata":{}},{"cell_type":"code","source":"# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n!pip install numpy\n!pip install pandas\n!pip install matplotlib\n!pip install scikit-learn\nimport numpy as np\nimport pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Note we are loading a slightly different (\"cleaned\") pickle file\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle\n# boston = pickle.load(open('data/boston_housing_clean.pickle', \"rb\" ))\nboston = pickle.load(open('boston_housing_clean.pickle', \"rb\" ))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boston.keys()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boston_data = boston['dataframe']\nboston_description = boston['description']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boston_data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discussion: \n\nSuppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset. \n\nSuppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n\nWhat would the steps be, in English, to do this?\n","metadata":{}},{"cell_type":"markdown","source":" \n","metadata":{}},{"cell_type":"markdown","source":"#### Coding this up\n\nThe [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) object in SciKit Learn tells the cross validation object (see below) how to split up the data:\n","metadata":{}},{"cell_type":"code","source":"X = boston_data.drop('MEDV', axis=1)\ny = boston_data.MEDV","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kf = KFold(shuffle=True, random_state=72018, n_splits=3)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for train_index, test_index in kf.split(X):\n    print(\"Train index:\", train_index[:10], len(train_index))\n    print(\"Test index:\",test_index[:10], len(test_index))\n    print('')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from sklearn.metrics import r2_score, mean_squared_error\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    lr.fit(X_train, y_train)\n        \n    y_pred = lr.predict(X_test)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)\n    \nscores","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A bit cumbersome, but do-able.\n","metadata":{}},{"cell_type":"markdown","source":"### **Resumen del texto:**\nEste cuaderno pertenece al segundo curso sobre validación cruzada y se enfoca en la **canalización de procesamiento de datos** para optimizar flujos de trabajo en **Machine Learning**. Se cubren los siguientes temas clave:\n\n- Uso de **KFolds** para dividir datos en múltiples pliegues y realizar validación cruzada.\n- Implementación de **cross_val_predict** y **GridSearchCV** para evaluar el rendimiento del modelo.\n- Introducción a la **regresión lineal**, junto con técnicas de **Lasso y Ridge** para evitar sobreajuste.\n- Uso de **Pickle** para cargar y manejar datos en formato de diccionario.\n- Aplicación práctica en el conjunto de datos **Boston Housing**, donde el objetivo es predecir el valor medio de viviendas usando diferentes características.\n- Implementación de la validación cruzada mediante **tres pliegues**, asegurando que los conjuntos de prueba sean exclusivos.\n- Uso de **r2_score** para medir el rendimiento del modelo en cada iteración.\n- Destacar la importancia de realizar múltiples pliegues y promediar los resultados para mejorar la generalización del modelo.\n- Próximo paso: agregar escalado y aplicar predicción cruzada con **cross_val_predict**.\n\nEste cuaderno proporciona una base sólida para aplicar validación cruzada de manera eficiente en modelos de Machine Learning.","metadata":{}},{"cell_type":"markdown","source":"### Discussion (Part 2): \n\nNow suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n\nWhat would the steps be _now_?\n","metadata":{}},{"cell_type":"markdown","source":"### Coding this up\n","metadata":{}},{"cell_type":"code","source":"scores = []\n\nlr = LinearRegression()\ns = StandardScaler()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    X_train_s = s.fit_transform(X_train)\n    \n    lr.fit(X_train_s, y_train)\n    \n    X_test_s = s.transform(X_test)\n    \n    y_pred = lr.predict(X_test_s)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)\n","metadata":{}},{"cell_type":"markdown","source":"This is getting quite cumbersome! \n\n_Very_ luckily, SciKit Learn has some wonderful functions that handle a lot of this for us.\n","metadata":{}},{"cell_type":"markdown","source":"### `Pipeline` and `cross_val_predict`\n","metadata":{}},{"cell_type":"markdown","source":"`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n","metadata":{}},{"cell_type":"code","source":"s = StandardScaler()\nlr = LinearRegression()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combine multiple processing steps into a `Pipeline`\n\nA pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  \n","metadata":{}},{"cell_type":"code","source":"estimator = Pipeline([(\"scaler\", s),\n                      (\"regression\", lr)])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `cross_val_predict`\n\n[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.\n","metadata":{}},{"cell_type":"code","source":"kf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = cross_val_predict(estimator, X, y, cv=kf)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r2_score(y, predictions)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.mean(scores) # almost identical!","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n\nWhen we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. \n","metadata":{}},{"cell_type":"markdown","source":"### **Resumen del texto:**\nEn esta segunda parte del cuaderno, se introduce el **preprocesamiento de datos**, la **canalización (pipeline)** y la **predicción cruzada (cross_val_predict)**, con el objetivo de optimizar el flujo de trabajo en Machine Learning.\n\n#### **Temas principales:**\n- **Escalado de datos** con **StandardScaler**, normalizando los valores para mejorar la estabilidad del modelo.\n- **Automatización con pipelines** de `sklearn.pipeline`, permitiendo encadenar varios pasos como el escalado y la regresión lineal.\n- **Uso de `cross_val_predict`**, que divide los datos en pliegues de entrenamiento y prueba sin necesidad de un bucle manual.\n- Se demuestra que **escalar los datos no afecta el rendimiento** en una regresión lineal sin regularización, pero es esencial para técnicas como **Lasso y Ridge**.\n- Explicación del proceso de **validación cruzada**, donde:\n  - Los datos se dividen en **tres pliegues**.\n  - Se entrena el modelo en **dos tercios** y se prueba en el **tercio restante**.\n  - Se repite el proceso para cubrir todo el conjunto de datos.\n  - Se obtiene el **r2_score** para evaluar el desempeño del modelo.\n- Se aclara que **cross_val_predict no ajusta un solo modelo**, sino que entrena múltiples modelos en distintos subconjuntos de datos.\n\n#### **Próximos pasos:**\n- En la siguiente sección, se abordará el **ajuste de hiperparámetros**, que no son aprendidos directamente por el modelo, sino que deben ser configurados manualmente para mejorar el rendimiento.\n\nEste cuaderno proporciona una **forma más eficiente de entrenar y validar modelos de Machine Learning** mediante la automatización de tareas repetitivas.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter tuning\n","metadata":{}},{"cell_type":"markdown","source":"### Definition\n\n**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that _generalizes_ well outside of your sample.\n\n### Mechanics\n\nWe can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html#numpy.geomspace) function.\n\n```python\nnp.geomspace(1, 1000, num=4)\n```\n\nproduces:\n\n```\narray([    1.,    10.,   100.,  1000.])\n```\n\nUse this function to generate a list of length 10 called `alphas` for hyperparameter tuning:\n","metadata":{}},{"cell_type":"code","source":"alphas = np.geomspace(1e-9, 1e0, num=10)\nalphas","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code below tunes the `alpha` hyperparameter for Lasso regression.\n","metadata":{}},{"cell_type":"code","source":"scores = []\ncoefs = []\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    \n    scores.append(score)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(zip(alphas,scores))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Lasso(alpha=1e-6).fit(X, y).coef_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Lasso(alpha=1.0).fit(X, y).coef_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.semilogx(alphas, scores, '-o')\nplt.xlabel('$\\\\alpha$')\nplt.ylabel('$R^2$');","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise\n\nAdd `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n\n**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n\n**Hint #2:** you should see a significant increase in cross validation accuracy from doing this\n","metadata":{}},{"cell_type":"code","source":"pf = PolynomialFeatures(degree=2)\nscores = []\n\nalphas = np.geomspace(0.001, 10, 5)\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    estimator = Pipeline([\n        (\"make_higher_degree\", pf),\n        (\"scaler\", s),\n        (\"lasso_regression\", las)])\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    score = r2_score(y, predictions)\n    \n    scores.append(score)\nscores","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you store the results in a list called `scores`, the following will work:\n","metadata":{}},{"cell_type":"code","source":"plt.semilogx(alphas, scores);","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n                    (\"scaler\", s),\n                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_estimator.named_steps[\"lasso_regression\"].coef_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Resumen del texto:**\nEsta sección del cuaderno se enfoca en el **ajuste de hiperparámetros**, particularmente en el uso de **regresión de Lasso** para regularizar modelos y mejorar su capacidad de generalización.\n\n#### **Puntos clave:**\n- **Diferencia entre hiperparámetros y parámetros:**  \n  - **Parámetros** → Aprendidos automáticamente por el modelo.  \n  - **Hiperparámetros** → Configurados manualmente para optimizar el rendimiento.  \n- **Propósito del ajuste de hiperparámetros:**  \n  - Encontrar el equilibrio entre **complejidad y error** mediante validación cruzada.\n  - Identificar los hiperparámetros óptimos que generalicen bien a datos nuevos.\n- **Uso de `np.geomspace` para definir valores de alfa:**  \n  - Genera una secuencia de valores espaciados geométricamente para probar distintos niveles de regularización.\n  - Valores más bajos → Modelos más complejos.  \n  - Valores más altos → Modelos más simples.  \n- **Implementación de regresión de Lasso con `Pipeline`:**  \n  - Se escalán los datos (`StandardScaler`), luego se entrena la regresión (`Lasso`).\n  - Se utiliza **validación cruzada (`cross_val_predict`)** para evaluar el desempeño en cada valor de alfa.\n  - Se selecciona el **alfa óptimo**, que equilibra precisión y generalización.\n- **Impacto de alfa en los coeficientes:**  \n  - **Alfa bajo** → Modelos más complejos con más coeficientes activos.  \n  - **Alfa alto** → Modelos más simples con coeficientes reducidos a cero (eliminación de variables irrelevantes).  \n- **Expansión del modelo con características polinomiales:**  \n  - Se agregan términos de interacción entre variables para mejorar el poder predictivo.\n  - **El orden importa:** Primero se crean características polinomiales y luego se escalan.\n- **Optimización de iteraciones máximas en Lasso:**  \n  - Se aumenta `max_iter` para garantizar la convergencia del modelo.\n- **Selección del mejor modelo:**  \n  - Se identifica el **mejor valor de alfa** basado en su rendimiento en validación cruzada.\n  - Se ajusta el modelo final con ese alfa y se evalúan los coeficientes eliminados por regularización.\n\n#### **Próximos pasos:**\n- **Explorar la regresión de Ridge**, que funciona de manera similar pero con una penalización diferente.\n- **Comparar modelos** para encontrar la mejor estrategia de regularización.\n\nEste cuaderno proporciona una guía estructurada para **ajustar modelos de Machine Learning** de manera eficiente y mejorar su capacidad de generalización a nuevos datos.","metadata":{}},{"cell_type":"markdown","source":"### Exercise\n\nDo the same, but with `Ridge` regression \n\nWhich model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?\n","metadata":{}},{"cell_type":"code","source":"pf = PolynomialFeatures(degree=2)\nscores = []\n\nalphas = np.geomspace(4, 20, 20)\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha, max_iter=100000)\n\n    estimator = Pipeline([\n        (\"polynomial_features\", pf),\n        (\"scaler\", s),\n        (\"ridge_regression\", ridge)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    score = r2_score(y, predictions)\n    scores.append(score)\nplt.plot(alphas, scores)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!\n","metadata":{}},{"cell_type":"markdown","source":"### Exercise:\n","metadata":{}},{"cell_type":"markdown","source":"Now, for whatever your best overall hyperparameter was: \n\n* Standardize the data\n* Fit and predict on the entire dataset\n* See what the largest coefficients were\n    * Hint: use \n    ```python\n    dict(zip(model.coef_, pf.get_feature_names()))\n    ```\n    for your model `model` to get the feature names from `PolynomialFeatures`.\n    \n    Then, use\n    ```python\n    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n    ```\n    \n    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n","metadata":{}},{"cell_type":"code","source":"# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2, include_bias=False)),\n                    (\"scaler\", s),\n                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names(),\n                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"col_names_dict","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_importances.sort_values(by=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grid Search CV\n","metadata":{}},{"cell_type":"markdown","source":"To do cross-validation, we used two techniques:\n- use `KFolds` and manually create a loop to do cross-validation\n- use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n\nTo do hyper-parameter tuning, we see a general pattern:\n- use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n\nPerhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Same estimator as before\nestimator = Pipeline([(\"polynomial_features\", PolynomialFeatures()),\n        (\"scaler\", StandardScaler()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'ridge_regression__alpha': np.geomspace(4, 20, 20)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid.fit(X, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid.best_score_, grid.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import (StandardScaler, \n                                   PolynomialFeatures)\nfrom scipy.stats.mstats import normaltest\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n\nfile_name='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ST0151EN-SkillsNetwork/labs/boston_housing.csv'\nboston_data = pd.read_csv(file_name)\n\nlr = LinearRegression()\ny_col = \"MEDV\"\nX = boston_data.drop(y_col, axis=1)\ny = boston_data[y_col]\n\npf = PolynomialFeatures(degree=2, include_bias=False)\nX_pf = pf.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n                                                    random_state=72018)\n\ns = StandardScaler()\nX_train_s = s.fit_transform(X_train)\n\nbc_result = boxcox(y_train)\ny_train_bc = bc_result[0]\nlam = bc_result[1]\n\nlr.fit(X_train_s, y_train_bc)\nX_test_s = s.transform(X_test)\ny_pred_bc = lr.predict(X_test_s)\n\ny_pred_tran = inv_boxcox(y_pred_bc, lam)\nprint(r2_score(y_pred_tran,y_test)) #RES 0.848052537981275\n\nlr = LinearRegression()\nlr.fit(X_train_s,y_train)\nlr_pred = lr.predict(X_test_s)\nr2_score(lr_pred,y_test) #RES 0.8667029116056716\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_predict = grid.predict(X)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This includes both in-sample and out-of-sample\nr2_score(y, y_predict)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Notice that \"grid\" is a fit object!\n# We can use grid.predict(X_test) to get brand new predictions!\ngrid.best_estimator_.named_steps['ridge_regression'].coef_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid.cv_results_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary\n\n1. We can manually generate folds by using `KFolds`\n2. We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`. \n   This will produce the out-of-bag prediction for each row.\n3. When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n4. .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n\n### Extensions\n\nHere are some additional items to keep in mind:\n* There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n* KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`.\n","metadata":{}}]}