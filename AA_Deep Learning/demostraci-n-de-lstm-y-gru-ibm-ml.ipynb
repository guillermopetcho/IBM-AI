{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LSTM and GRU Demo (Activity)**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **45** minutes\n","metadata":{}},{"cell_type":"markdown","source":"Vanilla RNNs work well when dealing with short-term dependencies but suffer from the vanishing gradient descent problem when it comes to long-term context dependencies. This is because some information is lost at each time step when traversing the layers. Gated RNNs have units that are designed to forget and to update relevant information and can provide a good solution to this problem. In this lab, we will talk about two types of gated RNNs: LSTM and GRUs.\n","metadata":{}},{"cell_type":"markdown","source":"## __Table of Contents__\n\n<ol>\n    <li><a href=\"#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"#Gated-RNNs\">Gated RNNs</a>\n        <ol>\n            <li><a href=\"#LSTM\">LSTM</a></li>\n            <li><a href=\"#GRU\">GRU</a></li>\n            <li><a href=\"#LSTM in Keras: Time-series forecasting\">LSTM in Keras: Time-series forecasting</a></li>\n            <li><a href=\"#Exercise 1: GRU in Keras - Reuters classification\">Exercise 1: GRU in Keras - Reuters classification</a></li>\n        </ol>\n    </li>\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab, you will be able to:\n\n - Explain the fundamental components of LSTM and GRU\n - Implement LSTM and GRU in Keras for various tasks and applications\n","metadata":{}},{"cell_type":"markdown","source":"----\n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!mamba install -qy tqdm\n!pip install tensorflow --upgrade","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n","metadata":{}},{"cell_type":"code","source":"# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport tensorflow as tf\nprint(tf. __version__)\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.losses import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Embedding,Masking,LSTM, GRU, Conv1D, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, SimpleRNN\nfrom tensorflow.keras.datasets import reuters\nfrom keras.utils import pad_sequences\n\n\nsns.set_context('notebook')\nsns.set_style('white')\nnp.random.seed(2024)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gated RNN\n\nThe two types of gated RNNs we will be studying in this lab are Long Short Term Memories (LSTM) and Gated Recurrent Units (GRU). GRU is simpler than LSTM. It's much faster and optimizes quicker. \n\n### LSTM\n\nThe key idea of LSTMs is to have two state representations: the hidden state $\\mathbf h$ and the cell state $\\mathbf C$ (instead of $\\mathbf s$). \n\nAn LSTM cell has a complex internal structure that makes it able to:\n\n* learn to recognize an important input,\n* store it in the long-term state,\n* preserve it for as long as it is needed,\n* extract it whenever it is needed.\n\n<img src=\"https://wiki.math.uwaterloo.ca/statwiki/images/thumb/9/98/LSTM.png/800px-LSTM.png\" alt=\"Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" style=\"width: 500px;\"> \n\nSource: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\nLSTM has the ability to remove or add information to the cell state, carefully regulated by structures called gates, which are a way to optionally let information through. There are three gates in an LSTM; forget gate, input gate, and output gate. They are composed out of a *sigmoid* neural net layer. Sigmoid-based layers output values near either 0 (gate closed) or 1 (gate open).\n","metadata":{}},{"cell_type":"markdown","source":"### GRU\n\nGRU is a simplification of the LSTM cell that performs similarly well while being faster to train. It has a single update gate controller that manages both the forget and input gates. Whenever one is open, the other is closed. There is no output gate. The reset gate controls which part of the previous state is shown to the main layer. The resulting model is simpler than standard LSTM models. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize.\n\n<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/gru.png?raw=1\" alt=\"Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" style=\"width: 300px;\"> \n","metadata":{}},{"cell_type":"markdown","source":"### LSTM in Keras: Time-series forecasting\n","metadata":{}},{"cell_type":"markdown","source":"Next, we will build a simple LSTM model to solve a many-to-one time-series prediction problem on a simulated sine wave.\n","metadata":{}},{"cell_type":"markdown","source":"Let's first start by generating some sine waves using the `np.sin()` function.\n","metadata":{}},{"cell_type":"code","source":"x = np.linspace(0, 50, 501)\ny = np.sin(x)\nplt.plot(x, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will convert this into a data frame for convenience.\n","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data=y, index=x, columns=['Sine'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are dealing with a univariate time series, so we have a total of one feature. We will be using 10% of the simulated data for testing.\n","metadata":{}},{"cell_type":"code","source":"# percentage of data used for testing\ntest_percent = 0.1\n# number of features\nn_features = 1\n# sequence length\nlength = 50\n# batch size \nbatch_size = 1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now create a training and testing dataset to train and test our model on.\n","metadata":{}},{"cell_type":"code","source":"test_point = np.round(len(df)*test_percent)\ntest_ind = int(len(df)-test_point)\n\ntrain = df.iloc[:test_ind]\ntest = df.iloc[test_ind:]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will perform some basic preprocessing using the `MinMaxScaler`, which normalizes the input.\n","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train)\nscaled_test = scaler.transform(test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`TimeseriesGenerator` is a utility class by Keras that is used in generating batches of temporal data. Given our scaled training data, we create 401 output sequences.\n","metadata":{}},{"cell_type":"code","source":"generator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)\nlen(generator)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will use the `LSTM()` layer with 50 units, and an input shape defined by the sequence length and number of features.\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(LSTM(50, input_shape=(length, n_features)))\n\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit_generator(generator, epochs=6)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will now evaluate the LSTM's performance in forecasting a few time steps from the first batch.\n","metadata":{}},{"cell_type":"code","source":"forecast = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(25):\n    current_pred = model.predict(current_batch)[0]\n    forecast.append(current_pred)\n    current_batch = np.append(current_batch[:, 1:, :], [[current_pred]], axis=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will invert the min-max scaling on the predictions to allow for a direct comparison with the ground truth.\n","metadata":{}},{"cell_type":"code","source":"forecast = scaler.inverse_transform(forecast)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecast_index = np.arange(50.1, 52.6, step=0.1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(df.index, df['Sine'])\nplt.plot(forecast_index, forecast)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1: GRU in Keras - Reuters classification\n\nIn this exercise, you will use the reuters dataset from Keras that we imported earlier to build a classification model using GRUs.\n","metadata":{}},{"cell_type":"markdown","source":"The dataset used is the Reuters newswire dataset from Keras. We have 11,228 newswires from Reuters, labeled over 46 topics.\n\n> Each newswire is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n\nIt returns the following:\n\n> x_train, x_test: lists of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words - 1.\n","metadata":{}},{"cell_type":"markdown","source":"Let us start by defining a few hyperparameters. \n\n* We use 10,000 to specify `num_words`, that is, the maximum possible index value loaded is `num_words` - 1. \n\n* Input length of the embedding layer is specified using `maxlen`\n\n* We will use 30% of the data for testing.\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nnum_words = 10000\nmaxlen = 1000\ntest_split = 0.3","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will use the Keras API to load in our reuters dataset.\n","metadata":{}},{"cell_type":"code","source":"# Load reuters Data from Keras datasets\n(X_train, y_train),(X_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.3)\nprint(len(X_train), 'train sequences')\nprint(len(X_test), 'test sequences')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In terms of pre-processing, we pad sequences to the same length using `pad_sequences`. Sequences longer than `num_timesteps` are truncated so that they fit the desired length. Whereas sequences that are shorter than `num_timesteps` are padded with value until they are `num_timesteps` long.\n","metadata":{}},{"cell_type":"code","source":"# data preprocessing\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\n\n\ny_train = tf.keras.utils.to_categorical(y_train, 46)\ny_test = tf.keras.utils.to_categorical(y_test, 46)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Start by defining the model architecture. Use a `GRU` layer with 128 units, and a dropout rate of 0.2. Use a dense layer with 46 units and softmax as the activation function. Print out the model summary.\n","metadata":{}},{"cell_type":"code","source":"# Write your solution here\nmodel = Sequential()\nmodel.add(Embedding(input_dim = num_words, output_dim = 300,input_length=1000))\nmodel.add(GRU(128, dropout=0.2))\nmodel.add(Dense(46, activation='softmax'))\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now compile the model using categorical_crossentropy as the loss function, adam as the optimizer and accuracy as the metric. Train the model using 256 as the batch size, over 10 training iterations and using 20% of the data for validation purposes. Finally, evaluate the model's performance on the test dataset.\n","metadata":{}},{"cell_type":"code","source":"# Write your solution here\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(X_train, y_train,batch_size=256,epochs=10,validation_split=0.2)\nmodel.evaluate(X_test,y_test)","metadata":{},"outputs":[],"execution_count":null}]}