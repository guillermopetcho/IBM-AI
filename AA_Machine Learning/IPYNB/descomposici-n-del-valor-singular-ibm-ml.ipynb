{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Singular-Value Decomposition (SVD)\n\nEstimated time needed: **45** minutes\n\nYou are a computer vision engineer and a self-driving car company has hired you.  The company would like you to remove all objects other than the street from several security videos using singular value decomposition to help test their vision system. You will use the  <a href=\"www.svcl.ucsd.edu/projects/background_subtraction/ucsdbgsub_dataset.html\"> Background Subtraction in Dynamic dataset  </a> from the  Statistical Visual Computing Laboratory (SVCL) at UCSD.\n\n<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/15104006386_1bf6bfe96a_b.jpeg\" width=\"600\">\n\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n<ol>\n    <li><a href=\"#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n        </ol>\n    </li>\n    <li><a href=\"#Singular Value Decomposition\">Singular Value Decomposition</a></li>\n    <li>\n        <a href=\"#Truncated SVD\">Truncated SVD</a>\n        <ol>\n            <li><a href=\"#Truncated SVD in Sklearn\">Truncated SVD in Sklearn</a></li>\n            <li><a href=\"#Background Model using SVD\">Background Model using SVD</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"#Exercises\">Exercises</a>\n        <ol>\n            <li><a href=\"#Exercise 1\">Exercise 1</a></li>\n            <li><a href=\"#Exercise 2\">Exercise 2</a></li>\n            <li><a href=\"#Exercise 3\">Exercise 3</a></li>\n            <li><a href=\"#Exercise 4\">Exercise 4</a></li>\n            <li><a href=\"#Exercise 5\">Exercise 5</a></li>\n        </ol>\n    </li>\n    <li><a href=\"#SVD from Scratch (optional)\">SVD from Scratch (optional)</a></li>\n    <li><a href=\"#Relationship between SVD and PCA (optional)\">Relationship between SVD and PCA (optional)</a></li>\n    \n </ol>    \n     \n  \n","metadata":{}},{"cell_type":"markdown","source":"***\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\n\n*   **Understand** what is SVD in terms of Matrix Decomposition\n*   **Understand** Truncated SVD \n*   **Apply** Truncated SVD  using numpy and Sklearn\n*   **Apply** Truncated SVD  to real data \n*   **Understand** the relationship between SVD and PCA (optional)\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Datasets\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Download and unzip the datasets:\n","metadata":{"tags":[]}},{"cell_type":"code","source":"import skillsnetwork\n\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/traffic.tar.gz\")\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/peds.tar.gz\")\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/boats.tar.gz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.568376Z","iopub.execute_input":"2025-07-15T16:27:07.568671Z","iopub.status.idle":"2025-07-15T16:27:07.700566Z","shell.execute_reply.started":"2025-07-15T16:27:07.568634Z","shell.execute_reply":"2025-07-15T16:27:07.695860Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2499557078.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskillsnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mskillsnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/traffic.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mskillsnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/peds.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mskillsnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/boats.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skillsnetwork'"],"ename":"ModuleNotFoundError","evalue":"No module named 'skillsnetwork'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing  required libraries\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The following required modules are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Anaconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install pandas==1.3.4 ...\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.701445Z","iopub.status.idle":"2025-07-15T16:27:07.701820Z","shell.execute_reply.started":"2025-07-15T16:27:07.701631Z","shell.execute_reply":"2025-07-15T16:27:07.701646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Install <a href=\"https://www.sympy.org/en/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01\">SymPy </a> for printing matrices \n","metadata":{}},{"cell_type":"code","source":"!conda install -c anaconda sympy -y","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.703259Z","iopub.status.idle":"2025-07-15T16:27:07.703857Z","shell.execute_reply.started":"2025-07-15T16:27:07.703578Z","shell.execute_reply":"2025-07-15T16:27:07.703607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n_We recommend you import all required libraries in one place (here):_\n","metadata":{}},{"cell_type":"code","source":"# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.704735Z","iopub.status.idle":"2025-07-15T16:27:07.705218Z","shell.execute_reply.started":"2025-07-15T16:27:07.705008Z","shell.execute_reply":"2025-07-15T16:27:07.705027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\n\nfrom os import listdir,getcwd\nfrom os.path import isfile, join\nfrom random import randint\nfrom PIL import Image\n\nimport seaborn as sns \nimport matplotlib.pylab as plt\n%matplotlib inline\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n\nfrom sympy import Matrix, init_printing,Symbol\nfrom numpy.linalg import qr,eig,inv,matrix_rank,inv,svd\ninit_printing()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.707835Z","iopub.status.idle":"2025-07-15T16:27:07.708477Z","shell.execute_reply.started":"2025-07-15T16:27:07.708260Z","shell.execute_reply":"2025-07-15T16:27:07.708279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining Helper functions\n","metadata":{}},{"cell_type":"code","source":"def get_data_Matrix (mypath=\"peds\"):\n    cwd = getcwd()\n\n    mypath=join(cwd,mypath)\n    files = [ join(mypath,f) for f in listdir(mypath) if isfile(join(mypath, f)) and f.startswith(\".\")==False]\n    # Read image\n    img = Image.open(files[0])\n    I=np.array(img)\n    # Output Images\n\n    Length,Width=I.shape\n   \n    X=np.zeros((len(files),Length*Width))\n    for i,file in enumerate(files):\n        img = Image.open(file)\n        I=np.array(img)\n        X[i,:]=I.reshape(1,-1)\n    return X,Length,Width","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.709700Z","iopub.status.idle":"2025-07-15T16:27:07.710548Z","shell.execute_reply.started":"2025-07-15T16:27:07.710311Z","shell.execute_reply":"2025-07-15T16:27:07.710337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Singular Value Decomposition\nThe Singular-Value Decomposition, or SVD for short, will decompose a real or complex $N × D$ matrix $\\mathbf{X}$ of rank $r$ as follows:\n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{X}= \\mathbf {US V^{T}}$$\n","metadata":{}},{"cell_type":"markdown","source":"In many applications  $N \\ge D$, but SVD can be used for any matrix $\\mathbf{X}$. For example, in computer vision and image processing tasks we sometimes have $D \\ge N$.\n","metadata":{}},{"cell_type":"markdown","source":"The matrix $\\mathbf{S}$ contains the nonnegative <b>singular values</b> of $\\mathbf{X}$, with diagonal entries ${\\sigma _{i}}$ else $0$, the entries are ordered by importance in descending order with respect to $i$, i.e:\n\n$$\\sigma _{1}>\\sigma _{2},..>\\sigma _{r}$$\n","metadata":{}},{"cell_type":"markdown","source":"The matrix $\\mathbf {U}$ is $NxD$ and  has the orthornormal columns  $\\mathbf{u}_{1}, ..., \\mathbf{u}_{D} $ called the <b>left singular vectors</b>.\n\nThe matrix $\\mathbf {V}$ is $DxD$ and has the orthornormal columns $\\mathbf{v}_{1}, ..., \\mathbf{v}_{D}$ called the <b>right singular vectors</b>  (note  that $\\mathbf{V}$ transpose, $\\mathbf {V^{T}}$ is returned as output in numpy's `svd` function).\n","metadata":{}},{"cell_type":"markdown","source":" SVD decomposition returns the full shape of a non-square matrix, the non colored  parts of the decomposition **N-D** terms in the matrix U are zeros, we see many of the squares are redundant.\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/matrix.png\" width=\"600\" alt=\"full svd\">\n","metadata":{}},{"cell_type":"markdown","source":"Consider the matrix $\\mathbf{X}$:\n","metadata":{}},{"cell_type":"code","source":"X=np.array([[1.0,2],[2,1],[3,3]])\nMatrix(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.711688Z","iopub.status.idle":"2025-07-15T16:27:07.712840Z","shell.execute_reply.started":"2025-07-15T16:27:07.712526Z","shell.execute_reply":"2025-07-15T16:27:07.712549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can perform SVD on any matrix in numpy by using the function `svd` from `numpy.linalg`:\n","metadata":{}},{"cell_type":"code","source":"U, s, VT =svd(X, full_matrices=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.714246Z","iopub.status.idle":"2025-07-15T16:27:07.714660Z","shell.execute_reply.started":"2025-07-15T16:27:07.714455Z","shell.execute_reply":"2025-07-15T16:27:07.714474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When $\\mathbf{X}$ is a 2D array, it is factorized as $U\\times np.diag(s)\\times V^T$, where $U$ and $V^T$ are 2D orthogonal  matrices  and $s$ is an 1D array of $\\mathbf{X}$'s singular values. When $\\mathbf{X}$ is higher-dimensional (such as sparse matrices), the parameter ```full_matrices=False```, the skinny SVD is used and the zero elements are dropped. This can be summarized in the following image:\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/skinny-SVD.png\" width=\"600\" alt=\"skinnysvd\">\n","metadata":{}},{"cell_type":"markdown","source":"We have the <b>left singular vectors</b> of $\\mathbf{X}$:\n","metadata":{}},{"cell_type":"code","source":"Matrix(U)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.716090Z","iopub.status.idle":"2025-07-15T16:27:07.716498Z","shell.execute_reply.started":"2025-07-15T16:27:07.716310Z","shell.execute_reply":"2025-07-15T16:27:07.716327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have the <b>singular values</b> of $\\mathbf{X}$, as the output is an 1-D array we use the function ```np.diag``` to convert the output into a diagonal matrix:\n","metadata":{}},{"cell_type":"code","source":"S=np.diag(s)\n\nMatrix(S)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.718760Z","iopub.status.idle":"2025-07-15T16:27:07.719089Z","shell.execute_reply.started":"2025-07-15T16:27:07.718948Z","shell.execute_reply":"2025-07-15T16:27:07.718962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally we have the <b>right singular vectors</b> of $\\mathbf{X}$, as the output is transposed they are the rows of matrix ```VT```:\n","metadata":{}},{"cell_type":"code","source":"Matrix(VT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.721492Z","iopub.status.idle":"2025-07-15T16:27:07.721862Z","shell.execute_reply.started":"2025-07-15T16:27:07.721703Z","shell.execute_reply":"2025-07-15T16:27:07.721720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can reconstruct the matrix $\\mathbf{X}$:\n","metadata":{}},{"cell_type":"code","source":"X_=U@S@VT\nX_=np.round(X_)\nMatrix(X_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.723276Z","iopub.status.idle":"2025-07-15T16:27:07.723894Z","shell.execute_reply.started":"2025-07-15T16:27:07.723619Z","shell.execute_reply":"2025-07-15T16:27:07.723642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It may be more intuitive if you think of SVD reconstructing the matrix as a linear combination of $r$ rank-1 matrices and the associated singular values ($r$ is the rank of $\\mathbf{X}$): \n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{X}=\\sum_{i=1}^{r}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^{T}$$\n","metadata":{}},{"cell_type":"code","source":"X_2=s[0]*U[:,0:1]@VT[0:1,:]+s[1]*U[:,1:2]@VT[1:2,:]\nMatrix(X_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.725794Z","iopub.status.idle":"2025-07-15T16:27:07.726273Z","shell.execute_reply.started":"2025-07-15T16:27:07.726061Z","shell.execute_reply":"2025-07-15T16:27:07.726101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same matrix is returned!\n","metadata":{}},{"cell_type":"markdown","source":"## Truncated SVD\n","metadata":{}},{"cell_type":"markdown","source":"A singular value $\\sigma_i$ can be thought of as a measure of redundancy of a rank-1 matrix produced by $\\mathbf{u}_i\\mathbf{v}_i^T$. A smaller $\\sigma_i$ means the associated rank-1 matrix, which can be interpreted as one \"ingredient\" of $\\mathbf{X}$, is of less importance in terms of the amount of variance/information it preserves.\n\n\nAs the information contained by the matrix becomes less important or redundant, its corresponding singular value $\\sigma_i$ approaches zero. If we just keep the most important \"ingredients\" of $\\mathbf{X}$, we can approximate $\\mathbf{X}$ by a weighted sum of $L$ rank-1 matrices where  $L<r$:\n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{X}\\approx\\sum_{i=1}^{L}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^{T}$$\n","metadata":{}},{"cell_type":"markdown","source":"Consider the following matrix, the columns are almost linearly independent:\n","metadata":{}},{"cell_type":"code","source":"X=np.array([[1,2],[2,4],[4,8.0001]])\nMatrix(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.727171Z","iopub.status.idle":"2025-07-15T16:27:07.727500Z","shell.execute_reply.started":"2025-07-15T16:27:07.727375Z","shell.execute_reply":"2025-07-15T16:27:07.727388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We perform SVD:\n","metadata":{}},{"cell_type":"code","source":"U, s, VT =svd(X, full_matrices=False)\nS=np.diag(s)\nMatrix(S)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.728533Z","iopub.status.idle":"2025-07-15T16:27:07.728828Z","shell.execute_reply.started":"2025-07-15T16:27:07.728692Z","shell.execute_reply":"2025-07-15T16:27:07.728707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see the first rank-1 matrix is almost identical to the Matrix:\n","metadata":{}},{"cell_type":"code","source":"X_hat=np.round(s[0]*U[:,0:1]@VT[0:1,:])\nMatrix(X_hat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.730593Z","iopub.status.idle":"2025-07-15T16:27:07.730918Z","shell.execute_reply.started":"2025-07-15T16:27:07.730778Z","shell.execute_reply":"2025-07-15T16:27:07.730792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instead of adding the rank-1 matrices, we can achieve the same result with matrix multiplications. Select the top $L$ singular values, top $L$ left and right singular vectors, we can approximate $\\mathbf{X}$ by:\n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{X} \\approx \\mathbf{U}_{:,1:L} \\mathbf{S}_{1:L,1:L} \\mathbf{V}_{1:L,:}^{T}$$\n","metadata":{}},{"cell_type":"markdown","source":"Summarized in the following image:\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/trunked_-svd.png\" width=\"600\" alt=\"skinnysvd\">\n","metadata":{}},{"cell_type":"markdown","source":"We can code the values in numpy:\n","metadata":{}},{"cell_type":"code","source":"L=1\nXhat=U[:,:L]@S[0:L,0:L]@VT[:L,:]\nMatrix(Xhat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.732546Z","iopub.status.idle":"2025-07-15T16:27:07.732864Z","shell.execute_reply.started":"2025-07-15T16:27:07.732722Z","shell.execute_reply":"2025-07-15T16:27:07.732735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use the singular values to determine the reconstruction error, similar to the **cumulative explained variance**, we will go into the exact relationship later. The term is given by: \n","metadata":{}},{"cell_type":"markdown","source":"$$C(L)=\\dfrac{\\sum_{i=1}^{L}\\sigma_i}{\\sum_{i=1}^{r}\\sigma_i}$$\n","metadata":{}},{"cell_type":"code","source":"print(f\"With {L} singular value and its corresponding singular vectors, {s[0:L]/s.sum()} variance of X is explained\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.734237Z","iopub.status.idle":"2025-07-15T16:27:07.734598Z","shell.execute_reply.started":"2025-07-15T16:27:07.734458Z","shell.execute_reply":"2025-07-15T16:27:07.734474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot the **cumulative explained variance** as a function of $L$ (the number of singular values chosen).\n","metadata":{}},{"cell_type":"code","source":"plt.figure()\nplt.plot(np.cumsum(s)/s.sum())\nplt.xlabel('L')\nplt.title('Cumulative explained singular value')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.736109Z","iopub.status.idle":"2025-07-15T16:27:07.736443Z","shell.execute_reply.started":"2025-07-15T16:27:07.736308Z","shell.execute_reply":"2025-07-15T16:27:07.736325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Truncated SVD in Sklearn\n","metadata":{}},{"cell_type":"markdown","source":"Truncated SVD  performs **linear  PCA**, a method of dimensionality reduction. Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently. We will explore the relationship between PCA and SVD later on. \n\nSome important parameters in `sklearn.decomposition.TruncatedSVD` that are worth noting:\n\n- `n_components`: int, default=2; Desired dimensionality of output data. If algorithm=’arpack’, must be strictly less than the number of features. If algorithm=’randomized’, must be less than or equal to the number of features. The default value is useful for visualisation.\n\n- `algorithm`: {‘arpack’, ‘randomized’}, default=’randomized’; SVD solver to use. Either “arpack” for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or “randomized” for the randomized algorithm due to [Halko (2009)](https://arxiv.org/abs/0909.4061?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01).\n\n- `n_iter`: int, default=5; Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in [randomized_svd](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_svd.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#sklearn.utils.extmath.randomized_svd) to handle sparse matrices that may have large slowly decaying spectrum.\n\n_Read more in the scikit-learn documentation of [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01)._\n","metadata":{}},{"cell_type":"markdown","source":"First we create a ```TruncatedSVD``` object setting ```n_components=1``` this is analogous to ```L=1``` :\n","metadata":{}},{"cell_type":"code","source":"svd_ = TruncatedSVD(n_components=1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.737781Z","iopub.status.idle":"2025-07-15T16:27:07.738115Z","shell.execute_reply.started":"2025-07-15T16:27:07.737945Z","shell.execute_reply":"2025-07-15T16:27:07.737957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```fit_transform```  behaves like the projections onto the principle components \n","metadata":{}},{"cell_type":"code","source":"Z=svd_.fit_transform(X)\nZ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.739319Z","iopub.status.idle":"2025-07-15T16:27:07.739616Z","shell.execute_reply.started":"2025-07-15T16:27:07.739488Z","shell.execute_reply":"2025-07-15T16:27:07.739504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With `inverse_transform`, we can find the approximation of the original $X$:\n","metadata":{}},{"cell_type":"code","source":"Xhat=svd_.inverse_transform(Z)\nMatrix(np.round(Xhat))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.747147Z","iopub.status.idle":"2025-07-15T16:27:07.747853Z","shell.execute_reply.started":"2025-07-15T16:27:07.747682Z","shell.execute_reply":"2025-07-15T16:27:07.747712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Background Model using SVD\n","metadata":{}},{"cell_type":"markdown","source":"In this section you will use SVD for developing a Background Model. \n\nBackground subtraction is a widely used approach to detect moving objects in a sequence of frames from static cameras. The base in this approach is detecting moving objects from the differences between the current frame and reference frame, which is often called 'Background Image' or 'Background Model'. \n\nThe function ```get_data_Matrix```  will create a Design  matrix ```X``` where each row corresponds to a flattened image of a sidewalk with pedestrians recorded by a camera.\n\n```Length``` and ```Width``` are the rectangular dimensions of the image. We will use SVD to remove the pedestrians: \n","metadata":{}},{"cell_type":"code","source":"X,Length,Width=get_data_Matrix(mypath=\"peds\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.749717Z","iopub.status.idle":"2025-07-15T16:27:07.750228Z","shell.execute_reply.started":"2025-07-15T16:27:07.749918Z","shell.execute_reply":"2025-07-15T16:27:07.749933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are 170 images in the dataset and each is $152\\times 232$ (32654 pixels).\n","metadata":{}},{"cell_type":"code","source":"X.shape, Length, Width","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.752076Z","iopub.status.idle":"2025-07-15T16:27:07.753575Z","shell.execute_reply.started":"2025-07-15T16:27:07.753361Z","shell.execute_reply":"2025-07-15T16:27:07.753389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot five random images from the matrix:\n","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    frame=randint(0, X.shape[0]-1)\n    plt.imshow(X[randint(0, X.shape[0]-1),:].reshape(Length,Width),cmap=\"gray\")\n    plt.title(\"frame: \"+str(frame))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.754864Z","iopub.status.idle":"2025-07-15T16:27:07.755269Z","shell.execute_reply.started":"2025-07-15T16:27:07.755063Z","shell.execute_reply":"2025-07-15T16:27:07.755091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We perform SVD on the images:\n","metadata":{}},{"cell_type":"code","source":"U, s, VT =svd(X, full_matrices=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.756662Z","iopub.status.idle":"2025-07-15T16:27:07.756985Z","shell.execute_reply.started":"2025-07-15T16:27:07.756850Z","shell.execute_reply":"2025-07-15T16:27:07.756867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"S=np.diag(s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.758457Z","iopub.status.idle":"2025-07-15T16:27:07.758776Z","shell.execute_reply.started":"2025-07-15T16:27:07.758634Z","shell.execute_reply":"2025-07-15T16:27:07.758651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can reconstruct the image using Truncated SVD with ```L=1``` and assign the result to to ```Xhat```:\n","metadata":{}},{"cell_type":"code","source":"L=1\nXhat=U[:,:L]@S[0:L,0:L]@VT[:L,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.759384Z","iopub.status.idle":"2025-07-15T16:27:07.759612Z","shell.execute_reply.started":"2025-07-15T16:27:07.759504Z","shell.execute_reply":"2025-07-15T16:27:07.759514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we plot the first image, we will see the pedestrians are gone:\n","metadata":{}},{"cell_type":"code","source":"plt.imshow(Xhat[0,:].reshape(Length,Width),cmap=\"gray\")\nplt.title('Truncated SVD L=1')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.759978Z","iopub.status.idle":"2025-07-15T16:27:07.760179Z","shell.execute_reply.started":"2025-07-15T16:27:07.760077Z","shell.execute_reply":"2025-07-15T16:27:07.760088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercises\n","metadata":{}},{"cell_type":"markdown","source":"### Exercise 1\n\nReshape the 5 random rows of ```Xhat``` and plot them as images.\n","metadata":{}},{"cell_type":"code","source":"# TODO\nfor i in range(5):\n    frame=randint(0, X.shape[0]-1)\n    plt.imshow(Xhat[randint(0, X.shape[0]-1),:].reshape(Length,Width),cmap=\"gray\")\n    plt.title(\"frame: \"+str(frame))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.761607Z","iopub.status.idle":"2025-07-15T16:27:07.762160Z","shell.execute_reply.started":"2025-07-15T16:27:07.761875Z","shell.execute_reply":"2025-07-15T16:27:07.761910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the exercise above that the rows of the matrix ```Xhat``` are all similar . The ```L``` determines the reconstruction properties.\n","metadata":{}},{"cell_type":"markdown","source":"### Exercise 2\n\nPlot the Cumulative explained variance against the number of singular values $L$.\n","metadata":{}},{"cell_type":"code","source":"#TODO\nplt.plot(np.cumsum(s)/s.sum())\nplt.xlabel('L')\nplt.title('Cumulative explained  singular value')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.764571Z","iopub.status.idle":"2025-07-15T16:27:07.764878Z","shell.execute_reply.started":"2025-07-15T16:27:07.764737Z","shell.execute_reply":"2025-07-15T16:27:07.764750Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 3\nPerform Truncated SVD with ```L=10``` and plot 5 random rows of Xhat.\n","metadata":{}},{"cell_type":"code","source":"#TODO\nL=10\nXhat=U[:,:L]@S[0:L,0:L]@VT[:L,:]\nfor i in range(5):\n    frame=randint(0, X.shape[0]-1)\n    plt.imshow(Xhat[randint(0, X.shape[0]-1),:].reshape(Length,Width),cmap=\"gray\")\n    plt.title(\"frame: \"+str(frame))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.766700Z","iopub.status.idle":"2025-07-15T16:27:07.767072Z","shell.execute_reply.started":"2025-07-15T16:27:07.766886Z","shell.execute_reply":"2025-07-15T16:27:07.766905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 4\n Apply   ```get_data_Matrix``` with ```mypath=\"boats\"```, plot 5 random boat images. Then perform Truncated SVD using the function ```svd``` with  L=1 and plot the first row of ```Xhat``` as an image.\n","metadata":{}},{"cell_type":"code","source":"#TODO\nX,Length,Width=get_data_Matrix (mypath=\"boats\")\n\nfor i in range(5):\n    frame=randint(0, X.shape[0]-1)\n    plt.imshow(X[randint(0, X.shape[0]-1),:].reshape(Length,Width),cmap=\"gray\")\n    plt.title(\"frame: \"+str(frame))\n    plt.show()\n    \nU, s, VT =svd(X, full_matrices=False)\nL=1\nXhat=U[:,:L]@S[0:L,0:L]@VT[:L,:]\n\nplt.imshow(Xhat[0,:].reshape(Length,Width),cmap=\"gray\")\nplt.title('Truncated SVD L=1')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.768292Z","iopub.status.idle":"2025-07-15T16:27:07.768652Z","shell.execute_reply.started":"2025-07-15T16:27:07.768458Z","shell.execute_reply":"2025-07-15T16:27:07.768476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 5\nApply   ```get_data_Matrix``` with ```mypath=\"traffic\"```, plot 5 random images. Then perform Truncated SVD  using `TruncatedSVD` with ```n_components=1``` and plot the first row of ```Xhat``` as an image.\n","metadata":{}},{"cell_type":"code","source":"#TODO\nX,Length,Width=get_data_Matrix (mypath=\"traffic\")\n\nfor i in range(5):\n    frame=randint(0, X.shape[0]-1)\n    plt.imshow(X[randint(0, X.shape[0]-1),:].reshape(Length,Width),cmap=\"gray\")\n    plt.title(\"frame: \"+str(frame))\n    plt.show()\n    \nsvd_ = TruncatedSVD(n_components=1, n_iter=7, random_state=42)\nscore=svd_.fit_transform(X)\nXhat=svd_.inverse_transform(score)\nplt.imshow(Xhat[0,:].reshape(Length,Width),cmap=\"gray\")\nplt.imshow(Xhat[0,:].reshape(Length,Width),cmap=\"gray\")\nplt.title('Truncated SVD L=1')\n\nplt.show()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.770751Z","iopub.status.idle":"2025-07-15T16:27:07.771058Z","shell.execute_reply.started":"2025-07-15T16:27:07.770933Z","shell.execute_reply":"2025-07-15T16:27:07.770946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  SVD from Scratch (optional)\n","metadata":{}},{"cell_type":"markdown","source":"There are several ways to derive SVD, let's do one that is simple to code and related to PCA. We have the SVD factorization:\n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{X}= \\mathbf {US V^{T}}$$\n","metadata":{}},{"cell_type":"code","source":"X=np.array([[1,2],[2,4],[4,8]])\nMatrix(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.772475Z","iopub.status.idle":"2025-07-15T16:27:07.772795Z","shell.execute_reply.started":"2025-07-15T16:27:07.772644Z","shell.execute_reply":"2025-07-15T16:27:07.772661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the last section you recall that $DxD$  matrix $\\mathbf{C}$   is a symmetric matrix, this is almost  the same matrix used in PCA if we can scale the covariance matrix by $N$ and obtain the eigenvalues and eigenvectors (eigenvectors have unit norm so multiplying constant will not effect them): \n","metadata":{}},{"cell_type":"markdown","source":"$$\\mathbf{C'}=\\mathbf{X}^T \\mathbf{X}=\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{T}$$\n","metadata":{}},{"cell_type":"markdown","source":"The $\\mathbf{V}$ is the same as the right singular vectors, in Python we can evaluate ```V``` as follows: \n","metadata":{}},{"cell_type":"code","source":"C=X.T@X\neigen_vectors1 , V=eig(C)\nMatrix(V)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.775004Z","iopub.status.idle":"2025-07-15T16:27:07.775412Z","shell.execute_reply.started":"2025-07-15T16:27:07.775276Z","shell.execute_reply":"2025-07-15T16:27:07.775294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also perform the same decomposition with the Gram matrix:\n","metadata":{}},{"cell_type":"markdown","source":"$G=\\mathbf{X} \\mathbf{X}^T= \\mathbf{U \\Lambda U^{T}}$\n","metadata":{}},{"cell_type":"markdown","source":"Where $\\mathbf{U}$ are the the right singular vectors \n","metadata":{}},{"cell_type":"code","source":"G=X@X.T\neigen_vectors2 , U=eig(G)\nMatrix(U)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.776660Z","iopub.status.idle":"2025-07-15T16:27:07.776988Z","shell.execute_reply.started":"2025-07-15T16:27:07.776851Z","shell.execute_reply":"2025-07-15T16:27:07.776867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are several way to obtains the singular values as we know  $\\mathbf{X}= \\mathbf {USV^{T}}$. With some algebra we know that $\\mathbf{S}=\\mathbf{U^TXV}$\n","metadata":{}},{"cell_type":"code","source":"S=np.round((U.T@X@V))\nMatrix(S)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.778124Z","iopub.status.idle":"2025-07-15T16:27:07.780959Z","shell.execute_reply.started":"2025-07-15T16:27:07.780790Z","shell.execute_reply":"2025-07-15T16:27:07.780810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Singular values are not in descending order, and the shape difference is because this is not the economy-size decomposition.\nNow that we have all the components, we can reconstruct the matrix:\n","metadata":{}},{"cell_type":"code","source":"X_=np.round(U@S@V.T)\nMatrix(X_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.782164Z","iopub.status.idle":"2025-07-15T16:27:07.782511Z","shell.execute_reply.started":"2025-07-15T16:27:07.782381Z","shell.execute_reply":"2025-07-15T16:27:07.782395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Relationship between SVD and PCA (optional)\n","metadata":{}},{"cell_type":"markdown","source":"In the section above we derived SVD. Let's use this to better understand how SVD and PCA are almost the same. Let's use the following dateset:\n","metadata":{}},{"cell_type":"code","source":"N=200\nu=np.array([[1.0,1.0],[0.10,-0.10]])/(2)**(0.5)\nX_=np.dot(4*np.random.randn(N,2),u)+10\nX=X_-X_.mean(axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.783437Z","iopub.status.idle":"2025-07-15T16:27:07.783717Z","shell.execute_reply.started":"2025-07-15T16:27:07.783592Z","shell.execute_reply":"2025-07-15T16:27:07.783604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's compare SVD to sklearn's PCA  setting ```n_components=1```\n","metadata":{}},{"cell_type":"code","source":"U, s, VT =svd(X, full_matrices=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.785480Z","iopub.status.idle":"2025-07-15T16:27:07.785851Z","shell.execute_reply.started":"2025-07-15T16:27:07.785659Z","shell.execute_reply":"2025-07-15T16:27:07.785672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pca = PCA(n_components=1)\nprojection=pca.fit_transform(X)\nX_sklearn=pca.inverse_transform(projection)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.786855Z","iopub.status.idle":"2025-07-15T16:27:07.787393Z","shell.execute_reply.started":"2025-07-15T16:27:07.787107Z","shell.execute_reply":"2025-07-15T16:27:07.787124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As $\\mathbf{V}$ are the principal components, the projections on the principal components can be found by multiplying $\\mathbf{X}$ and $\\mathbf{V}$, if we want the projection on the first principal components, we can take the row of $\\mathbf{V}$ with the largest Singular-Value Value and compare it to scikit-learn's. We see the result is identical:\n","metadata":{}},{"cell_type":"code","source":"projection_=X@VT[0,:]\nprint (\"error SVD vs scikit-learn's PCA\",(projection_-projection).sum()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.788578Z","iopub.status.idle":"2025-07-15T16:27:07.789223Z","shell.execute_reply.started":"2025-07-15T16:27:07.788955Z","shell.execute_reply":"2025-07-15T16:27:07.788977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also find the projections using  $\\mathbf{XV}= \\mathbf {US}$. Here we find the projection on the first principal component to compare to scikit-learn's\n","metadata":{}},{"cell_type":"code","source":"projection_=U@np.diag(s)[:,0]\nprint (\"error SVD vs scikit-learn's PCA\",(projection_-projection).sum()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.790395Z","iopub.status.idle":"2025-07-15T16:27:07.790663Z","shell.execute_reply.started":"2025-07-15T16:27:07.790542Z","shell.execute_reply":"2025-07-15T16:27:07.790554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The inverse transform on the first  principal component is simply the low rank approximation:\n","metadata":{}},{"cell_type":"code","source":"L=1\nXhat=U[:,:L]@np.diag(s[0:L])@VT[:L,:] # resconstruct X\nprint (\"error SVD vs scikit-learn's PCA\",((Xhat-X_sklearn)**2).sum()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.791865Z","iopub.status.idle":"2025-07-15T16:27:07.792142Z","shell.execute_reply.started":"2025-07-15T16:27:07.792015Z","shell.execute_reply":"2025-07-15T16:27:07.792028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The relationship between the explained variance and singular  values are given by  $\\lambda_i = s_i^2/(N-1)$ we can verify the for the first singular value:\n","metadata":{}},{"cell_type":"code","source":"s[0]**2/(200-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.793370Z","iopub.status.idle":"2025-07-15T16:27:07.793827Z","shell.execute_reply.started":"2025-07-15T16:27:07.793647Z","shell.execute_reply":"2025-07-15T16:27:07.793662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pca.explained_variance_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:07.795226Z","iopub.status.idle":"2025-07-15T16:27:07.795513Z","shell.execute_reply.started":"2025-07-15T16:27:07.795385Z","shell.execute_reply":"2025-07-15T16:27:07.795399Z"}},"outputs":[],"execution_count":null}]}