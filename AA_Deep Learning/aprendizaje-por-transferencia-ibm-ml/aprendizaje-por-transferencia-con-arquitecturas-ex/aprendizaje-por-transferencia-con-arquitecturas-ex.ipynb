{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"eb16dd8ca4216333f71b5267e56ce2b100b8476d7a5bf8c58fff12fed5ff54bc","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **End-to-End Example: Transfer Learning**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **45** minutes\n","metadata":{}},{"cell_type":"markdown","source":"In this lab, we will walk through an end-to-end example of transfer learning and fine-tuning. \n","metadata":{}},{"cell_type":"markdown","source":"<h1> Is the waste product organic or a recyclable? </h1></s>\n","metadata":{}},{"cell_type":"markdown","source":"You are a data science intern at a waste management service. Your manager has asked you to create a waste classification pipeline that categorizes waste streams based on disposal options: organic or recyclable. \n\nIn this lab we're going to train a transfer learning model to perform this image classification task.\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/transfer_learning.gif\" width=\"600\" height=\"600\">\n","metadata":{}},{"cell_type":"markdown","source":"## **Table of Contents**\n\n<ol>\n    <li><a href=\"#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n    </li>\n    <li>\n        <a href=\"#Background\">Background</a>\n        <ol>\n            <li><a href=\"#What is transfer learning?\">What is transfer learning?</a></li>\n        </ol>\n    </li>\n        <li>\n        <a href=\"#Example 1: Create a ML model for distinguishing recyclable and organic waste images\">Example 1: Create a ML model for distinguishing recyclable and organic waste images</a>\n            <ol>\n            <li><a href=\"#Dataset\">Dataset</a></li>\n            <li><a href=\"#Importing data\">Importing data</a></li>\n            <li><a href=\"#Data visualization\">Data visualization</a></li>\n            <li><a href=\"#Define configuration options\">Define configuration options</a></li>\n            <li><a href=\"#Loading Images using ImageGeneratorClass\">Loading Images using ImageGeneratorClass</a></li>\n            <li><a href=\"#Pre-trained models\">Pre-trained models</a></li>\n            <li><a href=\"#Compile the model\">Compile the model</a></li>\n            <li><a href=\"#Fine-Tuning\">Fine-Tuning</a></li>\n            <li><a href=\"#Evaluate both models on test data\">Evaluate both models on test data</a></li>\n        </ol>\n        </li>\n    <li>\n        <a href=\"#Example 2: Use Transfer Learning for identifying Stop Signs\">Example 2: Use Transfer Learning for identifying Stop Signs</a>\n            <ol>\n            <li><a href=\"#Loading images\">Loading images</a></li>\n            <li><a href=\"#Defining a helper function\">Defining a helper function</a></li>\n            <li><a href=\"#Pre-trained Model 1: Incepton-v3\">Pre-trained Model 1: Incepton-v3</a></li>\n            <li><a href=\"#Pre-trained Model 2: MobileNet\">Pre-trained Model 2: MobileNet</a></li>\n            <li><a href=\"#Pre-trained Model 3: ResNet-50\">Pre-trained Model 3: ResNet-50</a></li>\n         </ol>\n        </li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab you will be able to:\n\n- __Perform__ pre-processing and image augmentation on ImageGeneratorClass objects in Keras. \n- __Implement__ transfer learning in five general steps: \n    - obtain pre-trained model, \n    - create base model, \n    - freeze layers, \n    - train new layers on dataset, \n    - improve model through fine tuning.\n- __Build__ an end-to-end VGG16-based transfer learning model for binary image classification tasks.\n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n*   [`keras`](https://keras.io/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for loading datasets.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip install tensorflow_datasets`, `!pip install --upgrade tensorflow` and `!pip install opendatasets` in the following code cell.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade tensorflow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport datetime\nimport os\nimport random, shutil\nimport glob\nimport skillsnetwork\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.image import imread\n\nfrom os import makedirs,listdir\nfrom shutil import copyfile\nfrom random import seed\nfrom random import random\nimport keras \nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Input\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import InceptionV3\nfrom sklearn import metrics\n\n\nsns.set_context('notebook')\nsns.set_style('white')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Background\n","metadata":{}},{"cell_type":"markdown","source":"### What is transfer learning?\n","metadata":{}},{"cell_type":"markdown","source":"Most popular models are difficult to train from scratch as they require huge datasets (like ImageNet), a large number of training iterations, and very heavy computing machinery. The basic features (edges, shapes) learned by early layers in a network are generalizable. While the later layers in an already trained network tend to capture features that are more particular to a specific image classification task. \n\n__Transfer learning__ uses the idea that if we keep the early layers of a pre-trained network, and re-train the later layers on a specific dataset, we might be able to leverage some state of that network on a related task.\n","metadata":{}},{"cell_type":"markdown","source":"A typical transfer learning workflow in Keras looks something like this:\n    \n1. Initialize base model, and load pre-trained weights (like ImageNet).\n2. \"Freeze\" layers in the base model by setting `training = False`.\n3. Define a new model that goes on top of the output of the base model's layers.\n4. Train resulting model on your data set.\n","metadata":{}},{"cell_type":"markdown","source":"## Example 1: Create a ML model for distinguishing recyclable and organic waste images\n","metadata":{}},{"cell_type":"markdown","source":"### Dataset\n","metadata":{}},{"cell_type":"markdown","source":"The dataset used in this Lab was found on Kaggle, which is an awesome community for people who enjoy anything related to data science and machine learning. We will be using the Waste Classification Dataset [dataset](https://www.kaggle.com/datasets/techsash/waste-classification-data):\n","metadata":{}},{"cell_type":"markdown","source":"> PROBLEM: \n> - Waste management is a big problem in our country. Most of the wastes end up in landfills.\n\n> APPROACH: \n> - Segregated into two classes (Organic and recyclable)\n> - Automated the process by using IOT and machine learning\n\n> IMPLEMENTATION:\n> - Dataset is divided into train data (85%) and test data (15%)\n> - Training data - 22,564 images Test data - 2,513 images\n","metadata":{}},{"cell_type":"markdown","source":"* The dataset contains ~25,000 images of recyclable and organic products split into a train and test set.\n* Our goal is to train an algorithm on these files and to predict the labels for images in our test set (1 = recyclable, 0 = organic).\n","metadata":{}},{"cell_type":"markdown","source":"Now that we have obtained the necessary context and some insight in today's dataset, we're moving on to the practical part: using transfer learning to classify images. In this task, we will be using Tensorflow, which is a highly used ML library for training neural networks, and Keras, which is an API that makes this process simpler.\n","metadata":{}},{"cell_type":"markdown","source":"We will go through the following steps to create our classification model:\n","metadata":{}},{"cell_type":"markdown","source":"1. Import data directly from Kaggle.\n2. Visualize a few random images from the train set.\n3. Load images in using the ImageGeneratorClass from Keras.\n4. Define model configuration options.\n5. Perform some image augmentation for improved model generalizability.\n6. Load, compile and train a pre-trained model like VGG-16.\n7. Perform inference on the test set. \n","metadata":{}},{"cell_type":"markdown","source":"### Importing Data\n","metadata":{}},{"cell_type":"markdown","source":"This will create a `o-vs-r-split` directory in your environment.\n","metadata":{}},{"cell_type":"code","source":"import skillsnetwork\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/o-vs-r-split.tar.gz\",\n                           overwrite=True)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define configuration options\n","metadata":{}},{"cell_type":"markdown","source":"It's time to define some model configuration options.\n","metadata":{}},{"cell_type":"markdown","source":"* __img_rows__ and __img_cols__ are used to specify width and height of the images expected by MobileNet.\n* __batch size__ is set to 32.\n* The __number of epochs__ (that is, iterations) is set to 5, but as we are using Early Stopping, the number of iterations might be lower if early stopping conditions are met before 5 iterations.\n* The __number of classes__ is 2.\n* We will use 20% of the data for __validation__ purposes.\n* We want to be able to see the model output, so we set __verbosity__ to 1, or True.\n* The __path__ is a path to the directory with the training data and __path_test__ should contain the test data.\n* The __input_shape__ is common for an image: (w, h, d).\n* We have two __labels__ in our dataset: organic (O), recyclable (R).\n* The __checkpoint_path__ is where ModelCheckpoint will save our model.\n","metadata":{}},{"cell_type":"code","source":"img_rows, img_cols = 150, 150\nbatch_size = 32\nn_epochs = 10\nn_classes = 2\nval_split = 0.2\nverbosity = 1\npath = 'o-vs-r-split/train/'\npath_test = 'o-vs-r-split/test/'\ninput_shape = (img_rows, img_cols, 3) #RGB\nlabels = ['O', 'R']\nseed = 10\ncheckpoint_path='ORnet.h5'","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading Images using ImageGeneratorClass\n","metadata":{}},{"cell_type":"markdown","source":"Transfer learning works best when models are trained on smaller datasets. We are using 10,000 images from the original dataset of 25,000 images. We are setting aside 10% of the set for testing purposes.\n","metadata":{}},{"cell_type":"markdown","source":"The folder structure looks as follows:\n","metadata":{}},{"cell_type":"markdown","source":"```python\no-vs-r-split/\n└── train\n    ├── O\n    └── R\n└── test\n    ├── O\n    └── R\n```\n","metadata":{}},{"cell_type":"markdown","source":"#### Image Augmentation\n","metadata":{}},{"cell_type":"markdown","source":"Now we will create ImageDataGenerators used for training, validation, and testing.\n\nImage data generators create batches of tensor image data with real-time data augmentation. They loop over the data in batches and are useful in feeding data to the training process. We specify a 20% validation split.\n\nWe will perform a few image augmentation steps. We set __rescale__ to 1./255. This means that each image pixel will be divided by 255 in order to normalize the image. We set the __width_shift_range__ and __height_shift_range__ to 0.1 each, and the __horizontal_flip__ to True. \n* The __width_shift_range__ shifts the image horizontally (left or right). \n* The __height_shift_range__ shifts the image vertically (up or down). \n* The __horizontal_flip__ randomly flip inputs horizontally.\n","metadata":{}},{"cell_type":"code","source":"# Create ImageDataGenerators for training and validation and testing\ntrain_datagen = ImageDataGenerator(\n    validation_split = val_split,\n    rescale=1.0/255.0,\n\twidth_shift_range=0.1, \n    height_shift_range=0.1, \n    horizontal_flip=True\n)\n\nval_datagen = ImageDataGenerator(\n    validation_split = val_split,\n    rescale=1.0/255.0,\n\twidth_shift_range=0.1, \n    height_shift_range=0.1, \n    horizontal_flip=True\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale=1.0/255.0\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since it is undesirable to load all of our image data into memory all at once, we make use of the __flow_from_directory__ method. It takes the ImageDataGenerator training/validation sets and flows image batches from a specified folder. \n\n* We set __directory__ to the __path__, specifying the path to our training dataset. \n* We specify classes using __labels__ we had defined in the configuration step.\n* We apply a __seed__ so our random initializer is generated in the same way each time we perform this step. This allows for direct comparison between experiments.\n","metadata":{}},{"cell_type":"code","source":"# use the labels defined before to \n# find number of images belonging to each category\ntrain_generator = train_datagen.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = seed,\n    batch_size = batch_size, \n    class_mode='binary',\n    shuffle = True,\n    target_size=(img_rows, img_rows),\n    subset = 'training'\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_generator = val_datagen.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = seed,\n    batch_size = batch_size, \n    class_mode='binary',\n    shuffle = True,\n    target_size=(img_rows, img_rows),\n    subset = 'validation'\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_generator = test_datagen.flow_from_directory(\n    directory = path_test,\n    classes = labels,\n    class_mode='binary',\n    seed = seed,\n    batch_size = batch_size, \n    shuffle = True,\n    target_size=(img_rows, img_rows)\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There were 5810 images belonging to 2 classes found in the train directory, and 1452 images belonging to the same two classes were found in the validation directory, and 828 images were found in the test set.\n","metadata":{}},{"cell_type":"markdown","source":"Let's look at a few augmented images:\n","metadata":{}},{"cell_type":"code","source":"IMG_DIM = (100, 100)\n\ntrain_files = glob.glob('./o-vs-r-split/train/O/*')\ntrain_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in train_files]\ntrain_imgs = np.array(train_imgs)\ntrain_labels = [fn.split('/')[3].split('.')[0].strip() for fn in train_files]\n\nimg_id = 0\nO_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n                                   batch_size=1)\nO = [next(O_generator) for i in range(0,5)]\nfig, ax = plt.subplots(1,5, figsize=(16, 6))\nprint('Labels:', [item[1][0] for item in O])\nl = [ax[i].imshow(O[i][0][0]) for i in range(0,5)]\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-trained Models\n","metadata":{}},{"cell_type":"markdown","source":"Pre-trained models are saved networks that have previously been trained on some large (somewhat related) datasets. They are typically used for a large-scale image-classification task. They can be used as they are or could be customized to a given task using transfer learning. These pre-trained models form the basis of transfer learning.\n","metadata":{}},{"cell_type":"markdown","source":"We can leverage a pre-trained model's weighted layers to extract generic features. In this step we would not update the model's layers weights during training. This helps us utilize the knowledge from a source-domain task.\n","metadata":{}},{"cell_type":"markdown","source":"Fine-tuning, which is a more involved process, involves more than just replacing the final layer. We also try to retrain some of the previous layers.\n","metadata":{}},{"cell_type":"markdown","source":"In computer vision, some popular pre-trained models include: VGG-16, VGG-19, InceptionV3, XCeption, and ResNet-50.\n","metadata":{}},{"cell_type":"markdown","source":"#### VGG-16\n","metadata":{}},{"cell_type":"markdown","source":"Let us load the VGG16 model.\n\n```python\ntf.keras.applications.VGG16(\n    include_top=True,\n    weights=\"imagenet\",\n    input_shape=None,\n)\n```\nThe default input image size for this model is 224x224.\n\nFurther information on arguments can be found in the Keras [documentation](https://keras.io/api/applications/vgg/).\n\n>Note: Each Keras Application expects a specific kind of input preprocessing. For VGG16, call `tf.keras.applications.vgg16.preprocess_input` on your inputs before passing them to the `model.vgg16.preprocess_input` will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n\n>* __include_top__: whether to include the fully-connected layer at the top of the network.\n>* __weights__: one of None (random initialization), 'imagenet' (pre-trained on ImageNet), or the path to the weights file to be loaded.\n>* __input_shape__: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3). \n","metadata":{}},{"cell_type":"markdown","source":"First we load the model \n","metadata":{}},{"cell_type":"code","source":"from keras.applications import vgg16\ninput_shape = (150, 150, 3)\n\nvgg = vgg16.VGG16(include_top=False,\n                        weights='imagenet',\n                        input_shape=input_shape)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We flatten the output of a vgg model and assign it to the model ```output```, we then use a Model object ```basemodel``` to group the layers into an object for training and inference .\nWith the following inputs and outputs  \n\ninputs: ```vgg.input```\n\noutputs: ```tf.keras.layers.Flatten()(output)```\n","metadata":{}},{"cell_type":"code","source":"output = vgg.layers[-1].output\noutput = tf.keras.layers.Flatten()(output)\nbasemodel = Model(vgg.input, output)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we freeze the basemodel, like the lower layers.\n","metadata":{}},{"cell_type":"code","source":"basemodel.trainable = False\nfor layer in basemodel.layers: layer.trainable = False","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a new model on top. We add a Dropout layer for regularization, only these layers will change as for the lower layers we set `training=False` when calling the base model.\n","metadata":{}},{"cell_type":"code","source":"input_shape = basemodel.output_shape[1]\n\nmodel = Sequential()\nmodel.add(basemodel)\nmodel.add(Dense(512, activation='relu', input_dim=input_shape))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us print the model summary.\n","metadata":{}},{"cell_type":"code","source":"print(model.summary())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compile the model\n","metadata":{}},{"cell_type":"markdown","source":"Calling `compile()` freezes the behavior of that model, implying that the trainable attribute values at the time of compilation are preserved.\n","metadata":{}},{"cell_type":"markdown","source":"We use RMSProp as our optimizer with a learning rate of 2e-5, Binary Cross Entropy Loss as our loss function and Accuracy as our primary metric for model evaluation as we have 2 labels in our dataset.\n","metadata":{}},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=2e-5),\n              metrics=['accuracy'])\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use early stopping to avoid over-training the model. An over-trained model overfits the training dataset and has poor performance on unseen test sets. We will also use a exponential step-decay based learning rate scheduler.\n","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler\ncheckpoint_path='O_R_tlearn_image_augm_cnn_vgg16.h5'\n\n# define step decay function\nclass LossHistory_(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.lr.append(exp_decay(len(self.losses)))\n        print('lr:', exp_decay(len(self.losses)))\n\ndef exp_decay(epoch):\n    initial_lrate = 1e-5\n    k = 0.1\n    lrate = initial_lrate * np.exp(-k*epoch)\n    return lrate\n\n# learning schedule callback\nloss_history_ = LossHistory_()\nlrate_ = LearningRateScheduler(exp_decay)\n\nkeras_callbacks = [\n      EarlyStopping(monitor = 'loss', \n                    patience = 5, \n                    mode = 'min', \n                    min_delta=0.01),\n      ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, mode='min')\n]\n\ncallbacks_list_ = [loss_history_, lrate_, keras_callbacks]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fit and train the model\n","metadata":{}},{"cell_type":"code","source":"extract_feat_model = model.fit(train_generator, \n                              steps_per_epoch=10, \n                              epochs=5,\n                              validation_data=val_generator, \n                              validation_steps=10, \n                              verbose=1,\n                              callbacks = callbacks_list_)  ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fine-Tuning\n","metadata":{}},{"cell_type":"markdown","source":"Fine-tuning is an optional step in transfer learning, it usually ends up improving the performance of the model. It is easy to overfit the model in this step as we are re-training the entire model. So we use regularization (dropout layers), a lower learning rate, a small number of epochs (training iterations), and early stopping to know when the model has stopped improving and to prevent overfitting. \n","metadata":{}},{"cell_type":"markdown","source":"In order to fine-tune our model, we will use the VGG-16 model object stored in the `basemodel` variable. We will unfreeze convolution blocks 4 and 5 and keep the first 3 blocks frozen. This ensures that the convolution and pooling layers for blocks 4 and 5 are trainable; that is, weights get updated through backpropagation in each training iteration or epoch.\n","metadata":{}},{"cell_type":"markdown","source":"We can find the name of each layer\n","metadata":{}},{"cell_type":"code","source":"[layer.name for layer in basemodel.layers]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we then set ```'block5_conv1'```and ```'block4_conv1' ``` to trainable\n","metadata":{}},{"cell_type":"code","source":"basemodel.trainable = True\n\nset_trainable = False\n\nfor layer in basemodel.layers:\n    if layer.name in ['block5_conv1', 'block4_conv1']:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Similar to what we did before, we create a new model on top, add a Dropout layer for regularization, and set `training=False` when calling the base model.\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(basemodel)\nmodel.add(Dense(512, activation='relu', input_dim=input_shape))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\ncheckpoint_path='O_R_tlearn_image_augm_fine_tune_vgg16.h5'\n\n\n# learning schedule callback\nloss_history_ = LossHistory_()\nlrate_ = LearningRateScheduler(exp_decay)\n\nkeras_callbacks = [\n      EarlyStopping(monitor = 'loss', \n                    patience = 5, \n                    mode = 'min', \n                    min_delta=0.01),\n      ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, mode='min')\n]\n\ncallbacks_list_ = [loss_history_, lrate_, keras_callbacks]\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n              metrics=['accuracy'])\n              \nfine_tune_model = model.fit(train_generator, \n                    steps_per_epoch=10, \n                    epochs=5,\n                    callbacks = callbacks_list_,   \n                    validation_data=val_generator, \n                    validation_steps=10, \n                    verbose=1)       ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The fine tuned model has a validation accuracy higher than the previous transfer learning model.\n","metadata":{}},{"cell_type":"markdown","source":"### Evaluate both models on test data\n","metadata":{}},{"cell_type":"markdown","source":"Load saved models:\n","metadata":{}},{"cell_type":"code","source":"extract_feat_model = tf.keras.models.load_model('O_R_tlearn_image_augm_cnn_vgg16.h5')\nfine_tune_model = tf.keras.models.load_model('O_R_tlearn_image_augm_fine_tune_vgg16.h5')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load test images:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\n\nIMG_DIM = (150, 150)\n\n# Read in all O and R test images file paths. Shuffle and select 50 random test images. \ntest_files_O = glob.glob('./o-vs-r-split/test/O/*')\ntest_files_R = glob.glob('./o-vs-r-split/test/R/*')\ntest_files = test_files_O + test_files_R\ntest_files = shuffle(test_files)[0:50]\n\n# Extract label from file path\ntest_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in test_files]\ntest_imgs = np.array(test_imgs)\ntest_labels = [fn.split('/')[3].split('.')[0].strip() for fn in test_files]\n\n# Standardize\ntest_imgs_scaled = test_imgs.astype('float32')\ntest_imgs_scaled /= 255","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class2num_lt = lambda l: [0 if x == 'O' else 1 for x in l]\nnum2class_lt = lambda l: ['O' if x < 0.5 else 'R' for x in l]\n\ntest_labels_enc = class2num_lt(test_labels)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_extract_feat_model = extract_feat_model.predict(test_imgs_scaled, verbose=0)\npredictions_fine_tune_model = fine_tune_model.predict(test_imgs_scaled, verbose=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_extract_feat_model = num2class_lt(predictions_extract_feat_model)\npredictions_fine_tune_model = num2class_lt(predictions_fine_tune_model)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Extract Features Model')\nprint(metrics.classification_report(test_labels, predictions_extract_feat_model))\nprint('Fine-Tuned Model')\nprint(metrics.classification_report(test_labels, predictions_fine_tune_model))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Custom image: is your waste organic or a recyclable?\n","metadata":{}},{"cell_type":"code","source":"custom_im = test_imgs_scaled[2]\nplt.imshow(custom_im)\n\nnum2class_lt(extract_feat_model.predict(custom_im.reshape((1,\n                                                           test_imgs_scaled.shape[1], \n                                                           test_imgs_scaled.shape[2], \n                                                           test_imgs_scaled.shape[3])), verbose=0))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_im = test_imgs_scaled[3]\nplt.imshow(custom_im)\n\nnum2class_lt(extract_feat_model.predict(custom_im.reshape((1,\n                                                           test_imgs_scaled.shape[1], \n                                                           test_imgs_scaled.shape[2], \n                                                           test_imgs_scaled.shape[3])), verbose=0))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we are all set to see if our waste product is an organic or a recyclable.\n","metadata":{}},{"cell_type":"markdown","source":"## Example 2: Use Transfer Learning for identifying Stop Signs\n\nOther than VGG-16, there are many state-of-the-art CNN architectures we could explore, such as **InceptionNet, MobileNet, ResNet, Xception**, and so on. There are more than two dozen pre-trained models available from Keras that we could use directly for transfer learning. In this example, we will implement some of them for distinguishing stop signs. \n\nThe stop sign datasets will be downloaded using the following cells. Compared to a typical dataset of images, the stop sign dataset is relatively small as it only contains around 200 training images and 8 test images. This is when transfer learning should come to rescue, because training models from scratch would require a lot of data, otherwise you will likely overfit your model. \n","metadata":{}},{"cell_type":"markdown","source":"### Loading images\n","metadata":{}},{"cell_type":"markdown","source":"Uncomment the following cell to download the data files and unzip them.\n","metadata":{}},{"cell_type":"code","source":"await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/stop.zip\",\n                           overwrite=True)\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/not_stop.zip\",\n                           overwrite=True)\nawait skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/test_set_stop_not_stop.zip\",\n                           overwrite=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Creating image directories\n\nWe will rearange the image directories as follows:\n\n```python\nsigns/\n└── train\n    ├── stop\n    └── not_stop\n└── test\n    ├── stop\n    └── not_stop\n```\n\nWe will have a train and a test directory, and each contains images that have stop signs and those that don't. This setup of directories will be handy when we use the `flow_from_directory` function later to build data generators.\n","metadata":{}},{"cell_type":"code","source":"dataset_home = 'signs/'\nsubdirs = ['train/', 'test/']\nfor subdir in subdirs:\n    labeldirs = ['stop/', 'not_stop/']\n    for labeldir in labeldirs:\n        newdir = dataset_home + subdir + labeldir\n        makedirs(newdir, exist_ok = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Moving images\n\nLet's move the images according to the map above.\n","metadata":{}},{"cell_type":"code","source":"for file in listdir(\"stop\"):\n    if file != '.DS_Store':\n        shutil.copyfile(f\"stop/{file}\", f\"signs/train/stop/{file}\")\n        \nfor file in listdir(\"not_stop\"):\n    if file != '.DS_Store':\n        shutil.copyfile(f\"not_stop/{file}\", f\"signs/train/not_stop/{file}\")\n\ntest_path = \"test_set_stop_not_stop/\"\nfor file in listdir(test_path):\n    if file.startswith(\"stop\"):\n        shutil.copyfile(test_path+file, f\"signs/test/stop/{file}\")\n    elif file.startswith(\"not_stop\"):\n        shutil.copyfile(test_path+file, f\"signs/test/not_stop/{file}\")      \n       ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Displaying raw images\n\nUse the following code to display the first five images in the train set that have stop signs and the first five images that don't have stop signs.\n","metadata":{}},{"cell_type":"code","source":"train_stop = glob.glob('./signs/train/stop/*')\ntrain_not_stop = glob.glob('./signs/train/not_stop/*')\n\nfig1, ax1 = plt.subplots(1,5,figsize=(15,4))\nfig1.suptitle(\"STOP Signs\", fontsize=18)\nl1 = [ax1[i].imshow(imread(train_stop[i])) for i in range(5)]\n\nfig2, ax2 = plt.subplots(1,5,figsize=(15,4))\nfig2.suptitle(\"NO STOP Signs\", fontsize=18)\nl2 = [ax2[i].imshow(imread(train_not_stop[i])) for i in range(5)]\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Building image data generators\n","metadata":{}},{"cell_type":"markdown","source":"Same as what we did for the waste data, we will build image data generators that perform real-time image augmentation while returning batches of image data. We will apply image augmentation on the training data only. We use a validation split of 0.2 and a batch size of 30.\n","metadata":{}},{"cell_type":"code","source":"path = \"signs/train/\"\nlabels = ['stop', 'not_stop']\nseed = 123\nbatch_size = 30\ntarget_size = (112,112)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(validation_split=0.2,\n                                  rescale=1./255.,\n                                  rotation_range=40,\n                                  width_shift_range=0.2,\n                                  height_shift_range=0.2,\n                                  shear_range=0.2,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True)\n\nval_datagen = ImageDataGenerator(validation_split=0.2,\n                                  rescale=1./255.)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = seed,\n    batch_size = batch_size, \n    class_mode='binary',\n    shuffle = True,\n    target_size=target_size,\n    subset = 'training'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = seed,\n    batch_size = batch_size, \n    class_mode='binary',\n    shuffle = True,\n    target_size=target_size,\n    subset = 'validation'\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_generator.class_indices)\n\nprob2class = lambda x: 'Stop' if x < 0.5 else 'Not Stop' ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For our test images, we convert them into numpy arrays with pixel values scaled to 0-1. The test data will not be seen by the model during training or validation, but they will be used to evaluate the predictive power of our model at the end.\n","metadata":{}},{"cell_type":"code","source":"test_files = glob.glob('signs/test/stop/*.jpeg') + glob.glob('signs/test/not_stop/*.jpeg')\ntest_files = shuffle(test_files)\n\ntest_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=target_size)) for img in test_files]\ntest_imgs = np.array(test_imgs).astype('int')\n\n# Standardize\ntest_imgs_scaled = test_imgs.astype('float32')\ntest_imgs_scaled /= 255","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining a helper function for building, compiling, and fitting CNNs\n","metadata":{}},{"cell_type":"code","source":"def build_compile_fit(basemodel):\n    \n    # flatten the output of the base model\n    x = Flatten()(basemodel.output)\n    # add a fully connected layer \n    x = Dense(1024, activation='relu')(x)\n    # add dropout layer for regularization\n    x = Dropout(0.2)(x)\n    # add final layer for classification\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(basemodel.input, x)\n    model.compile(optimizer = optimizers.RMSprop(learning_rate=0.0001),\n                                                       loss='binary_crossentropy',\n                                                       metrics=['accuracy'])\n    callbacks = [EarlyStopping(monitor = 'loss', \n                    patience = 5, \n                    mode = 'min', \n                    min_delta=0.01)]\n\n    model.fit(train_generator,\n              validation_data = val_generator,\n              steps_per_epoch=5, # num of batches in one epoch\n              epochs=10,\n              callbacks=callbacks)\n    \n    return model","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-trained Model 1: Inception-v3\n","metadata":{}},{"cell_type":"markdown","source":"**Inception-v3** is a successor to Inception-v1 with 24 million parameters and ran 48 layers deep. \n\nInstead of focusing on increasing the depth of the network, InceptionNet focuses on increasing the width and depth of the model simultaneously to attain better accuracy, while keeping the computing resources constant. \n\nIt focuses on **parallel processing** and extraction of various feature maps concurrently using **Inception modules**, which are collections of convolutions with different filter sizes and pooling operations. The following is an illustration of the inception module in inception-v1 architecture:\n\n<center>\n<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/Inception_v1_module.png\"></center>\n\nPicture credits to [Wichai Puarungroj](https://www.researchgate.net/profile/Wichai-Puarungroj-2)\n\nThe inception architecture was refined in various ways, specifically for inception-v3, the following improvements are incorporated to achieve less expensive and still efficient networks:\n\n- Factorization Into Smaller Convolutions\n- Factorization Into Asymmetric Convolutions\n- Auxiliary Classifier used as regularizer\n- Efficient Grid Size Reduction\n\nYou can read more about the different versions of InceptionNet [here](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's import the pre-trained Inception-v3 architecture from keras applications for our transfer learning task:\n","metadata":{}},{"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\n\n# initialize the base model\nbasemodel = InceptionV3(input_shape=(112,112,3),\n                          include_top = False,\n                          weights = 'imagenet')\n\nfor layer in basemodel.layers:\n    layer.trainable = False\n\n# call the build_compile_fit function to complete model training\ninception_v3 = build_compile_fit(basemodel)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The pre-trained part of our **inception_v3** model utilizes the weights obtained from the imagenet dataset training. Only the layers that we added are trained on the stop sign's data.\n\nLet's now display the test images along with their class labels predicted by the fitted **inception_v3**:\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n\nfor i, ax in enumerate(ax.flat):\n    ax.imshow(test_imgs[i])\n    pred_class = prob2class(inception_v3.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n    \n    # print the predicted class label as the title of the image\n    ax.set_title(pred_class, fontsize=15)\n    ax.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-trained Model 2: MobileNet\n","metadata":{}},{"cell_type":"markdown","source":"**MobileNets** are very efficient and small deep learning architectures specially designed for mobile devices.\n\nIt uses of a new kind of convolution layer, known as **Depthwise Separable convolution**. The main difference between a 2D convolution and a Depthwise convolution is that the former is performed over multiple input channels by doing a weighted sum of the input pixels with the filter, whereas the latter is performed separately over each channel. \n\nFor example, if the input image has three channels, then the output of depthwise separable convolution will also have three channels. The next step is **Pointwise convolution**, which is similar to a regular convolution with a $1\\times1$ filter. By doing so, we can again merge the three channels from depthwise separable convolution into one to create new features.\n","metadata":{}},{"cell_type":"markdown","source":"Let's import the pre-trained MobileNet architecture from keras applications for our transfer learning task:\n","metadata":{}},{"cell_type":"code","source":"from keras.applications.mobilenet import MobileNet\n\n# initialize the base model\nbasemodel = MobileNet(input_shape=(112,112,3),\n                          include_top = False,\n                          weights = 'imagenet')\n\nfor layer in basemodel.layers:\n    layer.trainable = False\n    \n# call the build_compile_fit function to complete model training\nmobile_net = build_compile_fit(basemodel)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's display the test images along with the their class labels predicted by the fitted **mobile_net**:\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n\nfor i, ax in enumerate(ax.flat):\n    ax.imshow(test_imgs[i])\n    pred_class = prob2class(mobile_net.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n    \n    # print the predicted class label as the title of the image\n    ax.set_title(pred_class, fontsize=15)\n    ax.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-trained Model 3: ResNet-50\n","metadata":{}},{"cell_type":"markdown","source":"**ResNet** features special skip connections which add the output from an earlier layer directly to a later layer and heavy use of batch normalization. It allows us to design deep CNNs without compromising the model’s convergence and accuracy. The basic building blocks for ResNets are the convolution and identity blocks. \n\nEssentially, ResNet uses the network layers to fit a residual mapping $F(x) + x$, instead of trying to learn the desired underlying mapping $H(x)$ directly with stacked layers. \n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/resnet.png\" width=\"30%\"></center>\n\nThe formulation of $F(x) + x$ can be realized by the feedforward neural networks with **shortcut connections**. Shortcut connections add the output from the previous layer directly to the output of the current layer, which can be seen as **identity mapping**.\n\nBy doing so, the network overcomes the vanishing gradient problem because now the gradient signals could travel back to early layers through this shortcut.\n","metadata":{}},{"cell_type":"markdown","source":"Similarly, let's import the pre-trained ResNet-50 architecture from keras applications.\n","metadata":{}},{"cell_type":"code","source":"from keras.applications import ResNet50\n\n# initialize the base model\nbasemodel = ResNet50(input_shape=(112,112,3),\n                          include_top = False,\n                          weights = 'imagenet')\n\nfor layer in basemodel.layers:\n    layer.trainable = False\n\n# call the build_compile_fit function to complete model training\nresnet_50 = build_compile_fit(basemodel)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's display the test images along with their class labels predicted by **resnet_50**:\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n\nfor i, ax in enumerate(ax.flat):\n    ax.imshow(test_imgs[i])\n    pred_class = prob2class(resnet_50.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n    \n    # print the predicted class label as the title of the image\n    ax.set_title(pred_class, fontsize=15)\n    ax.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Authors\n","metadata":{}},{"cell_type":"markdown","source":"[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n","metadata":{}},{"cell_type":"markdown","source":"Kopal Garg is a Masters student in Computer Science at the University of Toronto.\n","metadata":{}},{"cell_type":"markdown","source":"[Roxanne Li](https://www.linkedin.com/in/roxanne-li/) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n","metadata":{}},{"cell_type":"markdown","source":"## Change Log\n","metadata":{}},{"cell_type":"markdown","source":"| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n| ----------------- | ------- | ----------- | ------------------ |\n| 2022-05-31        | 0.1     | Kopal Garg  | Created Lab        |\n| 2022-06-28        | 0.1     | Roxanne Li  | Added examples     |\n| 2022-09-07        | 0.1     | Steve Hord  | QA pass edits      |\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}