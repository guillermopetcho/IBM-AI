{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"befc0861afdad8a41bf6225db9a520efcbd34115e99e074fa7ca4245e1daa9b7","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n\n# Random Forests (RF) for classification with Python\n\nEstimated time needed: **45** minutes\n\n## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Understand the difference between Bagging and Random Forest\n*   Understand  that Random Forests have less Correlation between predictors in their ensemble, improving accuracy\n*   Apply Random Forest\n*   Understand Hyperparameters selection in  Random Forest\n","metadata":{}},{"cell_type":"markdown","source":"### **¬øQu√© es Random Forest?** üå≥üå≤  \n\n**Random Forest** es un algoritmo de **aprendizaje supervisado** basado en el concepto de **ensembles** o m√©todos de conjunto. Es una mejora de los **√°rboles de decisi√≥n** y se construye combinando m√∫ltiples √°rboles para hacer predicciones m√°s precisas y estables.  \n\nüìå **Principales caracter√≠sticas de Random Forest:**  \n‚úÖ **Es un caso especial de Bagging:** Usa la t√©cnica de **Bootstrap Aggregating (Bagging)** para reducir la varianza y evitar el sobreajuste.  \n‚úÖ **Genera m√∫ltiples √°rboles de decisi√≥n:** Cada √°rbol se entrena con un subconjunto aleatorio de los datos.  \n‚úÖ **Introduce aleatorizaci√≥n en las caracter√≠sticas:** No solo selecciona filas aleatorias (como en Bagging), sino que tambi√©n **elige aleatoriamente un subconjunto de columnas** en cada divisi√≥n del √°rbol.  \n‚úÖ **Hace predicciones mediante votaci√≥n o promedio:**  \n   - En **clasificaci√≥n**, el resultado final se obtiene por **votaci√≥n mayoritaria** entre los √°rboles.  \n   - En **regresi√≥n**, el resultado es el **promedio de las predicciones** de los √°rboles.  \n‚úÖ **Robusto ante el sobreajuste:** Gracias a la aleatorizaci√≥n y la combinaci√≥n de m√∫ltiples √°rboles, **reduce la varianza** y mejora la **precisi√≥n fuera de muestra**.  \n\n---\n\n### **¬øC√≥mo funciona?** üîç  \n\n1Ô∏è‚É£ **Se crean m√∫ltiples √°rboles de decisi√≥n** usando subconjuntos aleatorios del conjunto de datos original (t√©cnica bootstrap).  \n2Ô∏è‚É£ **Cada √°rbol se entrena de manera independiente**, tomando un subconjunto aleatorio de caracter√≠sticas en cada divisi√≥n.  \n3Ô∏è‚É£ **Para hacer una predicci√≥n:**  \n   - En **clasificaci√≥n**, cada √°rbol vota por una clase y la clase con m√°s votos gana.  \n   - En **regresi√≥n**, se promedia la predicci√≥n de todos los √°rboles.  \n\n---\n\n### **Ejemplo en Python con Scikit-Learn** üêç  \n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n\n# Cargar dataset de ejemplo (Iris)\ndata = load_iris()\nX, y = data.data, data.target\n\n# Dividir en conjunto de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear y entrenar el modelo de Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Hacer predicciones\ny_pred = rf.predict(X_test)\n\n# Evaluar la precisi√≥n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Precisi√≥n del modelo: {accuracy:.2f}')\n```\n\n---\n\n### **Ventajas de Random Forest** ‚úÖ  \n\nüîπ **Maneja datos con alta dimensionalidad** (muchas caracter√≠sticas).  \nüîπ **Es robusto al ruido** y al sobreajuste.  \nüîπ **Maneja valores faltantes** bastante bien.  \nüîπ **Funciona bien con datos no lineales**.  \nüîπ **Puede usarse tanto para clasificaci√≥n como para regresi√≥n**.  \n\n### **Desventajas de Random Forest** ‚ùå  \n\nüî∏ **Puede ser computacionalmente costoso** si hay demasiados √°rboles.  \nüî∏ **Pierde interpretabilidad** en comparaci√≥n con un solo √°rbol de decisi√≥n.  \nüî∏ **No siempre es la mejor opci√≥n si los datos son extremadamente grandes** (otros modelos como Gradient Boosting pueden ser m√°s eficientes).  \n\n---\n\n### **Conclusi√≥n** üí°  \n\nRandom Forest es un **modelo poderoso y vers√°til** que mejora la precisi√≥n y estabilidad de los √°rboles de decisi√≥n al reducir el sobreajuste. Su capacidad para manejar datos ruidosos y su rendimiento s√≥lido lo convierten en una opci√≥n ideal en muchos problemas de clasificaci√≥n y regresi√≥n. üöÄ","metadata":{}},{"cell_type":"markdown","source":"In this notebook, you will learn Random Forests (RF) for classification and Regression. Random Forest is similar to Bagging using multiple model versions and aggregating the ensemble of models to make a single prediction. RF uses an ensemble of tree‚Äôs and introduces randomness into each tree by randomly selecting a subset of the features for each node to split on. This makes the predictions of each tree uncorrelated, improving results when the models are aggregated. In this lab, we will illustrate the sampling process of RF to Bagging, then demonstrate how each predictor for random forest are not correlated. Finally, we will apply Random Forests to several datasets using Grid-Search to find the optimum  Hyperparameters.\n\n---\n\nEn este cuaderno, aprender√° sobre Bosques Aleatorios (RF) para clasificaci√≥n y regresi√≥n. El Bosque Aleatorio es similar al Bagging, ya que utiliza m√∫ltiples versiones del modelo y agrega el conjunto de modelos para generar una √∫nica predicci√≥n. El RF utiliza un conjunto de √°rboles e introduce aleatoriedad en cada uno seleccionando aleatoriamente un subconjunto de caracter√≠sticas para cada nodo. Esto hace que las predicciones de cada √°rbol no est√©n correlacionadas, lo que mejora los resultados al agregar los modelos. En este laboratorio, ilustraremos el proceso de muestreo del RF al Bagging y, a continuaci√≥n, demostraremos c√≥mo cada predictor del Bosque Aleatorio no est√° correlacionado. Finalmente, aplicaremos Bosques Aleatorios a varios conjuntos de datos mediante la b√∫squeda en cuadr√≠cula para encontrar los hiperpar√°metros √≥ptimos.\n","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of contents</h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"https://#RFvsBag\">What's the difference between RF and Bagging </a></li>\n        <li><a href=\"https://#Example\">Cancer Data Example</li>\n        <li><a href=\"https://practice/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">Practice</a></li>\n\n</div>\n<br>\n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"Let's first import the required libraries:\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas\n!pip install numpy\n!pip install matplotlib   \n!pip install seaborn\n!pip install --upgrade scikit-learn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pylab as plt\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\n%matplotlib inline \nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom tqdm import tqdm\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ignore error warnings\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function will calculate the accuracy of the training and testing data given a model.\n","metadata":{}},{"cell_type":"code","source":"def get_accuracy(X_train, X_test, y_train, y_test, model):\n    return  {\"test Accuracy\":metrics.accuracy_score(y_test, model.predict(X_test)),\"trian Accuracy\": metrics.accuracy_score(y_train, model.predict(X_train))}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function calculates the average correlation between predictors and displays the pairwise correlation between  predictors.\n","metadata":{}},{"cell_type":"code","source":"\ndef get_correlation(X_test, y_test,models):\n    #This function calculates the average correlation between predictors  \n    n_estimators=len(models.estimators_)\n    prediction=np.zeros((y_test.shape[0],n_estimators))\n    predictions=pd.DataFrame({'estimator '+str(n+1):[] for n in range(n_estimators)})\n    \n    for key,model in zip(predictions.keys(),models.estimators_):\n        predictions[key]=model.predict(X_test.to_numpy())\n    \n    corr=predictions.corr()\n    print(\"Average correlation between predictors: \", corr.mean().mean()-1/n_estimators)\n    return corr\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"RFvsBag\">  What's the difference between RF and Bagging </h2>\n","metadata":{}},{"cell_type":"markdown","source":"RF is similar to Bagging in that it uses model ensembles to make predictions. Unlike Bagging, when you add more models, RF does not suffer from Overfitting. In this section, we go over some of the differences between RF and Bagging, using the dataset:\n","metadata":{}},{"cell_type":"markdown","source":"### About the dataset\n\nWe will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically, it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n\nThis data set provides information to help you predict what behavior will help you to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n\nThe dataset includes information about:\n\n*   Customers who left within the last month ‚Äì the column is called Churn\n*   Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n*   Customer account information ‚Äì how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n*   Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents\n","metadata":{}},{"cell_type":"markdown","source":"Load Data From CSV File\n","metadata":{}},{"cell_type":"code","source":"churn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\n\nchurn_df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data pre-processing and feature selection\n","metadata":{}},{"cell_type":"markdown","source":"Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm:\n","metadata":{}},{"cell_type":"code","source":"churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'wireless','churn']]\nchurn_df['churn'] = churn_df['churn'].astype('int')\nchurn_df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Bootstrap Sampling\n\nBootstrap Sampling is a method that involves drawing sample data repeatedly with replacement from a data source to estimate a model parameter. Scikit-learn has methods for Bagging, but it is helpful to understand Bootstrap Sampling. We will import \"resample\".\n","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Consider the first five rows of data:\n","metadata":{}},{"cell_type":"code","source":"churn_df[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can perform a bootstrap sample using the function \"resample\"; we see the dataset is of the same size(5), but some rows are repeated:\n","metadata":{}},{"cell_type":"code","source":"for n in range(5):\n\n    print(resample(churn_df[0:5]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Select Variables at Random\n","metadata":{}},{"cell_type":"markdown","source":"Like Bagging, RF uses an independent bootstrap sample from the training data. In addition, we select $m$ variables at random out of all $M$ possible\nvariables. Let's do an example.\n","metadata":{}},{"cell_type":"code","source":"X=churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"there are 7 features\n","metadata":{}},{"cell_type":"code","source":"M=X.shape[1]\nM","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us select $ùëö=3$, and randomly sample features from the 5 Bootstrap Samples from above.\n","metadata":{}},{"cell_type":"code","source":"m=3","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We list out the index of the features\n","metadata":{}},{"cell_type":"code","source":"feature_index= range(M)\nfeature_index","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use the function to sample to  randomly select indexes\n","metadata":{}},{"cell_type":"code","source":"import random\nrandom.sample(feature_index,m)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We now randomly select features from the bootstrap samples to randomly select a subset of features for each node to split on.\n","metadata":{}},{"cell_type":"code","source":"for n in range(5):\n\n    print(\"sample {}\".format(n))\n    print(resample(X[0:5]).iloc[:,random.sample(feature_index,m)])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In Random Forest, we would use these data subsets to train each node of a tree.\n","metadata":{}},{"cell_type":"markdown","source":"## Train/Test dataset\n","metadata":{}},{"cell_type":"markdown","source":"Let's define X, and y for our dataset:\n","metadata":{}},{"cell_type":"code","source":"y = churn_df['churn']\ny.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train/Test dataset\n","metadata":{}},{"cell_type":"markdown","source":"We split our dataset into train and test set:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1)\nprint ('Train set', X_train.shape,  y_train.shape)\nprint ('Test set', X_test.shape,  y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Bagging  Review\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bagging improves models that suffer from overfitting; they do well on the training data, but they do not generalize well to unseen data. Decision Trees are a prime candidate for this reason. In addition, they are fast to train; We create a <code>BaggingClassifier</code> object,  with a Decision Tree as the <code>estimator</code>.\n","metadata":{}},{"cell_type":"code","source":"n_estimators=20\nBag= BaggingClassifier(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4,random_state=2),n_estimators=n_estimators,random_state=0,bootstrap=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We fit the model:\n","metadata":{}},{"cell_type":"code","source":"Bag.fit(X_train,y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The method <code>predict</code> aggregates the predictions by voting:\n","metadata":{}},{"cell_type":"code","source":"Bag.predict(X_test).shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see the training accuracy is slightly better but the test accuracy improves dramatically:\n","metadata":{}},{"cell_type":"code","source":"print(get_accuracy(X_train, X_test, y_train, y_test,  Bag))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each tree is similar; we can see this by plotting the correlation between each tree and the average correlation.\n","metadata":{}},{"cell_type":"code","source":"get_correlation(X_test, y_test,Bag).style.background_gradient(cmap='coolwarm')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It can be shown that this correlation reduces performance. Random forest minimizes the correlation between trees, improving results.\n","metadata":{}},{"cell_type":"markdown","source":"## Random  Forest\n","metadata":{}},{"cell_type":"markdown","source":"Random forests are a combination of trees such that each tree depends on a random subset of the features and data. As a result, each tree in the forest is different and usually performs better than Bagging. The most important parameters are the number of trees and the number of features to sample. First, we import <code>RandomForestClassifier</code>.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Like Bagging, increasing the number of trees improves results and does not lead to overfitting in most cases; but the improvements plateau as you add more trees. For this exxample, the number of trees in the forest (default=100):\n","metadata":{}},{"cell_type":"code","source":"n_estimators=20","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<code>max_features </code>   $m$ the number of features to consider when looking for the best split. If we have M features denoted by:\n","metadata":{}},{"cell_type":"code","source":"M_features=X.shape[1]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we have M features, a popular method to determine m is to use the square root of M\n","metadata":{}},{"cell_type":"markdown","source":"$m= floor(\\sqrt{M}) $\n","metadata":{}},{"cell_type":"code","source":"max_features=round(np.sqrt(M_features))-1\nmax_features","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use floor to make sure $m$ is an integer:\n","metadata":{}},{"cell_type":"markdown","source":"We create the RF object :\n","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier( max_features=max_features,n_estimators=n_estimators, random_state=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We train the model\n","metadata":{}},{"cell_type":"code","source":"model.fit(X_train,y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We obtain the training and testing accuracy; we see that RF does better than Bagging:\n","metadata":{}},{"cell_type":"code","source":"print(get_accuracy(X_train, X_test, y_train, y_test, model))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that each tree in RF is less correlated with the other trees than Bagging:\n","metadata":{}},{"cell_type":"code","source":"get_correlation(X_test, y_test,model).style.background_gradient(cmap='coolwarm')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"Example\">Cancer Data Example</h2>\n\nThe example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)[[http://mlearn.ics.uci.edu/MLRepository.html](http://mlearn.ics.uci.edu/MLRepository.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01)]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are:\n\n| Field name  | Description                 |\n| ----------- | --------------------------- |\n| ID          | Clump thickness             |\n| Clump       | Clump thickness             |\n| UnifSize    | Uniformity of cell size     |\n| UnifShape   | Uniformity of cell shape    |\n| MargAdh     | Marginal adhesion           |\n| SingEpiSize | Single epithelial cell size |\n| BareNuc     | Bare nuclei                 |\n| BlandChrom  | Bland chromatin             |\n| NormNucl    | Normal nucleoli             |\n| Mit         | Mitoses                     |\n| Class       | Benign or malignant         |\n\n<br>\n<br>\n\nLet's load the dataset:\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/cell_samples.csv\")\n\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now lets remove rows that have a ? in the <code>BareNuc</code> column:\n","metadata":{}},{"cell_type":"code","source":"df= df[pd.to_numeric(df['BareNuc'], errors='coerce').notnull()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We obtain the features:\n","metadata":{}},{"cell_type":"code","source":"X =  df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\n\nX.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We obtain the class labels:\n","metadata":{}},{"cell_type":"code","source":"y=df['Class']\ny.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We split the data into training and testing sets.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use <code>GridSearchCV</code> to search over specified parameter values  of the model.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create a <code>RandomForestClassifier</code> object and list the parameters using the method <code>get_params()</code>:\n","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.get_params().keys()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use GridSearch for Exhaustive search over specified parameter values. We see many of the parameters are similar to Classification trees; let's try a different parameter for <code>max_depth</code>, <code>max_features</code> and <code>n_estimators</code>.\n","metadata":{}},{"cell_type":"code","source":"param_grid = {'n_estimators': [2*n+1 for n in range(20)],\n             'max_depth' : [2*n+1 for n in range(10) ],\n             'max_features':[\"auto\", \"sqrt\", \"log2\"]}\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create the Grid Search object and fit it:\n","metadata":{}},{"cell_type":"code","source":"search = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy')\nsearch.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see the best accuracy score of the searched parameters was \\~98%.\n","metadata":{}},{"cell_type":"code","source":"search.best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The best parameter values are:\n","metadata":{}},{"cell_type":"code","source":"search.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can calculate accuracy on the test data using the test data:\n","metadata":{}},{"cell_type":"code","source":"\nprint(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"practice\">Practice</h2>\n","metadata":{}},{"cell_type":"markdown","source":"Imagine that you are a medical researcher compiling data for a study. You have collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.\n\nPart of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. The features of this dataset are Age, Sex, Blood Pressure, and the Cholesterol of the patients, and the target is the drug that each patient responded to.\n\nIt is a sample of multiclass classifier, and you can use the training part of the dataset to build a decision tree, and then use it to predict the class of a unknown patient, or to prescribe a drug to a new patient.\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv\", delimiter=\",\")\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's create the X and y for our dataset:\n","metadata":{}},{"cell_type":"code","source":"X = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\nX[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df[\"Drug\"]\ny[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now lets use a <code>LabelEncoder</code> to turn categorical features into numerical:\n","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1]) \n\n\nle_BP = preprocessing.LabelEncoder()\nle_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\nX[:,2] = le_BP.transform(X[:,2])\n\n\nle_Chol = preprocessing.LabelEncoder()\nle_Chol.fit([ 'NORMAL', 'HIGH'])\nX[:,3] = le_Chol.transform(X[:,3]) \n\nX[0:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split the data into training and testing data with a 80/20 split\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use GridSearch for Exhaustive search over specified parameter values.\n","metadata":{}},{"cell_type":"code","source":"param_grid = {'n_estimators': [2*n+1 for n in range(20)],\n             'max_depth' : [2*n+1 for n in range(10) ],\n             'max_features':[\"auto\", \"sqrt\", \"log2\"]}\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a <code>RandomForestClassifier </code>object called <cood>model</code> :\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details><summary>Click here for the solution</summary>\n\n```python\nmodel = RandomForestClassifier()\n\n```\n\n</details>\n","metadata":{}},{"cell_type":"markdown","source":"Create <code>GridSearchCV</code> object called `search` with the `estimator` set to <code>model</code>, <code>param_grid</code> set to <code>param_grid</code>, <code>scoring</code> set to <code>accuracy</code>, and  <code>cv</code> set to 3 and Fit the <code>GridSearchCV</code> object to our <code>X_train</code> and <code>y_train</code> data\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details><summary>Click here for the solution</summary>\n\n```python\nsearch = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy', cv=3)\nsearch.fit(X_train, y_train)\n\n```\n\n</details>\n","metadata":{}},{"cell_type":"markdown","source":"We can find the accuracy of the best model.\n","metadata":{}},{"cell_type":"code","source":"search.best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can find the best parameter values:\n","metadata":{}},{"cell_type":"code","source":"search.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can find the accuracy test data:\n","metadata":{}},{"cell_type":"markdown","source":"<details><summary>Click here for the solution</summary>\n\n```python\nprint(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))\n```\n\n</details>\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Want to learn more?</h2>\n\nIBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems ‚Äì by your enterprise as a whole. A free trial is available through this course, available here: <a href=\"https://www.ibm.com/analytics/spss-statistics-software?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01&utm_medium=Exinfluencer&utm_term=10006555\">SPSS Modeler</a>\n\nAlso, you can use Watson Studio to run these notebooks faster with bigger datasets. Watson Studio is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, Watson Studio enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of Watson Studio users today with a free account at <a href=\"https://www.ibm.com/cloud/watson-studio?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01&utm_medium=Exinfluencer&utm_term=10006555\">Watson Studio</a>\n","metadata":{}},{"cell_type":"markdown","source":"### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n\n### Other Contributors\n\n## <h3 align=\"center\"> ¬© IBM Corporation 2020. All rights reserved. <h3/>\n\n<!--## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n| ----------------- | ------- | ---------- | ------------------ |\n| 2022-02-09        | 0.1     | Joseph Santarcangelo | Created Lab Template |\n| 2022-05-03        | 0.2     | Richard Ye           | QA pass              |--!>\n\n\n\n\n","metadata":{}}]}