{"metadata":{"kernelspec":{"display_name":"JL_DEV","language":"python","name":"jl_dev"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Multi-Dimensional Scaling**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **45** minutes\n","metadata":{}},{"cell_type":"markdown","source":"## Use cases of Multi-Dimensional Scaling\n\n*   Recognizing families of parts in order to design cellular manufacturing systems.\n*   Creating groups of products when designing assembly areas.\n*   Market research, multi-dimensional scaling is often used to plot data such as the perception of products in an easy to interpret, visual way.\n\nFor instance, suppose a realtor has many listings to sell. Each listing has several attributes such as number of bedrooms, number of bathrooms, square feet, etc. You as a Data Scientist, are hired by the realtor to find out the similarities and dissimilarities of the listings, so that the brokers can use this information when providing recommendations for the buyers.\n\nHowever, since the number of attributes each listing has is bigger than what could be visualized, to have a clearer sense of how different each listing is you would need to reproduce the listing data on a lower dimension.\n\n<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/realtor.jpeg\" width=60%>\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Multi-Dimensional Scaling (MDS) is a family of algorithms, one version of which is Principal Component Analysis (PCA). Like PCA, MDS can be used for dimensionality reduction; MDS can also be used to map complex differences into visual space. Additional articles on MDS:   <a href=\"https://link.springer.com/chapter/10.1007/978-3-642-82580-4_139?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\"> \\[1]</a>, <a href=\"https://www.djsresearch.co.uk/glossary/item/Multi-Dimensional-Scaling?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\"> \\[2]</a>\n\nThere are several different categories of Multidimensional scaling (MDS). In this lab, we will review  Metric MDS as well as Non-Metric MDS scaling using **scikit-learn** library. For more information on MDS, please see <a href=\"https://arxiv.org/pdf/2009.08136.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\"> \\[3]</a>.\n","metadata":{}},{"cell_type":"markdown","source":"Steps in MDS analysis:\n\n*   Pre-process and generate training and testing datasets\n*   Train and fine-tune logistic regression models\n*   Interpret trained logistic regression models\n*   Evaluate trained logistic regression models\n","metadata":{}},{"cell_type":"markdown","source":"## **Table of Contents**\n\n<ol>\n    <li><a href=\"https://#Objectives\">Objectives</a></li>\n    <li><a href=\"https://#Datasets\">Datasets</a></li>\n    <li>\n        <a href=\"https://#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n            <li><a href=\"https://#Defining-Helper-Functions\">Defining-Helper-Functions</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"https://#Metric-MDS\">Metric MDS</a>\n        <ol>\n            <li><a href=\"https://#From-Relative-Location-to-Absolute-Location\">From Relative Location to Absolute Location</a></li>\n            <li><a href=\"https://#Example-1\">Example 1</a></li>\n        </ol>\n    </li>\n    <li><a href=\"https://#Non-Metric-MDS\">Non-Metric MDS</a></li>\n    <li>\n        <a href=\"https://#Dimensionality-reduction-with-MDS\">Dimensionality reduction with MDS</a>\n        <ol>\n            <li><a href=\"https://#Exercise-1\">Exercise 1 </a></li>\n        </ol>    \n    </li>   \n    <li><a href=\"https://#T-Distributed-Stochastic-Neighbor-Embedding-(optional)\">T-Distributed Stochastic Neighbor Embedding (optional)</a></li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"***\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n","metadata":{}},{"cell_type":"markdown","source":"After completing this lab you will be able to:\n","metadata":{}},{"cell_type":"markdown","source":"*   **Understand** different types of Multi-Dimensional Scaling\n*   **Understand** concepts of Metric MDS and Non-Metric MDS, including: embedding space, minimization and Stress\n*   **Apply**  Metric-MDS and Non-Metric MDS\n*   **Apply** different distance metrics to  Metric MDS and Non-Metric MDS\n*   **Apply**  MDS to dimensionality reduction\n","metadata":{}},{"cell_type":"markdown","source":"## Datasets\n\nDatasets for this lab are gathered from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) under the MIT License.\n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for mathematical operations.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for visualizing the data.\n*   [`scipy`](https://docs.scipy.org/doc/scipy/reference/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for complex numerical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n#!mamba install -qy pandas==1.3.4 numpy==1.21.4 matplotlib==3.5.0 scikit-learn==0.20.1 scipy==1.7.3\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import euclidean, cityblock, cosine\nimport sklearn.metrics.pairwise\nimport seaborn as sns\nimport folium\n# Import matplotlib for 3d plotting:\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import  MinMaxScaler\nfrom matplotlib import offsetbox\n# Make matplotlib work in jupyter notebook\n%matplotlib inline","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining Helper Functions\n","metadata":{}},{"cell_type":"markdown","source":"This function plots out labeled scatter plots of latitude and longitude data\n","metadata":{}},{"cell_type":"code","source":"def plot_points(df,color=\"red\",title=\"\"):\n\n    X=df['lon']\n    Y=df['lat']\n\n    annotations=df.index\n\n    plt.figure(figsize=(8,6))\n    plt.scatter(X,Y,s=100,color=color)\n    plt.title(title)\n    plt.xlabel(\"lat\")\n    plt.ylabel(\"log\")\n    for i, label in enumerate(annotations):\n        plt.annotate(label, (X[i], Y[i]))\n    plt.axis('equal')\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function plots out labelled scatter plots of digits dataset in two dimensions  after a Dimensionality reduction:\n","metadata":{}},{"cell_type":"code","source":"def plot_embedding(X, title, ax):\n    X = MinMaxScaler().fit_transform(X)\n    for digit in digits.target_names:\n        ax.scatter(\n            *X[y == digit].T,\n            marker=f\"${digit}$\",\n            s=60,\n            color=plt.cm.Dark2(digit),\n            alpha=0.425,\n            zorder=2,\n        )\n    shown_images = np.array([[1.0, 1.0]])  # just something big\n    for i in range(X.shape[0]):\n        # plot every digit on the embedding\n        # show an annotation box for a group of digits\n        dist = np.sum((X[i] - shown_images) ** 2, 1)\n        if np.min(dist) < 4e-3:\n            # don't show points that are too close\n            continue\n        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n        imagebox = offsetbox.AnnotationBbox(\n            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n        )\n        imagebox.set(zorder=1)\n        ax.add_artist(imagebox)\n\n    ax.set_title(title)\n    ax.axis(\"off\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metric MDS\n","metadata":{}},{"cell_type":"markdown","source":"Metric MDS  represents points in an embedding space $\\boldsymbol{Z}$  by preserving the distances $d\\_{i,j}$ distance between $i-th $ and $j-th $ objects. Each distance is given by:\n","metadata":{}},{"cell_type":"markdown","source":"\\begin{pmatrix}\nd\\_{1,1} & d\\_{1,2} & \\cdots & d\\_{1,N} \\\\\\\\\\\\\\\\\nd\\_{2,1} & d\\_{2,2} & \\cdots & d\\_{2,N} \\\\\\\\\\\\\\\\\n\\vdots & \\vdots & & \\vdots \\\\\\\\\\\\\\\\\nd\\_{N,1} & d\\_{N,2} & \\cdots & d\\_{N,N}\n\\end{pmatrix}.\n","metadata":{}},{"cell_type":"markdown","source":"For the distance $d\\_{i,j}$ between objects $x_i$ and $x_j$,  we find corresponding points $z_i$ and $z_j$ that minimize the cost function called “**Stress**”, which is a residual sum of squares:\n","metadata":{}},{"cell_type":"markdown","source":"$\\text{Stress}*D(z\\_1,z\\_2,...,z_N)=\\Biggl(\\sum*{i\\ne j=1,...,N}\\bigl(d\\_{ij}-|z_i-z_j|\\bigr)^2\\Biggr)^{1/2}$\n","metadata":{}},{"cell_type":"markdown","source":"The goal is to find the embeddings   $z_i$ ,$z_j$ whose euclidean distance is most similar to the original $d\\_{i,j}$. We will experiment with several different distance metrics for $d\\_{i,j}$, and for the embedding space we can use any distance metric $d(z_i,z_j)$.  We will focus on the euclidean distance $d(z_i,z_j)= |z_i-z_j|$. For more check out <a href=\"https://arxiv.org/pdf/2009.08136.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\"> \\[1]</a>.\n","metadata":{}},{"cell_type":"markdown","source":"### From Relative Location to Absolute  Location\n","metadata":{}},{"cell_type":"markdown","source":"To better understand how MDS works, we would like to find the  position of several cities, given the length of the Relative latitude and longitude. Latitude is an angle that specifies the north–south position of a point on the Earth, which ranges from 0° at the Equator to 90° (North or South) at the poles. Lines of constant latitude, or parallels, run east–west as circles parallel to the equator. Longitude(λ) specifies the east–west position of a point on the Earth's surface, usually denoted by the Greek letter lambda (λ). Meridians (lines running from pole to pole) connect points with the same longitude as show here.\n","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/images/lat-log.png\" width=\"300\" alt=\"loglat\"  />\n</center>\n","metadata":{}},{"cell_type":"markdown","source":"Latitude and longitude are calculated by examining celestial bodies' angles, sometimes in combination with time. We can calculate the difference between the angle relatively easily. This is shown in the figure below, where the boat and the city have different angles with the sun:\n","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/images/Screen_Shot_2022-03-01_at_6.26.10_PM.png\"  width=\"600\" />\n</center>\n","metadata":{}},{"cell_type":"markdown","source":"To make things simpler, we will use angles between the cities. Here is the length of the relative latitude and longitude of several cities $d\\_{ij}$.\n","metadata":{}},{"cell_type":"code","source":"distance=pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/distance.csv').set_index('name')\ndistance.head(8)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For Multidimensional Scaling in `sklearn`, we import the `MDS` constructor from the `manifold` module:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import MDS","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create an MDS object `embedding` with the following parameters:\n","metadata":{}},{"cell_type":"markdown","source":"`n_components`: Number of dimensions in which to immerse the dissimilarities, default=2\n\n`precomputed`: Pre-computed dissimilarities are passed directly to **fit** and **fit_transform**\n\n`max_iter` : Maximum number of iterations of the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01'> SMACOF</a> algorithm for a single run, default = 300\n\n`eps`: Relative tolerance with respect to stress at which to declare convergence, default=1e-3\n","metadata":{}},{"cell_type":"code","source":"embedding =  MDS(dissimilarity='precomputed',n_components=2,random_state=0,max_iter=300,eps=1e-3)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We apply `fit_transform` to the data from  `distance` array that fits the model and returns the embedded coordinates. The stress is minimized via  <a href='https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01'> SMACOF</a>.\n","metadata":{}},{"cell_type":"code","source":"X_transformed = embedding.fit_transform(distance)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We then plot the coordinates as an 'X_transformed'  approximation of the longitude and latitude.\n","metadata":{}},{"cell_type":"code","source":"df_t=pd.DataFrame(X_transformed , columns=[\"lon\",\"lat\"], index=distance.columns)\ndf_t.head(8)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also obtain the embeddings using the attribute `embedding_`.\n","metadata":{}},{"cell_type":"code","source":"embedding.embedding_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can obtain the stress (sum of squared distance of the disparities and the distances for all constrained points).\n","metadata":{}},{"cell_type":"code","source":"embedding.stress_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, we will use the pairwise `dissimilarity_matrix_` function.\n","metadata":{}},{"cell_type":"code","source":"embedding.dissimilarity_matrix_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let’s compare the embeddings  to the longitude and latitude of each city.\n","metadata":{}},{"cell_type":"code","source":"# Make a data frame with dots to show on the map\ndf = pd.DataFrame({\n   'lon':[-58, 2, 145, 30.32, -4.03, -73.57, 36.82, -38.5],\n   'lat':[-34, 49, -38, 59.93, 5.33, 45.52, -1.29, -12.97],\n   'name':['Buenos Aires', 'Paris', 'Melbourne', 'St Petersbourg', 'Abidjan', 'Montreal', 'Nairobi', 'Salvador']})\ndf=df.set_index('name')\ndf.head(10)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can compare the embeddings to the original data:\n","metadata":{}},{"cell_type":"code","source":"plot_points(df,title='original dataset')\n\nplot_points(df_t,color='blue',title='Embedded Coordinates using Euclidean distance ')","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see the longitude relationship persevered, but the latitude seems to be distorted. This is because MDS preserves the stress relationship between data points. We can verify this by examining the clusters in the embedded coordinates.\n\nBy examining the embedded coordinates, we see three groups or clusters. If we examine the distance dataframe, we see these groups are near each other. For example, Melbourne and Buenos Aires have a distance of four, this corresponds to the cluster of the cities at the bottom. Similarly, if you examine the relationship between Paris, St Petersbourg and Montreal, you see they are near each other. The same relation holds for the rest of the other cities.\n","metadata":{}},{"cell_type":"markdown","source":"### Example 1\n\nIn this case, we can recreate the distance dataframe using `pdist`. This calculates the Pairwise distances between data points in df. We then apply `squareform` that converts the result to a square-form distance matrix:\n\n$\\sqrt{(lon_i-lon_j)^2 + (lat_i-lat_j)^2 }$\n","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import squareform, pdist\n\ndistance=pd.DataFrame(squareform(pdist(df.iloc[:, 1:])), columns=df.index, index=df.index)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see MDS clusters on the distance matrix; unlike the principal component analysis, we can apply different distances. In the following  lines of code, we use other distance metrics on the longitude and latitude of different cities, then apply MDS. We plot out the results and see each distance has its own unique property.\n","metadata":{}},{"cell_type":"code","source":"dist=['cosine','cityblock','seuclidean','sqeuclidean','cosine','hamming','jaccard','chebyshev','canberra','braycurtis']\nplot_points(df,title='original dataset')\nfor d in dist:\n\n    distance=pd.DataFrame(squareform(pdist(df.iloc[:, 1:],metric=d)), columns=df.index, index=df.index)\n\n    embedding =  MDS(dissimilarity='precomputed', random_state=0,n_components=2)\n    X_transformed = embedding.fit_transform(distance)\n    df_t=pd.DataFrame(X_transformed , columns=df.columns, index=df.index)\n\n    plot_points(df_t,title='Embedded Coordinates using '+d ,color='blue')","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Non-Metric MDS\n","metadata":{}},{"cell_type":"markdown","source":"In  Non-Metric MDS, we apply a function $f(.)$, a distance metric, $d(z_i,z_j)$, for the distances between the points in the embedding space.  $f(.)$ is a monotonic function. This means it's always increasing, this preserves the order of the distance. For example, if one distance is larger before applying the function, it will still be larger after applying the function.\n","metadata":{}},{"cell_type":"markdown","source":"$\\text{Stress}*D(z\\_1,z\\_2,...,z_N)=\\Biggl(\\frac{\\sum*{i\\ne j=1,...,N}\\bigl(f(d(z_i,z_j))-d\\_{i,j}\\bigr)^2}{\\sum d\\_{i,j}^2}\\Biggl)^{1/2}$\n","metadata":{}},{"cell_type":"markdown","source":"To  perform non-metric MDS , we first set `metric` to False.\n","metadata":{}},{"cell_type":"code","source":"metric=False\nembedding =  MDS(dissimilarity='precomputed',n_components=2,metric=metric,random_state=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We then apply `fit_transform()` to the data from array distance that fits the model and returns the embedded coordinates, and convert it to a dataframe:\n","metadata":{}},{"cell_type":"code","source":"X_transformed = embedding.fit_transform(distance)\ndf_t=pd.DataFrame(X_transformed , columns=df.columns, index=df.index)\ndf_t.head(8)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Non-metric MDS does better at preserving the horizontal scale, but the cities are negatively scaled in the longitudinal direction. At the same time, some cities like Paris and Montreal seem to have a different relationship  in the latitudinal direction.\n","metadata":{}},{"cell_type":"code","source":"plot_points(df,title='original dataset')\n\nplot_points(df_t,color='blue',title='Embedded Coordinates using Euclidean distance ')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also apply different distances to Non-Metric MDS.\n","metadata":{}},{"cell_type":"code","source":"dist=['cosine','cityblock','seuclidean','sqeuclidean','cosine','hamming','jaccard','chebyshev','canberra','braycurtis']\nplot_points(df,title='original dataset')\nmetric=False\nfor d in dist:\n\n    distance=pd.DataFrame(squareform(pdist(df.iloc[:, 1:],metric=d)), columns=df.index, index=df.index)\n\n    embedding =  MDS(dissimilarity='precomputed', random_state=0,n_components=2,metric=False)\n    X_transformed = embedding.fit_transform(distance)\n    df_t=pd.DataFrame(X_transformed , columns=df.columns, index=df.index)\n\n    plot_points(df_t,title='Embedded Coordinates using '+d ,color='blue')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dimensionality Reduction with MDS\n","metadata":{}},{"cell_type":"markdown","source":"MDS can also be used for Dimensionality Reduction. We just need to set $d\\_{ij}=||x_i-x_j||\\ $, where the dimension of $x_i$ is greater than $z_j$. Consider the  digits dataset that consists of 8x8 pixel images of digits ranging from 0 to 9.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_digits\n\ndigits = load_digits(n_class=6)\nX, y = digits.data, digits.target\nn_samples, n_features = X.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have 64 features or dimensions of $x_i$\n","metadata":{}},{"cell_type":"code","source":"print(\"samples:\", n_samples, \"features\", n_features)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot out the different $x_i$  by converting them to a square array:\n","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\nfor idx, ax in enumerate(axs.ravel()):\n    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n    ax.axis(\"off\")\n_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create an embedding object. As we are using the features X,  we set dissimilarity to `euclidean`, which is the default choice for the `dissimilarity` parameter in **sklearn.manifold.MDS**.\n","metadata":{}},{"cell_type":"code","source":"embedding=MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can apply MDS to the data X, we will fit the model and make a prediction simultaneously using the <code>fit_transform</code> method.\n","metadata":{}},{"cell_type":"code","source":"X_transformed=embedding.fit_transform(X)","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We plot the samples in the embedding space and overlay images of the digit and the colour representing the digit. We see that although there is some overlap, each cluster corresponds to a different digit, this is despite the fact that the method is unsupervised.\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nplot_embedding(X_transformed, \"Metric MDS \", ax)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"In this exercise, you will apply metric  MDS using different distance metrics in the list dist to the digit dataset X_norm that has been normalized.\n","metadata":{}},{"cell_type":"code","source":"dist=['cosine','cityblock','hamming','jaccard','chebyshev','canberra','braycurtis']\nscaler = MinMaxScaler()\nX_norm=scaler.fit_transform(X)\n\n# your code starts here#\n\nfor d in dist:\n\n    distance=squareform(pdist(X_norm,metric=d))\n    print( d)\n\n    embedding =  MDS(dissimilarity='precomputed', random_state=0,n_components=2)\n    X_transformed = embedding.fit_transform(distance)\n    fig, ax = plt.subplots()\n    plot_embedding(X_transformed, \"Metric MDS \", ax)\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## T-Distributed Stochastic Neighbor Embedding (optional)\n","metadata":{}},{"cell_type":"markdown","source":"T-Distributed Stochastic Neighbor Embedding (TSNE) is a method for visualizing high-dimensional data. It usually works better than MDS for visualizing data. The method is based on probabilities, unlike MDS, you can't use the distances.\n","metadata":{}},{"cell_type":"markdown","source":"We apply the algorithm to the same dataset. As we see the setup is very similar.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_embedded = TSNE(n_components=2, init='random').fit_transform(X)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots()\nplot_embedding(X_embedded , \"test\", ax)\nplt.show()","metadata":{},"outputs":[],"execution_count":null}]}