{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"922171359584e6082dace195bc7775efb8befb07a6794017ceee7bdf6f73137b","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n\n# Machine Learning Foundation\n\n## Course 3, Part f: Boosting and Stacking LAB\n\nEstimated time needed: **45** minutes\n\n## Objectives\n\nAfter completing this demo you will be able to:\n\n*   Understand how Gradient Boosting helps reduce error\n*   Identify the benefits of using AdaBoost\n*   Interpret the benefits of stacking models and comparing their results to boosted models\n","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of contents</h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"https://#Q1\">Question 1</a></li>\n        <li><a href=\"https://#Q2\">Question 2</a></li>\n        <li><a href=\"https://#Q3\">Question 3</a></li>\n        <li><a href=\"https://#Q4\">Question 4</a></li>\n        <li><a href=\"https://#Q5\">Question 5</a></li>\n        <li><a href=\"https://#Q6\">Question 6</a></li>\n</div>\n<br>\n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nWe will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n\nFor each record in the dataset it is provided:\n\n*   Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n*   Triaxial angular velocity from the gyroscope.\n*   A 561-feature vector with time and frequency domain variables.\n*   Its activity label.\n\nMore information about the features is available on the website shown above.\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn\n!pip install pandas\n!pip install numpy\n!pip install matplotlib   \n!pip install seaborn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nimport seaborn as sns, pandas as pd, numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T15:50:57.461982Z","iopub.execute_input":"2025-03-23T15:50:57.462367Z","iopub.status.idle":"2025-03-23T15:51:00.506632Z","shell.execute_reply.started":"2025-03-23T15:50:57.462327Z","shell.execute_reply":"2025-03-23T15:51:00.505465Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<h2 id=\"Q1\">Question 1</h2>\n\n*   Import the data from the file `Human_Activity_Recognition_Using_Smartphones_Data.csv` and examine the shape and data types. For the data types, there will be too many to list each column separately. Rather, aggregate the types by count.\n*   Determine if the float columns need to be scaled.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\ndata = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data has quite a few predictor columns.\n","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And they're all float values. The only non-float is the categories column, which is being predicted.\n","metadata":{}},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The minimum and maximum value for the float columns is -1.0 and 1.0, respectively. However, scaling is never required for tree-based methods.\n","metadata":{}},{"cell_type":"code","source":"# Mask to select float columns\nfloat_columns = (data.dtypes == float)\n\n# Verify that the maximum of all float columns is 1.0\nprint( (data.loc[:,float_columns].max()==1.0).all() )\n\n# Verify that the minimum of all float columns is -1.0\nprint( (data.loc[:,float_columns].min()==-1.0).all() )\n### END SOLUTION","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"Q2\">Question 2</h2>\n\n*   Integer encode the activities.\n*   Split the data into train and test data sets. Decide if the data will be stratified or not during the train/test split.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndata['Activity'] = le.fit_transform(data['Activity'])\n\nle.classes_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.Activity.unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**NOTE**: We are about to create training and test sets from `data`. On those datasets, we are going to run grid searches over many choices of parameters. This can take some time. In order to shorten the grid search time, feel free to downsample `data` and create `X_train, X_test, y_train, y_test` from the downsampled dataset.\n\nNow split the data into train and test data sets. A stratified split was not used here. If there are issues with any of the error metrics on the test set, it can be a good idea to start model fitting over using a stratified split. Boosting is a pretty powerful model, though, so it may not be necessary in this case.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Alternatively, we could stratify the categories in the split, as was done previously\nfeature_columns = [x for x in data.columns if x != 'Activity']\n\nX_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data['Activity'],\n                 test_size=0.3, random_state=42)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape\n### END SOLUTION","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"En este laboratorio se trabaja con la base de datos de reconocimiento de actividad humana con smartphones, ya utilizada anteriormente para regresión logística. Se importan las bibliotecas necesarias, se carga el dataset (con 10.299 filas y 562 columnas), y se verifica que los datos estén preprocesados (valores entre -1 y 1, y en su mayoría de tipo flotante).\n\nAunque el escalado no es necesario para modelos basados en árboles, se comprueba que los datos estén listos. Luego, se codifica la variable de salida (actividad) como valores enteros usando un `LabelEncoder`, y se analiza si es necesario estratificar la división de entrenamiento y prueba (en este caso no lo es, porque los datos están balanceados).\n\nFinalmente, se definen las variables predictoras y la variable objetivo, se hace el split (70/30), y se prepara todo para iniciar el modelado con un clasificador de **gradient boosting** en el siguiente video.\n\n---\n\n¿Querés que te ayude a convertirlo también en una presentación o guía paso a paso?","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"Q3\">Question 3</h2>\n\n*   Fit gradient boosted tree models with all parameters set to their defaults with the following tree numbers (`n_estimators = [15, 25, 50, 100, 200, 400]`) and evaluate the accuracy on the test data for each of these models.\n*   Plot the accuracy as a function of estimator number.\n\n*Note:* there is no out-of-bag error for boosted models. And the `warm_flag=True` setting has a bug in the gradient boosted model, so don't use it. Simply create the model inside the `for` loop and set the number of estimators at this time. This will make the fitting take a little longer. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\nerror_list = list()\n\n# Iterate through various possibilities for number of trees\ntree_list = [15, 25, 50, 100, 200, 400]\nfor n_trees in tree_list:\n    \n    # Initialize the gradient boost classifier\n    GBC = GradientBoostingClassifier(n_estimators=n_trees, random_state=42)\n\n    # Fit the model\n    print(f'Fitting model with {n_trees} trees')\n    GBC.fit(X_train.values, y_train.values)\n    y_pred = GBC.predict(X_test)\n\n    # Get the error\n    error = 1.0 - accuracy_score(y_test, y_pred)\n    \n    # Store it\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerror_df = pd.concat(error_list, axis=1).T.set_index('n_trees')\n\nerror_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now plot the result.\n","metadata":{}},{"cell_type":"code","source":"sns.set_context('talk')\nsns.set_style('white')\n\n# Create the plot\nax = error_df.plot(marker='o', figsize=(12, 8), linewidth=5)\n\n# Set parameters\nax.set(xlabel='Number of Trees', ylabel='Error')\nax.set_xlim(0, max(error_df.index)*1.1);\n### END SOLUTION","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"Q4\">Question 4</h2>\n\n*   Using a grid search with cross-validation, fit a new gradient boosted classifier with the same list of estimators as question 3. Also try varying the learning rates (0.1, 0.01, 0.001, etc.), the subsampling value (1.0 or 0.5), and the number of maximum features (1, 2, etc.).\n*   Examine the parameters of the best fit model.\n*   Calculate relevant error metrics on this model and examine the confusion matrix.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.model_selection import GridSearchCV\n\n# The parameters to be fit\nparam_grid = {'n_estimators': tree_list,\n              'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n              'subsample': [1.0, 0.5],\n              'max_features': [1, 2, 3, 4]}\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The best model\nGV_GBC.best_estimator_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U scikit-learn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The error metrics. Classification report is particularly convenient for multi-class cases.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ny_pred = GV_GBC.predict(X_test)\nprint(classification_report(y_pred, y_test))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The confusion matrix. Note that the gradient boosted model has a little trouble distinguishing between activity class 1 and 2.\n","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"Q5\">Question 5</h2>\n\n*   Create an AdaBoost model and fit it using grid search, much like question 4. Try a range of estimators between 100 and 200.\n*   Compare the errors from AdaBoost to those from the GradientBoostedClassifier.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.001]}\n\nGV_ABC = GridSearchCV(ABC,\n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\nGV_ABC = GV_ABC.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The best model.\n","metadata":{}},{"cell_type":"code","source":"# The best model\nGV_ABC.best_estimator_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The error metrics. Note that the issues with class 1 and 2 appear to have become more problematic. Also note other issues for classes 3 - 5. AdaBoost is very sensitive to outliers, so that could be the problem here.\n","metadata":{}},{"cell_type":"code","source":"y_pred = GV_ABC.predict(X_test)\nprint(classification_report(y_pred, y_test))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Resumen: Modelado con Gradient Boosting y comparación con AdaBoost**\n\nEn esta sección del cuaderno, se utiliza el algoritmo **Gradient Boosting Classifier (GBC)** para analizar cómo varía la **precisión** del modelo al cambiar el número de árboles (estimadores). Para agilizar el entrenamiento, se limita el número máximo de características a 5.\n\n- Se prueba GBC con distintos números de árboles (15 a 400) y se evalúa la **tasa de error** (1 - precisión).\n- Se observa que al aumentar los árboles, el error disminuye, pero los rendimientos decrecen con más de 200 árboles.\n- Se grafica esta relación para visualizar la mejora en el desempeño.\n\nLuego, se realiza una **búsqueda de hiperparámetros con GridSearchCV** usando:\n- Tasa de aprendizaje: 0.1, 0.01, 0.001\n- Submuestreo: 1, 0.5\n- Máx. de características: 2, 3, 4\n- Número de árboles fijo en 400\n\nEl mejor modelo resultante tiene:\n- Tasa de aprendizaje: 0.1\n- Submuestreo: 0.5\n- Máx. de características: 4\n- Precisión general: **99%**, con muy buen desempeño en todas las clases, salvo leves confusiones entre clases 1 y 2 (sentado/parado).\n\nPara no volver a entrenar el modelo en caso de error o reinicio, se lo guarda con **`pickle`**.\n\nLuego, se repite el proceso con **AdaBoostClassifier**, ajustando también hiperparámetros mediante GridSearchCV. El mejor modelo encontró:\n- Tasa de aprendizaje: 0.01\n- Número de árboles: 100\n\nSin embargo, **AdaBoost rindió peor**, especialmente en clases 1 y 2, lo que puede deberse a su mayor sensibilidad a valores atípicos.\n\n---\n\n### **Conclusión:**\nGradient Boosting mostró mejor rendimiento general que AdaBoost en este dataset. En el siguiente módulo se trabajará con **Stacking (apilamiento)** usando un clasificador por votación.\n\n---\n\n¿Querés que te lo convierta en una diapositiva o guía práctica para estudiar?","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"Q6\">Question 6</h2>\n\n*   Fit a logistic regression model with regularization.\n*   Using `VotingClassifier`, fit the logistic regression model along with either the GratientBoostedClassifier or the AdaBoost model (or both) from questions 4 and 5.\n*   Determine the error as before and compare the results to the appropriate gradient boosted model(s).\n*   Plot the confusion matrix for the best model created in this set of exercises.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.linear_model import LogisticRegression\n\n# L2 regularized logistic regression\nLR_L2 = LogisticRegression(penalty='l2', max_iter=500, solver='saga').fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check the errors and confusion matrix for the logistic regression model.\n","metadata":{}},{"cell_type":"code","source":"y_pred = LR_L2.predict(X_test)\nprint(classification_report(y_pred, y_test))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And now the stacked model.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# The combined model--logistic regression and gradient boosted trees\nestimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\n\n# Though it wasn't done here, it is often desirable to train \n# this model using an additional hold-out data set and/or with cross validation\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performance for the voting classifier should improve relative to either logistic regression or gradient boosted trees alone. However, the fact that logistic regression does almost as well as gradient boosted trees is an important reminder to try the simplest model first. In some cases, its performance will be good enough.\n","metadata":{}},{"cell_type":"code","source":"y_pred = VC.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.show()\n#sns.set_context('talk')\n#cm = confusion_matrix(y_test, y_pred)\n#ax = sns.heatmap(cm, annot=True, fmt='d')\n### END SOLUTION","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n\n### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n\n### Other Contributors\n\n<a href=\"https://www.linkedin.com/in/richard-ye/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Richard Ye</a>\n\n## <h3 align=\"center\"> Machine Learning Foundation (C) 2020 IBM Corporation </h3>\n\n<!--## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n| ----------------- | ------- | ---------- | ------------------ |\n| 2022-03-23        | 0.1     | Joseph Santarcangelo | Created Lab Template |\n| 2022-05-03        | 0.2     | Richard Ye | Added in estimated time, objectives and table of contents |--!>\n\n\n","metadata":{}}]}