{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"a3a444c551a80a10d5c22a758b4131644ed42314141fd85ae1a801414c3f4129","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Foundation\n\n## Course 3, Part d: Decision Tree LAB\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nWe will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data.csv`.\n","metadata":{}},{"cell_type":"code","source":"!pip install numpy\n!pip install pandas\n!pip install seaborn\n!pip install matplotlib \n!pip install skict-learn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:32:47.581948Z","iopub.execute_input":"2025-03-20T15:32:47.582269Z","iopub.status.idle":"2025-03-20T15:32:50.935846Z","shell.execute_reply.started":"2025-03-20T15:32:47.582237Z","shell.execute_reply":"2025-03-20T15:32:50.934714Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Question 1\n\n* Import the data and examine the features.\n* We will be using all of them to predict `color` (white or red), but the colors feature will need to be integer encoded.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\ndata = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Wine_Quality_Data.csv\", sep=',')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:32:58.091203Z","iopub.execute_input":"2025-03-20T15:32:58.091640Z","iopub.status.idle":"2025-03-20T15:32:59.650607Z","shell.execute_reply.started":"2025-03-20T15:32:58.091605Z","shell.execute_reply":"2025-03-20T15:32:59.649435Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:33:01.518548Z","iopub.execute_input":"2025-03-20T15:33:01.518940Z","iopub.status.idle":"2025-03-20T15:33:01.558514Z","shell.execute_reply.started":"2025-03-20T15:33:01.518907Z","shell.execute_reply":"2025-03-20T15:33:01.557264Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality color  \n0      9.4        5   red  \n1      9.8        5   red  \n2      9.8        5   red  \n3      9.8        6   red  \n4      9.4        5   red  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed_acidity</th>\n      <th>volatile_acidity</th>\n      <th>citric_acid</th>\n      <th>residual_sugar</th>\n      <th>chlorides</th>\n      <th>free_sulfur_dioxide</th>\n      <th>total_sulfur_dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>color</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>red</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>red</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>red</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n      <td>red</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>red</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:33:08.775212Z","iopub.execute_input":"2025-03-20T15:33:08.775597Z","iopub.status.idle":"2025-03-20T15:33:08.782896Z","shell.execute_reply.started":"2025-03-20T15:33:08.775565Z","shell.execute_reply":"2025-03-20T15:33:08.781768Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"fixed_acidity           float64\nvolatile_acidity        float64\ncitric_acid             float64\nresidual_sugar          float64\nchlorides               float64\nfree_sulfur_dioxide     float64\ntotal_sulfur_dioxide    float64\ndensity                 float64\npH                      float64\nsulphates               float64\nalcohol                 float64\nquality                   int64\ncolor                    object\ndtype: object"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Convert the color feature to an integer. This is a quick way to do it using Pandas.\n","metadata":{}},{"cell_type":"code","source":"data['color'] = data['color'].replace({'white': 0, 'red': 1}).astype(int)\n### END SOLUTION","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:34:30.397130Z","iopub.execute_input":"2025-03-20T15:34:30.397521Z","iopub.status.idle":"2025-03-20T15:34:30.402910Z","shell.execute_reply.started":"2025-03-20T15:34:30.397488Z","shell.execute_reply":"2025-03-20T15:34:30.401581Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Question 2\n\n* Use `StratifiedShuffleSplit` to split data into train and test sets that are stratified by wine quality. If possible, preserve the indices of the split for question 5 below.\n* Check the percent composition of each quality level for both the train and test data sets.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\n# All data columns except for color\nfeature_cols = [x for x in data.columns if x not in 'color']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:34:45.889135Z","iopub.execute_input":"2025-03-20T15:34:45.889507Z","iopub.status.idle":"2025-03-20T15:34:45.894221Z","shell.execute_reply.started":"2025-03-20T15:34:45.889473Z","shell.execute_reply":"2025-03-20T15:34:45.893048Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Split the data into two parts with 1000 points in the test data\n# This creates a generator\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)\n\n# Get the index values from the generator\ntrain_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['color']))\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'color']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'color']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:35:01.033025Z","iopub.execute_input":"2025-03-20T15:35:01.033373Z","iopub.status.idle":"2025-03-20T15:35:01.048617Z","shell.execute_reply.started":"2025-03-20T15:35:01.033346Z","shell.execute_reply":"2025-03-20T15:35:01.047321Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Now check the percent composition of each quality level in the train and test data sets. The data set is mostly white wine, as can be seen below.\n","metadata":{}},{"cell_type":"code","source":"y_train.value_counts(normalize=True).sort_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:35:05.074129Z","iopub.execute_input":"2025-03-20T15:35:05.074516Z","iopub.status.idle":"2025-03-20T15:35:05.090355Z","shell.execute_reply.started":"2025-03-20T15:35:05.074464Z","shell.execute_reply":"2025-03-20T15:35:05.089194Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"color\n0    0.753866\n1    0.246134\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"y_test.value_counts(normalize=True).sort_index()\n### END SOLUTION","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:35:08.799576Z","iopub.execute_input":"2025-03-20T15:35:08.799916Z","iopub.status.idle":"2025-03-20T15:35:08.807556Z","shell.execute_reply.started":"2025-03-20T15:35:08.799889Z","shell.execute_reply":"2025-03-20T15:35:08.806633Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"color\n0    0.754\n1    0.246\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Question 3\n\n* Fit a decision tree classifier with no set limits on maximum depth, features, or leaves.\n* Determine how many nodes are present and what the depth of this (very large) tree is.\n* Using this tree, measure the prediction error in the train and test data sets. What do you think is going on here based on the differences in prediction error?\n\n---\n\n* Ajuste un clasificador de árbol de decisión sin límites definidos de profundidad máxima, características ni hojas.\n* Determine cuántos nodos hay y cuál es la profundidad de este árbol (muy grande).\n* Con este árbol, mida el error de predicción en los conjuntos de datos de entrenamiento y prueba. ¿Qué cree que ocurre en función de las diferencias en el error de predicción?","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:38:34.454616Z","iopub.execute_input":"2025-03-20T15:38:34.454984Z","iopub.status.idle":"2025-03-20T15:38:34.511256Z","shell.execute_reply.started":"2025-03-20T15:38:34.454957Z","shell.execute_reply":"2025-03-20T15:38:34.510219Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"The number of nodes and the maximum actual depth.\n\nEl número de nodos y la profundidad real máxima.","metadata":{}},{"cell_type":"code","source":"dt.tree_.node_count, dt.tree_.max_depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:38:37.431761Z","iopub.execute_input":"2025-03-20T15:38:37.432139Z","iopub.status.idle":"2025-03-20T15:38:37.438300Z","shell.execute_reply.started":"2025-03-20T15:38:37.432110Z","shell.execute_reply":"2025-03-20T15:38:37.437333Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(171, 22)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"A function to return error metrics.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:45:57.109200Z","iopub.execute_input":"2025-03-20T15:45:57.109590Z","iopub.status.idle":"2025-03-20T15:45:57.114635Z","shell.execute_reply.started":"2025-03-20T15:45:57.109557Z","shell.execute_reply":"2025-03-20T15:45:57.113545Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"The decision tree predicts a little better on the training data than the test data, which is consistent with (mild)  overfitting. Also notice the perfect recall score for the training data. In many instances, this prediction difference is even greater than that seen here. \n","metadata":{}},{"cell_type":"code","source":"# The error on the training and test data sets\ny_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n                              measure_error(y_test, y_test_pred, 'test')],\n                              axis=1)\n\ntrain_test_full_error\n### END SOLUTION","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:46:01.902117Z","iopub.execute_input":"2025-03-20T15:46:01.902512Z","iopub.status.idle":"2025-03-20T15:46:01.939087Z","shell.execute_reply.started":"2025-03-20T15:46:01.902478Z","shell.execute_reply":"2025-03-20T15:46:01.938028Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"              train      test\naccuracy   0.999818  0.984000\nprecision  0.999261  0.963710\nrecall     1.000000  0.971545\nf1         0.999631  0.967611","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>accuracy</th>\n      <td>0.999818</td>\n      <td>0.984000</td>\n    </tr>\n    <tr>\n      <th>precision</th>\n      <td>0.999261</td>\n      <td>0.963710</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>1.000000</td>\n      <td>0.971545</td>\n    </tr>\n    <tr>\n      <th>f1</th>\n      <td>0.999631</td>\n      <td>0.967611</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Question 4\n\n* Using grid search with cross validation, find a decision tree that performs well on the test data set. Use a different variable name for this decision tree model than in question 3 so that both can be used in question 6.\n* Determine the number of nodes and the depth of this tree.\n* Measure the errors on the training and test sets as before and compare them to those from the tree in question 3.\n  \n---\n\n* Mediante la búsqueda en cuadrícula con validación cruzada, encuentre un árbol de decisión que funcione correctamente en el conjunto de datos de prueba. Use un nombre de variable diferente para este modelo de árbol de decisión que el de la pregunta 3, de modo que ambos puedan usarse en la pregunta 6.\n* Determine el número de nodos y la profundidad de este árbol.\n* Mida los errores en los conjuntos de entrenamiento y prueba como antes y compárelos con los del árbol de la pregunta 3.","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n              'max_features': range(1, len(dt.feature_importances_)+1)}\n\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR = GR.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:50:10.553292Z","iopub.execute_input":"2025-03-20T15:50:10.553690Z","iopub.status.idle":"2025-03-20T15:50:15.731605Z","shell.execute_reply.started":"2025-03-20T15:50:10.553658Z","shell.execute_reply":"2025-03-20T15:50:15.730528Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"The number of nodes and the maximum depth of the tree.\n\nEl número de nodos y la profundidad máxima del árbol.\n","metadata":{}},{"cell_type":"code","source":"GR.best_estimator_.tree_.node_count, GR.best_estimator_.tree_.max_depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:47:11.767288Z","iopub.execute_input":"2025-03-20T15:47:11.767706Z","iopub.status.idle":"2025-03-20T15:47:11.774407Z","shell.execute_reply.started":"2025-03-20T15:47:11.767670Z","shell.execute_reply":"2025-03-20T15:47:11.773047Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(99, 7)"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"These test errors are a little better than the previous ones. So it would seem the previous example overfit the data, but only slightly so.\n\nEstos errores de prueba son un poco mejores que los anteriores. Por lo tanto, parecería que el ejemplo anterior sobreajusta los datos, pero solo ligeramente.","metadata":{}},{"cell_type":"code","source":"y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\n\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T15:54:12.740515Z","iopub.execute_input":"2025-03-20T15:54:12.740884Z","iopub.status.idle":"2025-03-20T15:54:12.775041Z","shell.execute_reply.started":"2025-03-20T15:54:12.740846Z","shell.execute_reply":"2025-03-20T15:54:12.773799Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"train_test_gr_error\n### END SOLUTION","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Question 5\n\n* Re-split the data into `X` and `y` parts, this time with `residual_sugar` being the predicted (`y`) data. *Note:* if the indices were preserved from the `StratifiedShuffleSplit` output in question 2, they can be used again to split the data.\n* Using grid search with cross validation, find a decision tree **regression** model that performs well on the test data set.\n* Measure the errors on the training and test sets using mean squared error.\n* Make a plot of actual *vs* predicted residual sugar.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfeature_cols = [x for x in data.columns if x != 'residual_sugar']\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'residual_sugar']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'residual_sugar']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:20:55.075051Z","iopub.execute_input":"2025-03-20T16:20:55.075506Z","iopub.status.idle":"2025-03-20T16:20:55.090364Z","shell.execute_reply.started":"2025-03-20T16:20:55.075464Z","shell.execute_reply":"2025-03-20T16:20:55.088371Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndr = DecisionTreeRegressor().fit(X_train, y_train)\n\nparam_grid = {'max_depth':range(1, dr.tree_.max_depth+1, 2),\n              'max_features': range(1, len(dr.feature_importances_)+1)}\n\nGR_sugar = GridSearchCV(DecisionTreeRegressor(random_state=42),\n                     param_grid=param_grid,\n                     scoring='neg_mean_squared_error',\n                      n_jobs=-1)\n\nGR_sugar = GR_sugar.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:20:56.899985Z","iopub.execute_input":"2025-03-20T16:20:56.900531Z","iopub.status.idle":"2025-03-20T16:21:05.395751Z","shell.execute_reply.started":"2025-03-20T16:20:56.900489Z","shell.execute_reply":"2025-03-20T16:21:05.394697Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"The number of nodes and the maximum depth of the tree. This tree has lots of nodes, which is not so surprising given the continuous data.\n","metadata":{}},{"cell_type":"code","source":"GR_sugar.best_estimator_.tree_.node_count, GR_sugar.best_estimator_.tree_.max_depth","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The error on train and test data sets. Since this is continuous, we will use mean squared error.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ny_train_pred_gr_sugar = GR_sugar.predict(X_train)\ny_test_pred_gr_sugar  = GR_sugar.predict(X_test)\n\ntrain_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n                                          name='MSE').to_frame().T\n\ntrain_test_gr_sugar_error","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A plot of actual vs predicted residual sugar.\n","metadata":{}},{"cell_type":"code","source":"sns.set_context('notebook')\nsns.set_style('white')\nfig = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nph_test_predict = pd.DataFrame({'test':y_test.values,\n                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\n\nph_test_predict.plot(marker='o', ls='', ax=ax)\nax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));\n### END SOLUTION","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Question 6 \n\nThis question requires an additional command line program (GraphViz) and Python library (PyDotPlus). GraphViz can be installed with a package manager on Linux and Mac. For PyDotPlus, either `pip` or `conda` (`conda install -c conda-forge pydotplus`) can be used to install the library.\n\nOnce these programs are installed:\n\n* Create a visualization of the decision tree from question 3, where wine color was predicted and the number of features and/or splits are not limited.\n* Create a visualization of the decision tree from question 4, where wine color was predicted but a grid search was used to find the optimal depth and number of features.\n\nThe decision tree from question 5 will likely have too many nodes to visualize.\n","metadata":{}},{"cell_type":"code","source":"# from io import StringIO\n# from IPython.display import Image\n# from sklearn.tree import export_graphviz\n# import pydotplus\n\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tree from question 3.\n","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\n# Create an output destination for the file\n\n# Assuming 'data' is your DataFrame and you have already defined the feature columns\nfeature_cols = [x for x in data.columns if x != 'residual_sugar']  # Adjust this based on your dataset\n\n# For the original decision tree (assuming `dt` is your trained model)\nplt.figure(figsize=(12, 8))\nplot_tree(dt, filled=True, feature_names=feature_cols, class_names=['Low', 'Medium', 'High'])  # Adjust class names as needed\nplt.title('Decision Tree')\nplt.savefig('wine_tree.png')  # Save the tree as an image\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tree fit with cross validation from question 4. This tree is much shallower than the previous one.\n","metadata":{}},{"cell_type":"code","source":"# For the pruned decision tree (assuming `GR` is your GridSearchCV instance)\nbest_tree = GR.best_estimator_\n\nplt.figure(figsize=(12, 8))\nplot_tree(best_tree, filled=True, feature_names=feature_cols, class_names=['Low', 'Medium', 'High'])  # Adjust class names as needed\nplt.title('Pruned Decision Tree')\nplt.savefig('wine_tree_prune.png')  # Save the pruned tree as an image\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### Machine Learning Foundation (C) 2020 IBM Corporation\n","metadata":{}}]}