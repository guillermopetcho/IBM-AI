{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"97e48ee95ca3e1fe48990c531bf64011ece31cbadfc1d945a98903cb88b52744","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Image Convolutions**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **30** minutes\n","metadata":{}},{"cell_type":"markdown","source":"In this lab, we will explore the purpose of Convolutions, how we can apply them to identify things within images, and the mechanics of the kernel which is essential for all convolutions.\n\n<h1><s>Break the</s> Create the mold</h1>\n\nYou have recently been contacted by a botanical garden. They want to use pictures of their flowers ðŸŒ¸  to create molds of flowers and use to create clay models for children to paint.\n\nThe only issue is theres no easy way to identify where the flowers are in the picture, so it would take a long time to identify and extract manually. You were asked to help process all the pictures in a way that makes the flowers very easy to identify!\n\n<center>\n    <img src=\"https://c.pxhere.com/photos/b5/0b/girl_garden_child_run_oleander_mediterranean_dress_summer-956570.jpg!d\" width=\"600\" alt=\"tulip flower\">\n<center>\n\nImage from [PxHere](https://pxhere.com/en/photo/956570?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n","metadata":{}},{"cell_type":"markdown","source":"## **Table of Contents**\n\n<ol>\n    <li><a href=\"https://#Objectives\">Objectives</a></li>\n    <li>\n        <a href=\"https://#Setup\">Setup</a>\n    </li>\n    <li>\n        <a href=\"https://#Background\">Background</a>\n    </li>\n</ol>\n\n<a href=\"https://#Exercises\">Exercises</a>\n\n<ol>\n    <li><a href=\"https://#E1\">Exercise 1: Implementing Edge Detection</a></li>\n    <li><a href=\"https://#E2\">Exercise 2: Implementing Corner/Blob Detection</a></li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"<h1 href=\"Objectives\">Objectives</h1>\n\nAfter completing this lab you will be able to:\n\n*   Explain how a convolution works on images\n*   Understand the purposes of different kernels that exist\n*   Apply kernels to images and obtain a useful result\n","metadata":{}},{"cell_type":"markdown","source":"***\n","metadata":{}},{"cell_type":"markdown","source":"<h2 href=\"Setup\">Setup</h2>\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n*   [`OpenCV`](https://docs.opencv.org/4.x/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for other image processing functions.\n*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade tensorflow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n*We recommend you import all required libraries in one place (here):*\n","metadata":{}},{"cell_type":"code","source":"# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\n\n\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tensorflow INFO and WARNING messages are not printed ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport PIL\nfrom PIL import Image, ImageOps\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport cv2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1 href=\"Background\">Background</h1>\n\nAs humans, it's quite easy for us to identify petals on a flower. We can see the edges of the petals, the shapes, and count how many there are. It's easy for us to pick them out from a busy picture. This is not the case for computers, which need some extra help when it comes to identifying objects in an image.\n\nNeural networks can mimic this ability using **Convolutions**. Convolutions enable computers to augment an image using matrix multiplication and with some specific kernels (filters), we can do some pretty cool things with them.\n\nLets dive deeper into how convolutions work.\n","metadata":{}},{"cell_type":"markdown","source":"### What does a CNN do?\n\nA CNN is a type of neural network which is designed to process image data. It works by moving an $n\\times m$ sized kernel (matrix) over an input image and performs element wise multiplication over an $n \\times m$ sized portion of your image. In this case, we have an input image of 5x5, and a kernel of 3x3.\n\n<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/convolution.png\" width=\"600\" alt=\"convolution example\">\n</center>\n\nA CNN layer will compute element wise multiplication over a 3x3 window (same size as kernel) on the input image. This means the top left pixel of the input image is multiplied by the top left pixel of the kernel, and so on.\n\n#### What are we computing?\n\nThese values are the brightness values of each pixel! They range between 0-255, and in this case because it's just one array, it means we are working with just one channel. Normally this is a grayscale image, whereas a three channel image would have three arrays with values ranging between 0-255 and we would perform convolutions over each channel. That will be discussed in a future lab.\n\nIn the end, the multiplied values are all added together to become the top left pixel value of the output image.\n","metadata":{}},{"cell_type":"markdown","source":"### Second step\n\nWe now *shift* the kernel one step right on the input image, and recalculate the element wise multiplication of our kernel with the new 3x3 window on the input image. This produces the next pixel value of our output image. The number of pixels we shift by is referred to as the `stride`, and in this example it is `1`.\n\n<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/convolution-1.png\" width=\"600\" alt=\"convolution example 2\">\n</center>\n\nWe can continue to shift our window over the input image until we have covered the entire image. Now we will have an output image of 3x3 which we can pass to the next layer. This whole process is known as convolution, which is where Convolutional Neural Networks get their name.\n","metadata":{}},{"cell_type":"markdown","source":"### Size of output image\n\nNotice that the resulting image is 3x3, compared to our input image of 5x5. This is because our kernel is 3x3 and the pixels in the output image are centered around the interior pixels of the input image. For each pixel in our output image, it took in 9 pixels from the input image. For the border pixels in the input image, there are not enough pixels surrounding it to calculate an output value.\n\nWe can calculate the output image size using the following formula:\n\n$$M_{out} = M_{in} - (K - 1)$$\n\nWhere $M$ represents the width of the input image (it can also represent the height, if our images are square).\n\nIn the above example,\n\n$$\nM_{in} = 5\\\\\\\\\\\\\\\\\nK = 3 \\\\\\\\\\\\\nM_{out} = 5 - (3 - 1) = 3\n$$\n\nThe final output is shown below.\n\n<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/convolution.gif\" width=\"700\" alt=\"convolution example 3\">\n</center>\n\n### Why do this?\n\nBy performing these convolutions, we are able to extract features such as horizontal or vertical lines, edges, and more from an image. For example, the above kernel is known as a Prewitt kernel (we will go over this in this lab) and it specifically looks for vertical lines in images.\n","metadata":{}},{"cell_type":"markdown","source":"## Importing data\n\nLets take a look at the flowers dataset from tensorflow, retrieved from here: [https://www.tensorflow.org/datasets/catalog/tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n","metadata":{}},{"cell_type":"code","source":"dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/flower_photos.tgz\"\ndata_dir = keras.utils.get_file(origin=dataset_url,\n                                fname='flower_photos',\n                                untar=True)\n\ndata_dir = pathlib.Path(data_dir)\n\nfor folder in data_dir.glob('[!LICENSE]*'):\n    print('The', folder.name, 'folder has',\n          len(list(folder.glob('*.jpg'))), 'pictures')\nimage_count = len(list(data_dir.glob('*/*.jpg')))\nprint(image_count, 'total images')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's extract some images we can use for this lab. We will set them all to be square images of 300x300, and display them as well.\n","metadata":{}},{"cell_type":"code","source":"pics = list()\npics_arr = list()\np_class = list()\n\nimg_width = 300\nimg_height = 300\n\nplt.figure(figsize=(20,5))\nfor idx, folder in enumerate(data_dir.glob('[!LICENSE]*')):\n    cat = list(data_dir.glob(folder.name + '/*'))\n    pic = PIL.Image.open(str(cat[0])).resize((img_width, img_height))\n    pic_arr = np.array(pic)\n    clss = folder.name\n    \n    plt.subplot(1,5,idx+1)\n    plt.imshow(pic)\n    plt.title(clss)\n    plt.axis('off')\n    \n    pics.append(pic)\n    pics_arr.append(pic_arr)\n    p_class.append(clss)\n    ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that after importing our images, they're all exactly square with 3200 x 300\\` pixels.\n","metadata":{}},{"cell_type":"code","source":"# Lets get an image to use for the rest of the exercises\nimg = pics[2]\nimg","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1 href=\"Exercises\">Exercise: Playing with kernels</h1>\n\nNow that we have our images, lets play around with the CNN tools we learned.\n\nIn this section, we will see what kernels can do for us. See here for more information: https://en.wikipedia.org/wiki/Feature_(computer_vision)\n\n## Types of kernels\n\nThere exist many kernels used in Computer Vision, and we'll explore some of them and see what they do with our images.\n\n### Edge detection Kernels\n\n[**Prewitt Operator**](https://en.wikipedia.org/wiki/Prewitt_operator?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n\nThis computes an approximation of the gradient between pixels in an image. It's commonly used for edge detection.\n\nThis operator convolves two kernels with an input image, and then approximates the gradient.\n\n\\begin{align\\*}\nG_x = \\begin{bmatrix}\n1 & 0 & -1\\\\\\\\\\\\\\\\\n1 & 0 & -1\\\\\\\\\\\\\n1 & 0 & -1\\\\\\\\\\\\\n\\end{bmatrix} * \\text{Img} \\quad\nG_y = \\begin{bmatrix}\n1 & 1 & 1\\\\\\\\\\\\\n0 & 0 & 0\\\\\\\\\\\\\n-1 & -1 & -1\\\\\\\\\\\\\n\\end{bmatrix} * \\text{Img}\n\\end{align\\*}\n\n[**Sobel Operator**](https://en.wikipedia.org/wiki/Sobel_operator?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\\\nThe Sobel operator performs edge detection just like the Prewitt operator, except with slightly different kernels.\n\n\\begin{align\\*}\nG_x = \\begin{bmatrix}\n1 & 0 & -1\\\\\\\\\\\\\n2 & 0 & -2\\\\\\\\\\\\\n1 & 0 & -1\\\\\\\\\\\\\n\\end{bmatrix} * \\text{Img} \\quad\nG_y = \\begin{bmatrix}\n1 & 2 & 1\\\\\\\\\\\\\n0 & 0 & 0\\\\\\\\\\\\\n-1 & -2 & -1\\\\\\\\\\\\\n\\end{bmatrix} * \\text{Img}\n\\end{align\\*}\n\nWhere:\n- $G_x$ represents the horizontal gradient approximations;\n- $G_y$ represents the vertical gradient approximations;\n- Img is the original input image.\n\n\nTo get the edges, we combine the two gradient approximations from above to produce a gradient magnitude $G$ using:\n\n\\begin{align\\*}\nG = \\sqrt{G_x^2 + G_y^2}\n\\end{align\\*}\n\nWhich will be a pixel value that is large when there is an edge (large change in pixel brightness) and small when there is a smooth transition between pixels.\n\nLets try to implement the Sobel operator using Keras and Numpy.\n","metadata":{}},{"cell_type":"markdown","source":"<h1 href=\"#E1\">Exercise 1: Implementing Edge Detection</h1>\n","metadata":{}},{"cell_type":"code","source":"# Lets define our custom kernels for the horizontal and vertical gradients\ndef v_grad(shape, dtype=None):\n    # Here we use a single numpy array to define our x gradient kernel\n    grad = np.array([\n        [1, 0, -1],\n        [2, 0, -2],\n        [1, 0, -1]\n    ]).reshape((3, 3, 1, 1))\n    # this line is quite important, we are saying we want one 3x3 kernel each for one channel of pixels (grayscale)\n    \n    # We check to make sure the shape of our kernel is the correct shape\n    # according to the initialization of the Convolutional layer below\n    assert grad.shape == shape\n    return keras.backend.variable(grad, dtype='float32')\n\ndef h_grad(shape, dtype=None):\n    grad = np.array([\n        [1, 2, 1],\n        [0, 0, 0],\n        [-1, -2, -1]\n        ]).reshape((3, 3, 1, 1))\n    \n    assert grad.shape == shape\n    return keras.backend.variable(grad, dtype='float32')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Building the Convolutional Neural Network\n\nHere we will build two very simple one layer convolutional neural networks, which will just apply our kernels over an input image. The definition of the function is below.\n\n**keras.layers.Conv2d**\n\n```python\nkeras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n```\n","metadata":{}},{"cell_type":"code","source":"# We define the input layer of our Neural Network\n# to take in an image of 300x300 with 1 channel\n# Both our models can share this, as it will not change between the two\n\ninput_layer = layers.Input(shape=(img_width, img_height, 1))\n\nh_conv = layers.Conv2D(filters=1, # the number of kernels we are using, kernel and filter are interchangeable terms\n                       kernel_size=3,\n                       kernel_initializer=h_grad,\n                       strides=1,\n                       padding='valid')  # 'valid' means no padding\n\nv_conv = layers.Conv2D(filters=1,\n                       kernel_size=3,\n                       kernel_initializer=v_grad,\n                       strides=1,\n                       padding='valid')\n\n\nh_model = keras.Sequential([input_layer, h_conv])\nv_model = keras.Sequential([input_layer, v_conv])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So we've built a very simple neural network with predefined kernels in our convolutional layer. In practice, there are many ways to initialize your kernels.\n\nBelow we have a summary of our model! We can see that there is just a layer that produces a matrix slightly smaller than our input image size of `300x300`, but we also get two matrices which each represent the x and y gradients.\n","metadata":{}},{"cell_type":"code","source":"h_model.summary()\nv_model.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We must convert our RGB image to grayscale, since our CNN models were only designed to apply 1 kernel to 1 channel of input.\n","metadata":{}},{"cell_type":"code","source":"gray = ImageOps.grayscale(img)\n\n# We need to add 1 dimension to our input image which represents the batch size\n# In this case, we just want to process 1 image.\ninput_img = np.array(gray).reshape((1, img_width, img_height, 1))\n\nout_d = h_model.layers[0].output_shape[1:]\n\n# Pass our input image into each model, and return\n# the output with a shape of (298,298,1) in variables named `Gy` and `Gx`.\n# WRITE YOUR CODE HERE\nGx = h_model.predict(input_img).reshape(out_d)\nGy = v_model.predict(input_img).reshape(out_d)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details><summary>Solution</summary>\n    <code>Gx = h_model.predict(input_img).reshape(out_d)</code></br>\n    <code>Gy = v_model.predict(input_img).reshape(out_d)</code>\n</details>\n","metadata":{}},{"cell_type":"code","source":"# Now that we have the two gradients,\n# try computing our gradient magnitude G using numpy\n# WRITE YOUR CODE HERE\nG = np.sqrt(np.add(np.multiply(Gx, Gx), np.multiply(Gy, Gy)))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<details><summary>Solution</summary>\n    <code>G = np.sqrt(np.add(np.multiply(Gx, Gx), np.multiply(Gy, Gy)))</code>\n</details>\n","metadata":{}},{"cell_type":"markdown","source":"Lets look at our image at each step of the process!\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 12))\nplt.subplot(2, 3, 1)\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.subplot(2, 3, 2)\nplt.imshow(gray, cmap=plt.get_cmap('gray'))\nplt.title(\"Grayscale Image\")\nplt.subplot(2, 3, 4)\nplt.imshow(Gx.astype('uint8'), cmap=plt.get_cmap('gray'))\nplt.title(\"Horizontal Gradient\")\nplt.subplot(2, 3, 5)\nplt.imshow(Gy.astype('uint8'), cmap=plt.get_cmap('gray'))\nplt.title(\"Vertical Gradient\")\nplt.subplot(2, 3, 6)\nplt.imshow(G, cmap=plt.get_cmap('gray'))\nplt.title(\"Image after Sobel Operator\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Congratulations! You now know how to implement a simple Prewitt (or Sobel) operator in a Neural Network and use it to detect edges in the image. Recall that these kernels were specifically designed for edge detection in computer vision applications. When you develop your own Convolutional Neural Networks in the future, you will most likely delegate initializing and optimizing the many kernels to Keras and you won't have to define the kernel values manually.\n\n<h2 href=\"Exercise-2\">Exercise 2: Corner and Blob detection</h2>\n\nIn computer vision, we can perform corner detection on images which could then be used to extract certain useful features or infer the contents of the image. You can learn more about it [here](https://en.wikipedia.org/wiki/Corner_detection?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n\nBlob detection is when you want to obtain regions within an image that differ in properties, such as brightness or color. In those regions, properties are relatively constant. You can learn more about this [here](https://en.wikipedia.org/wiki/Blob_detection?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n\n### [Difference of Gaussians (DoG)](https://en.wikipedia.org/wiki/Difference_of_Gaussians?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n\nThis is a method for both corner *and* blob detection. The way it works is by convolving two Gaussian kernels with different variances (sigma) over an image which will produce two blurred versions of the input image. We then subtract the two blurred images and get the resulting DoG processed image which should make it easy to identify corners and blobs.\n\nThe convolution can be illustrated with the following equation:\n\n\\begin{align\\*}\n\\Gamma \\_{\\sigma\\_1 ,sigma\\_2 }(x,y)=\nI \\cdot \\frac {1}{2\\pi \\sigma\\_1^{2}} e^{ - \\frac {x^{2}+y^{2}}{2\\sigma\\_1^{2}} } -\nI \\cdot \\frac {1}{2\\pi \\sigma\\_2^{2}} e^{ - \\frac {x^{2}+y^{2}}{2\\sigma\\_2^{2}} }\n\\end{align\\*}\n\nWhere\n\n*   the first term represents the image convolved with the first Gaussian with mean $0$, variance $\\sigma\\_1^2$ and\n*   the second term represents the image convolved with the second Gaussian with mean $0$, variance $\\sigma\\_2^2$.\n\nNote that $\\sigma\\_2^2 > \\sigma\\_1^2$.\n","metadata":{}},{"cell_type":"markdown","source":"There are built in methods such as scipy's `scipy.ndimage.filters.gaussian_filter` or OpenCV's `cv2.GuassianBlur` which can perform the convolution with a gaussian kernel for you.\n\nWe can implement the `cv2.GuassianBlur` function here!\n","metadata":{}},{"cell_type":"code","source":"sigma_sm = 5\nsigma_lg = 9\nblurred_sm = cv2.GaussianBlur(np.array(gray), (sigma_sm, sigma_sm), sigma_sm)\nblurred_lg = cv2.GaussianBlur(np.array(gray), (sigma_lg, sigma_lg), sigma_lg)\n\nDoG = blurred_sm - blurred_lg\n\nplt.figure(figsize=(18, 12))\nplt.subplot(2, 3, 1)\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.subplot(2, 3, 2)\nplt.imshow(gray, cmap=plt.get_cmap('gray'))\nplt.title(\"Grayscale Image\")\nplt.subplot(2, 3, 4)\nplt.imshow(blurred_sm.astype('uint8'), cmap=plt.get_cmap('gray'))\nplt.title(\"Blurred: Small Sigma\")\nplt.subplot(2, 3, 5)\nplt.imshow(blurred_lg.astype('uint8'), cmap=plt.get_cmap('gray'))\nplt.title(\"Blurred: Large Sigma\")\nplt.subplot(2, 3, 6)\nplt.imshow(DoG.astype('uint8'), cmap=plt.get_cmap('gray'))\nplt.title(\"Image after DoG Operator\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We've now seen how Difference of Gaussians can help us identify the outlines (corners and edges) of objects as well as blobs (areas with very similar properties) very clearly. These methods were all applied by convolving specific kernels over images! I hope this lab has given you a good understanding of how convolutions work, the types of kernels used for specific applications, and why they're useful.\n","metadata":{}},{"cell_type":"markdown","source":"With Difference of Gaussians, we can greatly enhance features within an image by applying two Gaussian kernels to the same image, and take their difference. This technique greatly helps in reducing noise in an image, and results in clearly defined corners of shapes, as well as easily identifiable blobs (regions with very similar properties).\n\nFeel free to play around with the kernel sizes and values for `sigma_sm` and `sigma_lg` to see how those values can change our output image.\n","metadata":{}},{"cell_type":"markdown","source":"And thats it! Hopefully you gained a better understanding of image convolutions through this lab, some important kernels (filters) used in computer vision and why they are useful. You now know how we can implement them using Keras as well.\n\n## Other kernels\n\nThis table is from [wikipedia](https://en.wikipedia.org/wiki/Feature_(computer_vision)#Detection), which shows all the different types of methods you can use to process images and detect different features within.\n\n|              Feature detector             | Edge | Corner | Blob | Ridge |\n|:-----------------------------------------:|:----:|:------:|:----:|:-----:|\n| Canny                                     |  Yes |   No   |  No  |   No  |\n| Sobel                                     |  Yes |   No   |  No  |   No  |\n| Harris & Stephens / Plessey               |  Yes |   Yes  |  No  |   No  |\n| SUSAN                                     |  Yes |   Yes  |  No  |   No  |\n| Shi & Tomasi                              |  No  |   Yes  |  No  |   No  |\n| Level curve curvature                     |  No  |   Yes  |  No  |   No  |\n| FAST                                      |  No  |   Yes  |  Yes |   No  |\n| Laplacian of Gaussian                     |  No  |   Yes  |  Yes |   No  |\n| Difference of Gaussians                   |  No  |   Yes  |  Yes |   No  |\n| Determinant of Hessian                    |  No  |   Yes  |  Yes |   No  |\n| Hessian strength feature measures         |  No  |   Yes  |  Yes |   No  |\n| MSER                                      |  No  |   No   |  Yes |   No  |\n| Principal curvature ridges                |  No  |   No   |  No  |  Yes  |\n| Grey-level blobs                          |  No  |   No   |  Yes |   No  |\n\n## Applying our knowledge to the botanical garden\n\nArmed with this knowledge, how would you help the botanical garden process their images to create clay molds?\n\nI would use the image produced by the Sobel operator, as it gives us a cleaner image to work with so kids can easily paint each section.\n","metadata":{}}]}