{"metadata":{"kernelspec":{"display_name":"py38","language":"python","name":"py38"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"# **(Supplemental) Term Frequency - Inverse Document Frequency**\n","metadata":{}},{"cell_type":"markdown","source":"Estimated time needed: **15** minutes\n","metadata":{}},{"cell_type":"markdown","source":"As we've learned for non-negative matrix factorization, one application of this unsupervised dimensionality reduction is by applying it on a tf-idf matrix.\n\n## Why tf-idf?\n\nAn intuitive way to describe this is that for a given term in a document, we multiply the count of that term in the document by the how rare that term is throughout all the documents we are looking at.\n\nImagine any corpus of data. You'll probably see many many words that appear in almost all documents, such as `the`, `and`, and `so`. If you wanted to quickly analyze text to find the most important words in documents, just looking at word counts isn't good enough. Those previous words would dominate the term frequency in volume and clutter our analysis.\n\nBy performing tf-idf, we can reduce the value assigned to these words that are really common in all our documents, and increase the value of words that may appear a lot in a certain document, but not frequently in other documents.\n\n### Applications\n\nWIth a tf-idf matrix, you can succintly capture important textual information from a large group of text documents. **A corpus is defined as a large structured set of text**. It gives you an efficient representation of what the important words are to each document, and potentially how the words can relate documents together.\n\n**We will be using tf-idf matrices in the next lab!**\n\n## What is tf-idf?\n\nA tf-idf matrix is a `term frequency - inverse document frequency` matrix. Every row within this matrix will represent a document, and every column represents a term (a term could be a single word or an n-tuple of words such as *United States of America*). A tf-idf matrix is actually an augmented `term frequency`, `bag of words` or `document-term` matrix.\n\n### What is `term frequency`?\n\nA `term frequency` matrix simply counts the number of occurences of a given word within a document.\n","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Understand what term frequency and tf-idf matrices are\n*   Explain the intuition behind both matrices and how they are calculated\n*   Apply tf-idf to a corpus of text and find the most important word in each document\n","metadata":{}},{"cell_type":"markdown","source":"***\n","metadata":{}},{"cell_type":"markdown","source":"## Setup\n","metadata":{}},{"cell_type":"markdown","source":"For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","metadata":{}},{"cell_type":"markdown","source":"### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n","metadata":{}},{"cell_type":"code","source":"# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You will need to run the following cell** to install them:\n","metadata":{}},{"cell_type":"code","source":"import piplite\nawait piplite.install(['skillsnetwork'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Required Libraries\n\n*We recommend you import all required libraries in one place (here):*\n","metadata":{}},{"cell_type":"code","source":"import re\nimport skillsnetwork\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Background\n","metadata":{}},{"cell_type":"markdown","source":"**Example**\n\nLets say we have two documents with one sentence each.\n\n*   *\"We like dogs and cats\"*\n*   *\"We like cars and planes\"*\n\nIf we vectorized these two documents into a `term frequency` matrix, we would get:\n\n| doc | We | like | and | dogs | cats | cars | planes |\n| --- | -- | ---- | --- | ---- | ---- | ---- | ------ |\n| 0   | 1  | 1    | 1   | 1    | 1    | 0    | 0      |\n| 1   | 1  | 1    | 1   | 0    | 0    | 1    | 1      |\n\nWe simply count the number of words in each document. (In sklearn, they sort the words alphabetically)\n\nLets convert this into a tf-idf matrix. The value of each element is run through the following function:\n\n$\\text{idf} = (\\log \\frac{N}{|{d\\in D: t\\in d}|} + 1)$\n\n$\\text{tfidf}(t,d, D) = f\\_{t,d} \\* \\text{idf}$\n\nWhere:\n\n*   $f\\_{t,d}$ is the raw count of the term $t$ in document $d$\n*   $N$ is the total number of documents in the corpus (num of all documents)\n*   $|{d\\in D: t\\in d}|$ is the number of documents where the term $t$ appears\n*   We add 1 to the idf portion such that any word that belongs in every document is not just ignored\n","metadata":{}},{"cell_type":"markdown","source":"### Converting to a tf-idf matrix:\n\nFor document 1 the tf-idf value for `like` would be $1 \\* (\\log(\\frac{2}{2})+1) = 1$, but the tf-idf value for `dog` would be $1 \\* (\\log(\\frac{2}{1})+1) = 1.693147$\n\nIf we computed this for every element, we would have:\n\n| doc | We | like | and | dogs   | cats   | cars   | planes |\n| --- | -- | ---- | --- | ------ | ------ | ------ | ------ |\n| 0   | 1  | 1    | 1   | 1.6931 | 1.6931 | 0      | 0      |\n| 1   | 1  | 1    | 1   | 0      | 0      | 1.6931 | 1.6931 |\n","metadata":{}},{"cell_type":"code","source":"### Doing it in code\n\n#This is the function from sklearn that can convert a list of document strings to a term frequency matrix.\n\n\nCountVectorizer(*, input='content', \n                encoding='utf-8', \n                decode_error='strict', \n                strip_accents=None, \n                lowercase=True, \n                preprocessor=None, \n                tokenizer=None, \n                stop_words=None, \n                token_pattern='(?u)\\b\\w\\w+\\b', \n                ngram_range=(1, 1), \n                analyzer='word', \n                max_df=1.0, \n                min_df=1, \n                max_features=None, \n                vocabulary=None, \n                binary=False, \n                dtype=<class 'numpy.int64'>)\nCountVectorizer(*, input='content',\n                encoding='utf-8', \n                decode_error='strict', \n                strip_accents=None, \n                lowercase=True, \n                preprocessor=None, \n                tokenizer=None, \n                stop_words=None, \n                token_pattern='(?u)\\b\\w\\w+\\b', \n                ngram_range=(1, 1), \n                analyzer='word', \n                max_df=1.0, \n                min_df=1, \n                max_features=None, \n                vocabulary=None, \n                binary=False, \n                dtype=<class 'numpy.int64'>)\n\n\n#This is the function that converts a term frequency matrix into a tf-idf matrix.\n\n\nTfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n\n\n#Lets implement the example above using these functions!\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corpus\nD = [\"We like dogs and cats\", \"We like cars and planes\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count Vectorizer creates a term frequency matrix\ncv = CountVectorizer()\ntf_mat = cv.fit_transform(D)\ntf = pd.DataFrame(tf_mat.toarray(), columns = cv.get_feature_names_out())\ntf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating the tfidf matrix\ntfidf_trans = TfidfTransformer(smooth_idf=False)\ntfidf_mat = tfidf_trans.fit_transform(tf)\ntfidf = pd.DataFrame(tfidf_mat.toarray(), columns = tfidf_trans.get_feature_names_out())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tf-idf matrix created above by sklearn does some normalization such that the norm (length) of each document vector (row) is 1. We can instead take the idf vector trained on our data and apply it directly to the term frequency matrix to get the non-normalized tf-idf matrix.\n","metadata":{}},{"cell_type":"code","source":"# Non-normalized tf-idf\npd.DataFrame(tfidf_trans.idf_ * tf.to_numpy(), columns = tfidf_trans.get_feature_names_out())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalized tf-idf\ntfidf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Note: These values are different from the ones we manually calculated as sklearn normalizes each document vector.*\n\n*I.e. $\\overrightarrow{d} \\cdot \\overrightarrow{d} = 1$*\n","metadata":{}},{"cell_type":"code","source":"# d\nprint(tfidf.iloc[0,:])\n# d * d\nnp.multiply(tfidf.iloc[0,:], tfidf.iloc[0,:]).sum().round()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exercises\n\nLets try creating a tf-idf matrix ourselves! Below we have loaded a [dataset from kaggle](https://www.kaggle.com/datasets/vivmankar/physics-vs-chemistry-vs-biology?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) of text, made up of news documents. This is an open domain dataset that is free to use.\n","metadata":{}},{"cell_type":"markdown","source":"Let's start by loading the data into a `pandas.DataFrame`:\n\nSince you're using `jupyterlite`, you will need to use the following method to load datasets:\n","metadata":{}},{"cell_type":"code","source":"URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/tfidf.csv'\nawait skillsnetwork.download_dataset(URL)\n# df = pd.read_json('tfidf.json')\ndf = pd.read_csv('tfidf.csv').iloc[:,1]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at some samples rows from the dataset we loaded:\n","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 1 - Count Vectorizering our text\n\nConvert this matrix of documents into a term frequency matrix. Note that this dataset has numbers, and we want to remove them for simplicity sake.\n\nUse the following function and plug it into `CountVectorizer(preprocessor=preprocess_text)` as an argument.\n\nWe also want to limit the Countvectorizer to just the top 500 words using the `max_features` argument.\n\n**Apply the `CountVectorizer` to the `df` Series and name the columns to the features from the `cv.get_feature_names_out()` function**\n","metadata":{}},{"cell_type":"code","source":"# Lets remove the numbers\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    return text","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Your solution here\ncv = CountVectorizer(max_features = 500, preprocessor = preprocess_text)\ntf = cv.fit_transform(df)\npd.DataFrame(tf.toarray(), columns = cv.get_feature_names_out())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 2 - Applying the tf-idf transformer\n\nNow that we have a term frequency matrix, we can apply the tf-idf function to it in order to obtain a matrix where the values represent how important a certain word is to their documents.\n\n**Apply the TfidfTransformer to the `tf` matrix and name the columns to the features from `CountVectorizer.get_feature_names_out()`**\n","metadata":{}},{"cell_type":"code","source":"# Your solution here\ntfidf_trans = TfidfTransformer()\ntfidf_mat = tfidf_trans.fit_transform(tf.toarray())\ntfidf = pd.DataFrame(tfidf_mat.toarray(), columns = cv.get_feature_names_out())\ntfidf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dense format matrices\n\nAs we can see above, both the term frequency and tf-idf matrices contain a lot of 0's. When dealing with very large corpus of text, or a corpus with a large amount of unique words/features, we will often store the information in a dense format. This saves us space in RAM, as well as reduces the sparsity of the original matrix.\n","metadata":{}},{"cell_type":"markdown","source":"**Normal format:**\n\n| doc | apple | orange | pear |\n| --- | ----- | ------ | ---- |\n| 0   | 0.5   | 0.3    | 0    |\n| 1   | 0     | 0      | 0.4  |\n\n**Dense format:**\n\n| doc | word   | TFIDF Value |\n| --- | ------ | ----------- |\n| 0   | apple  | 0.5         |\n| 0   | orange | 0.3         |\n| 1   | pear   | 0.4         |\n","metadata":{}},{"cell_type":"markdown","source":"In code:\n","metadata":{}},{"cell_type":"code","source":"tfidf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dense_tfidf = tfidf.stack()\ndense_tfidf[dense_tfidf != 0]","metadata":{},"outputs":[],"execution_count":null}]}