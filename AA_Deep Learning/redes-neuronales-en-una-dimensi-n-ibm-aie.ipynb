{"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"c7376ee8f379557856972fee70eb0b7cc6a72f702a09fc0195a7edce7f02b6a4","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Simple One Hidden Layer Neural Network</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Objective</h2><ul><li> How to create simple Neural Network in pytorch.</li></ul> \n","metadata":{}},{"cell_type":"markdown","source":"<h2>Table of Contents</h2>\n<p>In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.</p>\n\n<ul>\n    <li><a href=\"#Model\">Neural Network Module and Training Function</a></li>\n    <li><a href=\"#Makeup_Data\">Make Some Data</a></li>\n    <li><a href=\"#Train\">Define the Neural Network, Criterion Function, Optimizer, and Train the Model</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>25 min</strong></p>\n\n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Preparation</h2>\n","metadata":{}},{"cell_type":"markdown","source":"We'll need the following libraries\n","metadata":{}},{"cell_type":"code","source":"# Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Used for plotting the model\n","metadata":{}},{"cell_type":"code","source":"# The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!--Empty Space for separating topics-->\n","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"Model\">Neural Network Module and Training Function</h2> \n","metadata":{}},{"cell_type":"markdown","source":"Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice. \n","metadata":{}},{"cell_type":"code","source":"# Define the class Net\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n    \n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the training function:\n","metadata":{}},{"cell_type":"code","source":"# Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!--Empty Space for separating topics-->\n","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"Makeup_Data\">Make Some Data</h2>\n","metadata":{}},{"cell_type":"code","source":"# Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] > -4) & (X[:, 0] < 4)] = 1.0","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!--Empty Space for separating topics-->\n","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"Train\">Define the Neural Network, Criterion Function, Optimizer and Train the Model</h2>\n","metadata":{}},{"cell_type":"markdown","source":"Create the Cross-Entropy loss function: \n","metadata":{}},{"cell_type":"code","source":"# The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the Neural Network, Optimizer, and Train the Model:\n","metadata":{}},{"cell_type":"code","source":"# Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By examining the output of the  activation, you see by the 600th epoch that the data has been mapped to a linearly separable space.\n","metadata":{}},{"cell_type":"markdown","source":"we can make a prediction for a arbitrary one tensors \n","metadata":{}},{"cell_type":"code","source":"x=torch.tensor([0.0])\nyhat=model(x)\nyhat","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can make a prediction for some arbitrary one tensors  \n","metadata":{}},{"cell_type":"code","source":"X_=torch.tensor([[0.0],[2.0],[3.0]])\nYhat=model(X_)\nYhat","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we  can threshold the predication\n","metadata":{}},{"cell_type":"code","source":"Yhat=Yhat>0.5\nYhat","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>Practice</h3>\n","metadata":{}},{"cell_type":"markdown","source":"Repeat the previous steps above by using the MSE cost or total loss: \n","metadata":{}},{"cell_type":"code","source":"# Practice: Train the model with MSE Loss Function\n\n# Type your code here\nlearning_rate = 0.1\ncriterion_mse=nn.MSELoss()\nmodel=Net(D_in,H,D_out)\noptimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\ncost_mse=train(Y,X,model,optimizer,criterion_mse,epochs=1000)\nplt.plot(cost_mse)\nplt.xlabel('epoch')\nplt.title('MSE loss ')","metadata":{},"outputs":[],"execution_count":null}]}